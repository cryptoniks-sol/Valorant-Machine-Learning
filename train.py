#!/usr/bin/env python3
print("Starting Optimized Deep Learning Valorant Match Predictor...")

import difflib
import time
import sys
import traceback
from functools import wraps
import logging 
import requests
import json
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import argparse
import pickle
import time
import re
from tqdm import tqdm
import seaborn as sns
import traceback

# Deep learning imports
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Concatenate, Lambda
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, StratifiedKFold, TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from tensorflow.keras.regularizers import l1, l2


# API URL
API_URL = "http://localhost:5000/api/v1"

# Configure TensorFlow for better performance
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    print("GPU is available. Using GPU.")
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
else:
    print("GPU is not available. Using CPU.")

#-------------------------------------------------------------------------
# DATA COLLECTION AND PROCESSING FUNCTIONS
#-------------------------------------------------------------------------

class ModelConfig:
    """Configuration parameters for model training and prediction."""
    
    # Data collection
    CACHE_ENABLED = True
    CACHE_PATH = "cache/valorant_data_cache.pkl"
    CACHE_EXPIRATION_DAYS = 7
    API_URL = "http://localhost:5000/api/v1"
    
    # Feature selection
    MIN_FEATURE_IMPORTANCE = 0.01
    MAX_FEATURES = 50
    FEATURE_SELECTION_METHOD = "random_forest"  # Options: random_forest, mutual_info, recursive
    
    # Cross-validation
    CV_FOLDS = 10
    RANDOM_STATE = 42
    USE_STRATIFIED_CV = True
    
    # Neural network parameters
    NN_HIDDEN_LAYERS = [256, 128, 64]
    NN_DROPOUT_RATE = 0.5
    NN_LEARNING_RATE = 0.001
    NN_BATCH_SIZE = 32
    NN_EPOCHS = 100
    NN_EARLY_STOPPING_PATIENCE = 15
    
    # Ensemble configuration
    ENSEMBLE_MODELS = ["nn", "gb", "rf", "lr", "svm"]  # Neural network, Gradient boosting, Random forest, Logistic regression, SVM
    ENSEMBLE_WEIGHTS = {"nn": 1.0, "gb": 1.0, "rf": 1.0, "lr": 0.8, "svm": 0.8}  # Relative weights
    
    # Prediction calibration
    CALIBRATION_ENABLED = True
    CALIBRATION_METHOD = "isotonic"  # Options: isotonic, sigmoid, beta
    CALIBRATION_STRENGTH = 0.8  # How strongly to calibrate (0-1)
    
    # Team ordering
    CONSISTENT_TEAM_ORDERING = True  # Whether to use consistent team ordering
    TEAM_ORDERING_CRITERIA = [
        "ranking",         # Official ranking (lower number = higher ranking)
        "matches_played",  # Number of matches played (more data = more reliable)
        "win_rate",        # Overall win rate
        "recent_form",     # Recent form (last 5 matches)
        "score_differential"  # Average score differential
    ]


class BettingConfig:
    """Configuration parameters for betting strategy."""
    
    # Kelly criterion settings
    KELLY_FRACTION = 0.15  # Fraction of full Kelly to bet (more conservative)
    MAX_BET_PERCENTAGE = 0.02  # Maximum bet size as percentage of bankroll
    MAX_BET_AMOUNT = 500.0  # Absolute maximum bet size
    
    # Edge requirements
    MIN_EDGE_THRESHOLD = 0.05  # Minimum edge required for betting
    MIN_CONFIDENCE = 0.4  # Minimum model confidence required
    
    # Risk adjustments by bet type
    RISK_FACTORS = {
        "team1_ml": 1.0,       # Baseline risk
        "team2_ml": 1.0,       # Baseline risk
        "team1_plus_1_5": 0.9,  # Lower risk for +1.5 maps
        "team2_plus_1_5": 0.9,  # Lower risk for +1.5 maps
        "team1_minus_1_5": 1.2, # Higher risk for -1.5 maps
        "team2_minus_1_5": 1.2, # Higher risk for -1.5 maps
        "over_2_5_maps": 1.1,   # Slightly higher risk for totals
        "under_2_5_maps": 1.1   # Slightly higher risk for totals
    }
    
    # Bankroll management
    LOSS_STREAK_THRESHOLD = 3  # Number of consecutive losses to trigger reduction
    LOSS_STREAK_REDUCTION = 0.5  # Reduce bet size by 50% after loss streak
    WIN_STREAK_THRESHOLD = 5  # Number of consecutive wins to trigger increase
    WIN_STREAK_INCREASE = 1.2  # Increase bet size by 20% after win streak
    WIN_STREAK_CAP = 1.5  # Cap the streak multiplier at 50% increase
    
    # Drawdown protection
    MAX_DRAWDOWN_PCT = 0.15  # 15% drawdown triggers reduced bet sizing
    DRAWDOWN_REDUCTION = 0.6  # Reduce bet size by 40% during significant drawdown
    
    # Portfolio management
    MAX_SIMULTANEOUS_BETS = 2  # Maximum number of bets per match
    MAX_TOTAL_RISK_PCT = 0.03  # Maximum total risk as percentage of bankroll
    DIVERSIFICATION_REQUIRED = True  # Require diversification of bet types


class ForwardTestConfig:
    """Configuration parameters for forward testing."""
    
    # Data storage
    DATA_DIR = "forward_test"
    PREDICTIONS_FILE = "predictions.json"
    RESULTS_FILE = "results.json"
    SUMMARY_FILE = "summary.json"
    REPORT_FILE = "forward_test_report.html"
    
    # CLV calculation
    MARKET_EFFICIENCY = 0.7  # How efficiently the market responds to betting activity
    RECORD_CLOSING_ODDS = True  # Whether to record closing odds for CLV calculation
    
    # Tracking settings
    TRACK_ACTUAL_BETS = True  # Whether to track actual bets placed
    AUTO_GENERATE_REPORTS = True  # Whether to automatically generate reports
    REPORT_FREQUENCY = 10  # Generate new report every N new results


class BacktestConfig:
    """Configuration parameters for backtesting."""
    
    # Data filtering
    DEFAULT_TEAM_LIMIT = 50  # Default number of teams to include
    USE_CACHE = True  # Whether to use cached data
    CACHE_PATH = "cache/valorant_data_cache.pkl"
    
    # Time filtering
    DEFAULT_START_DATE = None  # Default start date (None = all data)
    DEFAULT_END_DATE = None  # Default end date (None = all data)
    
    # Bankroll settings
    DEFAULT_STARTING_BANKROLL = 1000.0  # Default starting bankroll
    DEFAULT_BET_PCT = 0.05  # Default maximum bet percentage
    DEFAULT_MIN_EDGE = 0.03  # Default minimum edge
    DEFAULT_CONFIDENCE_THRESHOLD = 0.2  # Default confidence threshold
    
    # Reporting
    GENERATE_VISUALIZATIONS = True  # Whether to generate visualizations
    SAVE_DETAILED_RESULTS = True  # Whether to save detailed results
    REALISTIC_ADJUSTMENTS = True  # Whether to apply realistic adjustments to results
    
    # Simulation settings
    SIMULATE_MARKET_MOVEMENT = True  # Whether to simulate market movement
    SIMULATE_LONG_TERM_VARIANCE = True  # Whether to simulate long-term variance
    SIMULATION_RUNS = 1000  # Number of simulation runs


# Consolidated configuration class
class Config:
    """Master configuration for the Valorant match prediction system."""
    
    MODEL = ModelConfig
    BETTING = BettingConfig
    FORWARD_TEST = ForwardTestConfig
    BACKTEST = BacktestConfig
    
    # Global settings
    DEBUG_MODE = False  # Enable additional debug output
    DATA_DIR = "data"  # Directory for data storage
    RESULTS_DIR = "results"  # Directory for results storage
    LOG_DIR = "logs"  # Directory for log files
    
    @classmethod
    def create_directories(cls):
        """Create necessary directories if they don't exist."""
        import os
        for directory in [cls.DATA_DIR, cls.RESULTS_DIR, cls.LOG_DIR, 
                         cls.FORWARD_TEST.DATA_DIR, "cache"]:
            if not os.path.exists(directory):
                os.makedirs(directory)
                print(f"Created directory: {directory}")

def debug_func(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        print(f"\n>>> ENTERING {func.__name__} <<<")
        try:
            result = func(*args, **kwargs)
            print(f">>> EXITING {func.__name__} <<<")
            return result
        except Exception as e:
            print(f"!!! ERROR in {func.__name__}: {e}")
            traceback.print_exc()
            return None
    return wrapper

def load_cache(cache_path="cache/valorant_data_cache.pkl"):
    """Load team data from the cache file."""
    print(f"\nLoading cached team data from {cache_path}...")
    try:
        with open(cache_path, 'rb') as f:
            print("Cache file opened successfully, attempting to load...")
            team_data = pickle.load(f)
            print(f"Pickle loaded, checking data type: {type(team_data)}")
            
        # Check if the cache has the expected structure
        if not isinstance(team_data, dict):
            print(f"Error: Invalid cache format - expected dictionary, got {type(team_data)}")
            return None
            
        teams_count = len(team_data)
        print(f"Teams in cache: {teams_count}")
        
        # Count matches
        match_counts = {}
        total_matches = 0
        for team, data in team_data.items():
            matches = len(data.get('matches', []))
            match_counts[team] = matches
            total_matches += matches
            
        print(f"Teams with most matches:")
        for team, count in sorted(match_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"  - {team}: {count} matches")
        
        # Get cache age
        cache_mtime = os.path.getmtime(cache_path)
        cache_age = time.time() - cache_mtime
        days_old = cache_age / (60 * 60 * 24)
        
        print(f"Cache loaded successfully with {teams_count} teams and {total_matches} total matches.")
        print(f"Cache is {days_old:.1f} days old (last updated {datetime.fromtimestamp(cache_mtime).strftime('%Y-%m-%d %H:%M:%S')})")
        
        if days_old > 7:
            print("WARNING: Cache is more than 7 days old. Consider updating it with cache.py.")
        
        # Check for key team data components
        first_team = next(iter(team_data.values()))
        print(f"First team data keys: {first_team.keys()}")
        
        if 'matches' in first_team:
            first_match = first_team['matches'][0] if first_team['matches'] else None
            if first_match:
                print(f"First match keys: {first_match.keys()}")
                
        return team_data
            
    except FileNotFoundError:
        print(f"Error: Cache file not found at {cache_path}")
        print("Please run cache.py first to create the cache.")
        return None
    except Exception as e:
        print(f"Error loading cache: {e}")
        traceback.print_exc()
        return None

def check_cache_freshness(cache_path="cache/valorant_data_cache.pkl", max_age_days=7):
    """
    Check if the cache is still fresh or needs updating.
    
    Args:
        cache_path (str): Path to the cache file
        max_age_days (int): Maximum acceptable age in days
        
    Returns:
        bool: True if cache is fresh, False otherwise
    """
    try:
        if not os.path.exists(cache_path):
            return False
            
        # Get cache age
        cache_mtime = os.path.getmtime(cache_path)
        cache_age = time.time() - cache_mtime
        days_old = cache_age / (60 * 60 * 24)
        
        return days_old <= max_age_days
        
    except Exception:
        return False

def get_cache_metadata(cache_dir="cache"):
    """
    Get metadata about the cache for display.
    
    Args:
        cache_dir (str): Directory containing the cache files
        
    Returns:
        dict: Cache metadata or empty dict if not found
    """
    meta_path = os.path.join(cache_dir, "cache_metadata.json")
    try:
        if os.path.exists(meta_path):
            with open(meta_path, 'r') as f:
                return json.load(f)
        return {}
    except Exception as e:
        print(f"Error reading cache metadata: {e}")
        return {}

def get_team_id_exact_only(team_name, region=None, max_retries=2):
    """
    Team ID search with EXACT matches only - no fuzzy matching.
    
    Args:
        team_name (str): Team name to search for
        region (str): Optional region filter
        max_retries (int): Maximum retry attempts
    
    Returns:
        str: Team ID or None if not found
    """
    print(f"Searching for team ID for '{team_name}'...")
    
    for attempt in range(max_retries + 1):
        try:
            url = f"{API_URL}/teams?limit=300"
            if region:
                url += f"&region={region}"
                print(f"Filtering by region: {region}")
            
            response = requests.get(url, timeout=30)
            if response.status_code != 200:
                print(f"Error fetching teams (attempt {attempt + 1}): {response.status_code}")
                if attempt < max_retries:
                    print("Retrying...")
                    time.sleep(2)
                    continue
                return None

            teams_data = response.json()
            if 'data' not in teams_data:
                print("No 'data' field found in the response")
                return None

            team_name_clean = team_name.strip()
            team_name_lower = team_name_clean.lower()
            
            # Strategy 1: EXACT match (case insensitive)
            for team in teams_data['data']:
                api_team_name = team['name'].strip()
                if api_team_name.lower() == team_name_lower:
                    print(f"Found exact match: {api_team_name} (ID: {team['id']})")
                    return team['id']
            
            # Strategy 2: EXACT match (case sensitive) - sometimes case matters
            for team in teams_data['data']:
                api_team_name = team['name'].strip()
                if api_team_name == team_name_clean:
                    print(f"Found exact case-sensitive match: {api_team_name} (ID: {team['id']})")
                    return team['id']
            
            # Strategy 3: Contains match - but only if the search term is in the API name
            # This handles cases like "Team Liquid" vs "Liquid" 
            for team in teams_data['data']:
                api_team_name = team['name'].strip().lower()
                if team_name_lower in api_team_name:
                    # Additional check: make sure it's a reasonable match
                    # (the API name shouldn't be dramatically longer)
                    if len(api_team_name) <= len(team_name_lower) * 2:
                        print(f"Found partial match (API contains search): {team['name']} (ID: {team['id']})")
                        return team['id']
            
            print(f"No exact match found for '{team_name}'")
            
            # If no match found and no region specified, try different regions
            if not region and attempt == 0:
                print(f"Searching across all regions...")
                regions = ['na', 'eu', 'br', 'ap', 'kr', 'ch', 'jp', 'lan', 'las', 'oce', 'mn', 'gc']
                for r in regions:
                    print(f"Trying region: {r}")
                    region_id = get_team_id_exact_only(team_name, r, max_retries=0)
                    if region_id:
                        return region_id
            
            return None
            
        except requests.exceptions.Timeout:
            print(f"Timeout error (attempt {attempt + 1})")
            if attempt < max_retries:
                print("Retrying...")
                time.sleep(3)
                continue
        except requests.exceptions.RequestException as e:
            print(f"Request error (attempt {attempt + 1}): {e}")
            if attempt < max_retries:
                print("Retrying...")
                time.sleep(2)
                continue
        except Exception as e:
            print(f"Unexpected error searching for team '{team_name}': {e}")
            if attempt < max_retries:
                print("Retrying...")
                time.sleep(2)
                continue
    
    return None

def find_team_in_collection_exact_only(team_name, team_data_collection):
    """
    Find team in collection with exact matching only.
    
    Args:
        team_name (str): Name to search for
        team_data_collection (dict): Collection of team data
    
    Returns:
        tuple: (found_name, similarity_score) or (None, 0) if not found
    """
    team_name_clean = team_name.strip()
    team_name_lower = team_name_clean.lower()
    
    # Strategy 1: Exact match (case insensitive)
    for cached_name in team_data_collection.keys():
        if cached_name.strip().lower() == team_name_lower:
            return cached_name, 1.0
    
    # Strategy 2: Exact match (case sensitive)
    for cached_name in team_data_collection.keys():
        if cached_name.strip() == team_name_clean:
            return cached_name, 1.0
    
    # Strategy 3: Contains match (only if search term is in cached name)
    for cached_name in team_data_collection.keys():
        cached_lower = cached_name.lower()
        if team_name_lower in cached_lower:
            # Make sure it's reasonable (cached name not dramatically longer)
            if len(cached_lower) <= len(team_name_lower) * 2:
                return cached_name, 0.9
    
    return None, 0

def fetch_api_data(endpoint, params=None):
    """Generic function to fetch data from API with error handling"""
    url = f"{API_URL}/{endpoint}"
    if params:
        url += f"?{'&'.join([f'{k}={v}' for k, v in params.items()])}"
    
    try:
        response = requests.get(url)
        if response.status_code != 200:
            print(f"Error fetching data from {endpoint}: {response.status_code}")
            return None
        return response.json()
    except Exception as e:
        print(f"Exception while fetching {endpoint}: {e}")
        return None

def fetch_team_details(team_id):
    """Fetch detailed information about a team, including the team tag."""
    if not team_id:
        return None, None
    
    team_data = fetch_api_data(f"teams/{team_id}")
    if not team_data or 'data' not in team_data:
        return None, None
    
    # Extract the team tag if available - it's in data.info.tag
    team_tag = None
    if ('data' in team_data and 
        isinstance(team_data['data'], dict) and 
        'info' in team_data['data'] and 
        isinstance(team_data['data']['info'], dict) and
        'tag' in team_data['data']['info']):
        team_tag = team_data['data']['info']['tag']
        print(f"Team tag found: {team_tag}")
    else:
        print(f"Team tag not found in data structure")
    
    return team_data, team_tag

def fetch_player_stats(player_name):
    """Fetch detailed player statistics using the API endpoint."""
    if not player_name:
        return None
    
    print(f"Fetching stats for player: {player_name}")
    player_data = fetch_api_data(f"player-stats/{player_name}")
    
    # Return player stats if successful
    if player_data and player_data.get('status') == 'OK' and 'data' in player_data:
        return player_data['data']
    
    return None

def fetch_team_player_stats(team_id):
    """Fetch detailed player statistics for a team from the team roster."""
    if not team_id:
        print(f"Invalid team ID: {team_id}")
        return []
    
    # Get team details with roster information
    team_data = fetch_api_data(f"teams/{team_id}")
    
    if not team_data or 'data' not in team_data or not isinstance(team_data['data'], dict):
        print(f"Invalid team data format for team ID: {team_id}")
        return []
        
    team_info = team_data['data']
    
    # Extract player info from the roster
    players = team_info.get('players', [])
    
    if not players:
        print(f"No players found in roster for team ID: {team_id}")
        return []
    
    print(f"Found {len(players)} players in roster for team: {team_info.get('info', {}).get('name', '')}")
    
    # Fetch individual player stats
    player_stats = []
    for player in players:
        player_id = player.get('id')
        player_name = player.get('user')  # Use the in-game username
        
        if not player_name:
            continue
            
        print(f"Fetching statistics for player: {player_name}")
        stats = fetch_player_stats(player_name)
        
        if stats:
            # Add additional player info from the team data
            stats['player_id'] = player_id
            stats['full_name'] = player.get('name', '')
            stats['country'] = player.get('country', '')
            
            player_stats.append(stats)
        else:
            print(f"No statistics found for player: {player_name}")
    
    print(f"Successfully fetched statistics for {len(player_stats)} out of {len(players)} players")
    return player_stats

def fetch_team_match_history(team_id):
    """Fetch match history for a specific team."""
    if not team_id:
        return None
    
    print(f"Fetching match history for team ID: {team_id}")
    return fetch_api_data(f"match-history/{team_id}")

def fetch_match_details(match_id):
    """Fetch detailed information about a specific match."""
    if not match_id:
        return None
    
    print(f"Fetching details for match ID: {match_id}")
    return fetch_api_data(f"match-details/{match_id}")

def fetch_match_economy_details(match_id):
    """Fetch economic details for a specific match."""
    if not match_id:
        return None
    
    print(f"Fetching economy details for match ID: {match_id}")
    return fetch_api_data(f"match-details/{match_id}", {"tab": "economy"})

def fetch_team_map_statistics(team_id):
    """Fetch detailed map statistics for a team using the team-stats API endpoint."""
    if not team_id:
        print("Invalid team ID")
        return {}
    
    print(f"Fetching map statistics for team ID: {team_id}")
    team_stats_data = fetch_api_data(f"team-stats/{team_id}")
    
    if not team_stats_data:
        print(f"No team stats data returned for team ID: {team_id}")
        return {}
    
    # Check for expected data structure
    if 'data' not in team_stats_data:
        print(f"Missing 'data' field in team stats for team ID: {team_id}")
        print(f"Received: {team_stats_data}")
        return {}
        
    if not team_stats_data['data']:
        print(f"Empty data array in team stats for team ID: {team_id}")
        return {}
    
    # Process the data
    map_stats = extract_map_statistics(team_stats_data)
    
    if not map_stats:
        print(f"Failed to extract map statistics for team ID: {team_id}")
    else:
        print(f"Successfully extracted statistics for {len(map_stats)} maps")
        
    return map_stats

def determine_team_order(team1_stats, team2_stats):
    """
    Determine consistent team ordering for prediction based on ranking and other metrics.
    
    The model performs best when given consistent team ordering rules. This function
    implements a strict priority system to determine which team should be placed first:
    
    Priority order:
    1. Team with higher official ranking (if available)
    2. Team with more matches played (more data usually means more reliable prediction)
    3. Team with higher win rate
    4. Team with higher recent form
    5. Team with higher score differential
    
    Returns:
        tuple: (ordered_team1_stats, ordered_team2_stats, teams_swapped)
               where teams_swapped is a boolean indicating if the original order was changed
    """
    teams_swapped = False
    
    # 1. Use official ranking if available
    team1_ranking = get_team_ranking_safe(team1_stats)
    team2_ranking = get_team_ranking_safe(team2_stats)
    
    if team1_ranking is not None and team2_ranking is not None:
        if team2_ranking < team1_ranking:  # Lower rank number = higher ranking
            return team2_stats, team1_stats, True
    
    # 2. Use match count if rankings not available or tied
    team1_matches = get_match_count(team1_stats)
    team2_matches = get_match_count(team2_stats)
    
    if team1_matches != team2_matches:
        if team2_matches > team1_matches:
            return team2_stats, team1_stats, True
    
    # 3. Use win rate if match counts are tied
    team1_winrate = team1_stats.get('win_rate', 0)
    team2_winrate = team2_stats.get('win_rate', 0)
    
    if abs(team1_winrate - team2_winrate) > 0.01:  # 1% threshold to avoid floating point issues
        if team2_winrate > team1_winrate:
            return team2_stats, team1_stats, True
    
    # 4. Use recent form if win rates are tied
    team1_form = team1_stats.get('recent_form', 0)
    team2_form = team2_stats.get('recent_form', 0)
    
    if abs(team1_form - team2_form) > 0.01:
        if team2_form > team1_form:
            return team2_stats, team1_stats, True
    
    # 5. Use score differential if everything else is tied
    team1_diff = team1_stats.get('score_differential', 0)
    team2_diff = team2_stats.get('score_differential', 0)
    
    if team2_diff > team1_diff:
        return team2_stats, team1_stats, True
    
    # Default: keep original order
    return team1_stats, team2_stats, teams_swapped

def get_team_ranking_safe(team_stats):
    """
    Safely extract team ranking from team stats.
    
    Args:
        team_stats (dict): Team statistics
        
    Returns:
        int or None: Team ranking if available, None otherwise
    """
    if 'opponent_quality' in team_stats and 'team_ranking' in team_stats['opponent_quality']:
        return team_stats['opponent_quality']['team_ranking']
    return None

def get_match_count(team_stats):
    """
    Get the number of matches played by a team.
    
    Args:
        team_stats (dict): Team statistics
        
    Returns:
        int: Number of matches played
    """
    if 'matches' in team_stats:
        matches = team_stats['matches']
        if isinstance(matches, list):
            return len(matches)
        elif isinstance(matches, (int, float)):
            return int(matches)
    return 0

def modify_prediction_for_swapped_teams(prediction, teams_swapped):
    """
    Modify prediction when teams have been swapped for consistency.
    
    Args:
        prediction (float): Original prediction (probability of team1 winning)
        teams_swapped (bool): Whether teams were swapped in the ordering process
        
    Returns:
        float: Adjusted prediction probability
    """
    if teams_swapped:
        # If teams were swapped, we need to invert the prediction
        return 1.0 - prediction
    return prediction

def prepare_data_with_consistent_ordering(team1_stats, team2_stats, selected_features):
    """
    Prepare prediction data with consistent team ordering.
    
    Args:
        team1_stats (dict): Statistics for team 1
        team2_stats (dict): Statistics for team 2
        selected_features (list): Features to include in prediction
        
    Returns:
        tuple: (features, teams_swapped)
    """
    # Apply consistent team ordering
    ordered_team1_stats, ordered_team2_stats, teams_swapped = determine_team_order(team1_stats, team2_stats)
    
    # Prepare features using the ordered team stats
    features = prepare_features_for_backtest(ordered_team1_stats, ordered_team2_stats, selected_features)
    
    return features, teams_swapped

def get_historical_team_stats(team_name, target_date, team_data_collection):
    """
    Get team statistics using only matches that occurred before the specified date.
    This prevents data leakage by ensuring we only use information that would have
    been available at prediction time.
    
    Args:
        team_name (str): Name of the team
        target_date (str): Date to get stats for, in format 'YYYY-MM-DD'
        team_data_collection (dict): Dictionary of team data
        
    Returns:
        dict: Team stats using only historical data available at target_date
    """
    if team_name not in team_data_collection:
        print(f"Warning: Team '{team_name}' not found in data collection")
        return None
    
    team_data = team_data_collection[team_name]
    past_matches = []
    
    # Filter matches to only include those before target_date
    for match in team_data.get('matches', []):
        match_date = match.get('date', '')
        if match_date and match_date < target_date:
            past_matches.append(match)
    
    if len(past_matches) < 3:
        print(f"Warning: Insufficient historical data for {team_name} before {target_date}")
        print(f"Only {len(past_matches)} matches available")
        return None
    
    # Calculate team stats using only historical matches
    team_stats = calculate_team_stats(past_matches, 
                                     team_data.get('player_stats', None),
                                     include_economy=True)
    
    # Add team identification for convenience
    team_stats['team_name'] = team_name
    team_stats['team_id'] = team_data.get('team_id', '')
    team_stats['team_tag'] = team_data.get('team_tag', '')
    
    # Get historical map statistics
    if 'map_statistics' in team_data:
        # We need to filter map statistics to only include matches before target_date
        # This is complex and requires detailed match-by-match filtering
        # For now, we'll include the map statistics with a warning
        team_stats['map_statistics'] = team_data.get('map_statistics', {})
    
    # Calculate historical opponent quality metrics
    if past_matches:
        team_stats['opponent_quality'] = analyze_opponent_quality(past_matches, team_data.get('team_id', ''))
    
    return team_stats

def analyze_historical_h2h(team1_name, team2_name, target_date, team_data_collection):
    """
    Analyze head-to-head record between two teams using only historical data
    available at the specified date.
    
    Args:
        team1_name (str): First team name
        team2_name (str): Second team name
        target_date (str): Cut-off date for historical data
        team_data_collection (dict): Dictionary of team data
        
    Returns:
        dict: Head-to-head statistics
    """
    if team1_name not in team_data_collection or team2_name not in team_data_collection:
        return {
            'matches': 0,
            'team1_wins': 0,
            'team2_wins': 0,
            'win_rate': 0.5,
            'average_score_diff': 0
        }
    
    # Get all historical matches for team1
    team1_data = team_data_collection[team1_name]
    h2h_matches = []
    
    # Filter for matches against team2 before target_date
    for match in team1_data.get('matches', []):
        match_date = match.get('date', '')
        opponent = match.get('opponent_name', '')
        
        if match_date and match_date < target_date and opponent == team2_name:
            h2h_matches.append(match)
    
    # If no direct matches found, check with variations of team2 name
    if not h2h_matches:
        for match in team1_data.get('matches', []):
            match_date = match.get('date', '')
            opponent = match.get('opponent_name', '')
            
            if match_date and match_date < target_date:
                # Check for partial matches in team names
                if (team2_name.lower() in opponent.lower() or 
                    opponent.lower() in team2_name.lower()):
                    h2h_matches.append(match)
    
    # Calculate h2h stats
    if not h2h_matches:
        return {
            'matches': 0,
            'team1_wins': 0,
            'team2_wins': 0,
            'win_rate': 0.5,
            'average_score_diff': 0
        }
    
    team1_wins = sum(1 for m in h2h_matches if m.get('team_won', False))
    team2_wins = len(h2h_matches) - team1_wins
    
    total_team1_score = sum(m.get('team_score', 0) for m in h2h_matches)
    total_team2_score = sum(m.get('opponent_score', 0) for m in h2h_matches)
    
    avg_score_diff = (total_team1_score - total_team2_score) / len(h2h_matches)
    
    return {
        'matches': len(h2h_matches),
        'team1_wins': team1_wins,
        'team2_wins': team2_wins,
        'win_rate': team1_wins / len(h2h_matches),
        'average_score_diff': avg_score_diff
    }

def implement_time_aware_cross_validation(team_data_collection, n_splits=5):
    """
    Implement time-aware cross-validation to prevent data leakage.
    
    Args:
        team_data_collection (dict): Dictionary of team data
        n_splits (int): Number of validation splits
        
    Returns:
        list: List of (train, test) indices
    """
    all_matches = []
    
    # Collect all matches with their dates
    for team_name, team_data in team_data_collection.items():
        for match in team_data.get('matches', []):
            match['team_name'] = team_name
            if 'date' in match and match['date']:
                all_matches.append(match)
    
    # Sort matches by date
    all_matches.sort(key=lambda x: x.get('date', ''))
    
    # Create time-based splits
    splits = []
    matches_per_split = len(all_matches) // n_splits
    
    for i in range(n_splits):
        if i < n_splits - 1:
            train_end = (i + 1) * matches_per_split
            test_start = train_end
            test_end = (i + 2) * matches_per_split
        else:
            train_end = (i + 1) * matches_per_split
            test_start = train_end
            test_end = len(all_matches)
        
        train_indices = list(range(0, train_end))
        test_indices = list(range(test_start, test_end))
        
        splits.append((train_indices, test_indices))
    
    return splits, all_matches

def prepare_time_aware_data(match, team_data_collection):
    """
    Prepare data for a match using only information available at match time.
    
    Args:
        match (dict): Match data with date
        team_data_collection (dict): Dictionary of team data
        
    Returns:
        tuple: (features, label) or (None, None) if data cannot be prepared
    """
    match_date = match.get('date', '')
    if not match_date:
        return None, None
    
    team1_name = match.get('team_name', '')
    team2_name = match.get('opponent_name', '')
    
    if not team1_name or not team2_name:
        return None, None
    
    if team1_name not in team_data_collection or team2_name not in team_data_collection:
        return None, None
    
    # Get historical stats for both teams
    team1_stats = get_historical_team_stats(team1_name, match_date, team_data_collection)
    team2_stats = get_historical_team_stats(team2_name, match_date, team_data_collection)
    
    if not team1_stats or not team2_stats:
        return None, None
    
    # Add historical head-to-head data
    h2h_stats = analyze_historical_h2h(team1_name, team2_name, match_date, team_data_collection)
    team1_stats['h2h'] = h2h_stats
    
    # Prepare features
    features = prepare_data_for_model(team1_stats, team2_stats)
    
    if not features:
        return None, None
    
    # Get label
    label = 1 if match.get('team_won', False) else 0
    
    return features, label

def time_aware_prediction(team1_name, team2_name, current_date, team_data_collection, ensemble_models, selected_features):
    """
    Make a prediction using only data available at prediction time.
    
    Args:
        team1_name (str): First team name
        team2_name (str): Second team name
        current_date (str): Current date in 'YYYY-MM-DD' format
        team_data_collection (dict): Dictionary of team data
        ensemble_models (list): Ensemble models for prediction
        selected_features (list): Selected features for prediction
        
    Returns:
        tuple: (win_probability, confidence, swapped)
    """
    # Get historical stats for both teams
    team1_stats = get_historical_team_stats(team1_name, current_date, team_data_collection)
    team2_stats = get_historical_team_stats(team2_name, current_date, team_data_collection)
    
    if not team1_stats or not team2_stats:
        print(f"Error: Insufficient historical data for prediction")
        return 0.5, 0.0, False
    
    # Apply consistent team ordering
    features, teams_swapped = prepare_data_with_consistent_ordering(
        team1_stats, team2_stats, selected_features
    )
    
    if features is None:
        print(f"Error: Failed to prepare features for prediction")
        return 0.5, 0.0, teams_swapped
    
    # Make prediction
    win_probability, raw_predictions, confidence = predict_with_ensemble(
        ensemble_models, features
    )
    
    # Adjust prediction if teams were swapped
    win_probability = modify_prediction_for_swapped_teams(win_probability, teams_swapped)
    
    return win_probability, confidence, teams_swapped

def fetch_upcoming_matches():
    """Fetch upcoming matches."""
    print("Fetching upcoming matches...")
    matches_data = fetch_api_data("matches")
    
    if not matches_data:
        return []
    
    return matches_data.get('data', [])

def fetch_events():
    """Fetch all events from the API."""
    print("Fetching events...")
    events_data = fetch_api_data("events", {"limit": 300})
    
    if not events_data:
        return []
    
    return events_data.get('data', [])

def parse_match_data(match_history, team_name):
    """
    Parse match history data for a team with improved map score extraction.
    """
    if not match_history or 'data' not in match_history:
        return []
    
    matches = []
    filtered_count = 0
    
    print(f"Parsing {len(match_history['data'])} matches for {team_name}")
    
    for match in match_history['data']:
        try:
            # Check if this match involves the team we're looking for
            team_found = False
            if 'teams' in match and len(match['teams']) >= 2:
                for team in match['teams']:
                    if (team.get('name', '').lower() == team_name.lower() or 
                        team_name.lower() in team.get('name', '').lower()):
                        team_found = True
                        break
           
            if not team_found:
                filtered_count += 1
                continue
                
            # Extract basic match info
            match_info = {
                'match_id': match.get('id', ''),
                'date': match.get('date', ''),
                'event': match.get('event', '') if isinstance(match.get('event', ''), str) else match.get('event', {}).get('name', ''),
                'tournament': match.get('tournament', ''),
                'map': match.get('map', ''),
                'map_score': ''  # Initialize map score as empty
            }
            
            # Extract teams and determine which is our team
            if 'teams' in match and len(match['teams']) >= 2:
                team1 = match['teams'][0]
                team2 = match['teams'][1]
                
                # Convert scores to integers for comparison
                team1_score = int(team1.get('score', 0))
                team2_score = int(team2.get('score', 0))
                
                # Set map score directly from the scores - this is critical for dataset building
                match_info['map_score'] = f"{team1_score}:{team2_score}"
                
                # Determine winners based on scores
                team1_won = team1_score > team2_score
                team2_won = team2_score > team1_score
                
                # Determine if our team is team1 or team2
                is_team1 = team1.get('name', '').lower() == team_name.lower() or team_name.lower() in team1.get('name', '').lower()
                
                if is_team1:
                    our_team = team1
                    opponent_team = team2
                    team_won = team1_won
                else:
                    our_team = team2
                    opponent_team = team1
                    team_won = team2_won
                
                # Add our team's info
                match_info['team_name'] = our_team.get('name', '')
                match_info['team_score'] = int(our_team.get('score', 0))
                match_info['team_won'] = team_won
                match_info['team_country'] = our_team.get('country', '')
                
                # Use actual team tag if available, don't hardcode it
                # First try to get it from the team data
                match_info['team_tag'] = our_team.get('tag', '')
                
                # If tag not in team data, try to extract it from name (common formats: "TAG Name" or "Name [TAG]")
                if not match_info['team_tag']:
                    team_name_parts = our_team.get('name', '').split()
                    if len(team_name_parts) > 0:
                        # Check if first word might be a tag (all caps, 2-5 letters)
                        first_word = team_name_parts[0]
                        if first_word.isupper() and 2 <= len(first_word) <= 5:
                            match_info['team_tag'] = first_word
                    
                    # If still no tag, check for [TAG] format
                    if not match_info['team_tag'] and '[' in our_team.get('name', '') and ']' in our_team.get('name', ''):
                        tag_match = re.search(r'\[(.*?)\]', our_team.get('name', ''))
                        if tag_match:
                            match_info['team_tag'] = tag_match.group(1)
                
                # Add opponent's info
                match_info['opponent_name'] = opponent_team.get('name', '')
                match_info['opponent_score'] = int(opponent_team.get('score', 0))
                match_info['opponent_won'] = not team_won  # Opponent's result is opposite of our team
                match_info['opponent_country'] = opponent_team.get('country', '')
                match_info['opponent_tag'] = opponent_team.get('tag', '')
                match_info['opponent_id'] = opponent_team.get('id', '')  # Save opponent ID for future reference
                
                # Add the result field
                match_info['result'] = 'win' if team_won else 'loss'               

                # Fetch match details for deeper statistics
                match_details = fetch_match_details(match_info['match_id'])
                if match_details:
                    # Add match details to the match_info
                    match_info['details'] = match_details
                
                matches.append(match_info)
            
        except Exception as e:
            print(f"Error parsing match: {e}")
            continue
    
    print(f"Skipped {filtered_count} matches that did not involve {team_name}")   
    # Summarize wins/losses
    wins = sum(1 for match in matches if match['team_won'])
    print(f"Processed {len(matches)} matches for {team_name}: {wins} wins, {len(matches) - wins} losses")
    
    return matches

#-------------------------------------------------------------------------
# DATA EXTRACTION AND PROCESSING
#-------------------------------------------------------------------------

def extract_map_statistics(team_stats_data):
    """Extract detailed map statistics from team stats API response."""
    def safe_percentage(value):
        try:
            return float(value.strip('%')) / 100
        except (ValueError, AttributeError, TypeError):
            return 0

    def safe_int(value):
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0

    if not team_stats_data or 'data' not in team_stats_data:
        return {}

    maps_data = team_stats_data['data']
    map_statistics = {}

    for map_entry in maps_data:
        map_name = map_entry.get('map', '').split(' ')[0]
        if not map_name:
            continue

        stats = map_entry.get('stats', [])
        if not stats:
            continue

        main_stats = stats[0]

        win_percentage = safe_percentage(main_stats.get('WIN%', '0%'))

        wins = safe_int(main_stats.get('W', '0'))
        losses = safe_int(main_stats.get('L', '0'))

        atk_first = safe_int(main_stats.get('ATK 1st', '0'))
        def_first = safe_int(main_stats.get('DEF 1st', '0'))

        atk_win_rate = safe_percentage(main_stats.get('ATK RWin%', '0%'))
        def_win_rate = safe_percentage(main_stats.get('DEF RWin%', '0%'))

        atk_rounds_won = safe_int(main_stats.get('RW', '0'))
        atk_rounds_lost = safe_int(main_stats.get('RL', '0'))

        agent_compositions = main_stats.get('Agent Compositions', [])

        # Group agents into teams of 5
        team_compositions = []
        current_comp = []

        for agent in agent_compositions:
            current_comp.append(agent)
            if len(current_comp) == 5:
                team_compositions.append(current_comp)
                current_comp = []

        agent_usage = {}
        for comp in team_compositions:
            for agent in comp:
                agent_usage[agent] = agent_usage.get(agent, 0) + 1

        sorted_agents = sorted(agent_usage.items(), key=lambda x: x[1], reverse=True)

        match_history = []
        for i in range(1, len(stats)):
            match_stat = stats[i]
            match_details = match_stat.get('Expand', '')
            if not match_details:
                continue

            parts = match_details.split()
            try:
                date = parts[0] if parts else ''
                score_index = next((i for i, p in enumerate(parts) if '/' in p), -1)

                opponent = ' '.join(parts[1:score_index]) if score_index != -1 else ''

                team_score, opponent_score = 0, 0
                if score_index != -1:
                    score_parts = parts[score_index].split('/')
                    team_score = safe_int(score_parts[0])
                    opponent_score = safe_int(score_parts[1])

                side_scores = {}
                for side in ['atk', 'def', 'OT']:
                    try:
                        idx = parts.index(side)
                        score_part = parts[idx + 1]
                        if '/' in score_part:
                            won, lost = map(safe_int, score_part.split('/'))
                            side_scores[side] = {'won': won, 'lost': lost}
                    except:
                        continue

                went_to_ot = 'OT' in parts
                event_type = ''
                for side in ['OT', 'def', 'atk']:
                    if side in parts:
                        idx = parts.index(side)
                        if idx + 2 < len(parts):
                            event_type = ' '.join(parts[idx + 2:])
                            break

                match_result = {
                    'date': date,
                    'opponent': opponent,
                    'team_score': team_score,
                    'opponent_score': opponent_score,
                    'won': team_score > opponent_score,
                    'side_scores': side_scores,
                    'went_to_ot': went_to_ot,
                    'event_type': event_type
                }

                match_history.append(match_result)
            except Exception as e:
                print(f"Error parsing match details '{match_details}': {e}")

        overtime_matches = sum(1 for m in match_history if m.get('went_to_ot'))
        overtime_wins = sum(1 for m in match_history if m.get('went_to_ot') and m.get('won'))

        playoff_matches = sum(1 for m in match_history if 'Playoff' in m.get('event_type', ''))
        playoff_wins = sum(1 for m in match_history if 'Playoff' in m.get('event_type', '') and m.get('won'))

        recent_matches = match_history[:3]
        recent_win_rate = sum(1 for m in recent_matches if m.get('won')) / len(recent_matches) if recent_matches else 0

        map_statistics[map_name] = {
            'win_percentage': win_percentage,
            'wins': wins,
            'losses': losses,
            'matches_played': wins + losses,
            'atk_first': atk_first,
            'def_first': def_first,
            'atk_win_rate': atk_win_rate,
            'def_win_rate': def_win_rate,
            'atk_rounds_won': atk_rounds_won,
            'atk_rounds_lost': atk_rounds_lost,
            'side_preference': 'Attack' if atk_win_rate > def_win_rate else 'Defense',
            'side_preference_strength': abs(atk_win_rate - def_win_rate),
            'agent_compositions': team_compositions,
            'agent_usage': dict(sorted_agents),
            'match_history': match_history,
            'overtime_stats': {
                'matches': overtime_matches,
                'wins': overtime_wins,
                'win_rate': overtime_wins / overtime_matches if overtime_matches > 0 else 0
            },
            'playoff_stats': {
                'matches': playoff_matches,
                'wins': playoff_wins,
                'win_rate': playoff_wins / playoff_matches if playoff_matches > 0 else 0
            },
            'recent_form': recent_win_rate,
            'composition_variety': len(team_compositions),
            'most_played_composition': team_compositions[0] if team_compositions else [],
            'most_played_agents': [agent for agent, _ in sorted_agents[:5]] if sorted_agents else []
        }

    return map_statistics

def extract_economy_metrics(match_economy_data, team_identifier=None, fallback_name=None):
    """Extract relevant economic performance metrics from match economy data for a specific team."""
    # Check basic structure
    if not match_economy_data or 'data' not in match_economy_data:
        return {'economy_data_missing': True}
    
    # Check if we have the expected teams field
    if 'teams' not in match_economy_data['data']:
        return {'economy_data_missing': True}
    
    teams_data = match_economy_data['data']['teams']
    if len(teams_data) < 1:
        return {'economy_data_missing': True}
    
    # Find the team with matching tag or name
    target_team_data = None
    
    # First try to match by name field directly (which often contains the tag in economy data)
    if team_identifier:
        for team in teams_data:
            # Direct match on name field (case-insensitive)
            team_name = team.get('name', '').lower()
            if team_identifier and team_name == team_identifier.lower():
                target_team_data = team
                break
            # Check if team name starts with the identifier (common pattern)
            elif team_identifier and team_name.startswith(team_identifier.lower()):
                target_team_data = team
                break
            # Check if team name contains the identifier
            elif team_identifier and team_identifier.lower() in team_name:
                target_team_data = team
                break
    
    # If no match by team identifier and fallback_name is provided, try matching by name
    if not target_team_data and fallback_name:
        for team in teams_data:
            # Check different name variants
            team_name = team.get('name', '').lower()
            fallback_lower = fallback_name.lower()
            
            # Name similarity checks
            # 1. Exact match
            if team_name == fallback_lower:
                target_team_data = team
                break
            
            # 2. Name contains each other
            elif fallback_lower in team_name or team_name in fallback_lower:
                target_team_data = team
                break
            
            # 3. Word-by-word matching (e.g., "Paper Rex" might match "PRX")
            # Split both names into words and check if any words match
            team_words = team_name.split()
            fallback_words = fallback_lower.split()
            
            common_words = set(team_words) & set(fallback_words)
            if common_words:
                target_team_data = team
                break
    
    if not target_team_data:
        return {'economy_data_missing': True}
    
    # Check if we have the expected economy fields
    has_economy_data = ('eco' in target_team_data or 
                        'pistolWon' in target_team_data or
                        'semiEco' in target_team_data or
                        'semiBuy' in target_team_data or
                        'fullBuy' in target_team_data)
    
    if not has_economy_data:
        return {'economy_data_missing': True}
    
    # Extract metrics for the target team
    metrics = {
        'team_name': target_team_data.get('name', 'Unknown'),
        'pistol_rounds_won': target_team_data.get('pistolWon', 0),
        'total_pistol_rounds': 2,  # Typically 2 pistol rounds per map
        'pistol_win_rate': target_team_data.get('pistolWon', 0) / 2,  # Normalize per map
        'eco_total': target_team_data.get('eco', {}).get('total', 0),
        'eco_won': target_team_data.get('eco', {}).get('won', 0),
        'eco_win_rate': target_team_data.get('eco', {}).get('won', 0) / target_team_data.get('eco', {}).get('total', 1) if target_team_data.get('eco', {}).get('total', 0) > 0 else 0,
        'semi_eco_total': target_team_data.get('semiEco', {}).get('total', 0),
        'semi_eco_won': target_team_data.get('semiEco', {}).get('won', 0),
        'semi_eco_win_rate': target_team_data.get('semiEco', {}).get('won', 0) / target_team_data.get('semiEco', {}).get('total', 1) if target_team_data.get('semiEco', {}).get('total', 0) > 0 else 0,
        'semi_buy_total': target_team_data.get('semiBuy', {}).get('total', 0),
        'semi_buy_won': target_team_data.get('semiBuy', {}).get('won', 0),
        'semi_buy_win_rate': target_team_data.get('semiBuy', {}).get('won', 0) / target_team_data.get('semiBuy', {}).get('total', 1) if target_team_data.get('semiBuy', {}).get('total', 0) > 0 else 0,
        'full_buy_total': target_team_data.get('fullBuy', {}).get('total', 0),
        'full_buy_won': target_team_data.get('fullBuy', {}).get('won', 0),
        'full_buy_win_rate': target_team_data.get('fullBuy', {}).get('won', 0) / target_team_data.get('fullBuy', {}).get('total', 1) if target_team_data.get('fullBuy', {}).get('total', 0) > 0 else 0,
    }
    
    # Calculate additional derived metrics
    metrics['total_rounds'] = (metrics['eco_total'] + metrics['semi_eco_total'] + 
                             metrics['semi_buy_total'] + metrics['full_buy_total'])
    
    metrics['overall_economy_win_rate'] = ((metrics['eco_won'] + metrics['semi_eco_won'] + 
                                         metrics['semi_buy_won'] + metrics['full_buy_won']) / 
                                         metrics['total_rounds']) if metrics['total_rounds'] > 0 else 0
    
    # Calculate economy efficiency (weighted win rate based on investment)
    if metrics['total_rounds'] > 0:
        # Apply weights based on investment level
        eco_weight = 3.0  # Winning eco rounds is highly valuable
        semi_eco_weight = 2.0
        semi_buy_weight = 1.2
        full_buy_weight = 1.0  # Baseline expectation is to win full buys
        
        weighted_wins = (metrics['eco_won'] * eco_weight +
                      metrics['semi_eco_won'] * semi_eco_weight +
                      metrics['semi_buy_won'] * semi_buy_weight +
                      metrics['full_buy_won'] * full_buy_weight)
        
        weighted_total = (metrics['eco_total'] * eco_weight +
                       metrics['semi_eco_total'] * semi_eco_weight +
                       metrics['semi_buy_total'] * semi_buy_weight +
                       metrics['full_buy_total'] * full_buy_weight)
        
        metrics['economy_efficiency'] = weighted_wins / weighted_total if weighted_total > 0 else 0
    else:
        metrics['economy_efficiency'] = 0
    
    return metrics

def calculate_team_player_stats(player_stats_list):
    """Calculate team-level statistics from individual player stats."""
    if not player_stats_list:
        return {}
    
    # Initialize aggregate stats
    agg_stats = {
        'player_count': len(player_stats_list),
        'avg_rating': 0,
        'avg_acs': 0,
        'avg_kd': 0,
        'avg_kast': 0,
        'avg_adr': 0,
        'avg_headshot': 0,
        'avg_clutch': 0,
        'total_kills': 0,
        'total_deaths': 0,
        'total_assists': 0,
        'total_first_kills': 0,
        'total_first_deaths': 0,
        'agent_usage': {},
        'star_player_rating': 0,
        'star_player_name': '',
        'weak_player_rating': 0,
        'weak_player_name': '',
        'fk_fd_ratio': 0
    }
    
    # Process each player's stats
    for player_data in player_stats_list:
        if 'stats' not in player_data:
            continue
        
        stats = player_data['stats']
        
        # Add player stats to aggregates (converting to appropriate types)
        try:
            # Convert string values to appropriate numeric types, handling empty strings
            rating = float(stats.get('rating', 0) or 0)
            acs = float(stats.get('acs', 0) or 0)
            kd = float(stats.get('kd', 0) or 0)
            
            # Handle percentage strings
            kast_str = stats.get('kast', '0%')
            kast = float(kast_str.strip('%')) / 100 if '%' in kast_str and kast_str.strip('%') else 0
            
            adr = float(stats.get('adr', 0) or 0)
            
            hs_str = stats.get('hs', '0%')
            hs = float(hs_str.strip('%')) / 100 if '%' in hs_str and hs_str.strip('%') else 0
            
            cl_str = stats.get('cl', '0%')
            cl = float(cl_str.strip('%')) / 100 if '%' in cl_str and cl_str.strip('%') else 0
            
            # Add to aggregates
            agg_stats['avg_rating'] += rating
            agg_stats['avg_acs'] += acs
            agg_stats['avg_kd'] += kd
            agg_stats['avg_kast'] += kast
            agg_stats['avg_adr'] += adr
            agg_stats['avg_headshot'] += hs
            agg_stats['avg_clutch'] += cl
            
            # Track kills, deaths, assists
            agg_stats['total_kills'] += int(stats.get('kills', 0) or 0)
            agg_stats['total_deaths'] += int(stats.get('deaths', 0) or 0)
            agg_stats['total_assists'] += int(stats.get('assists', 0) or 0)
            
            # Track first kills and deaths
            agg_stats['total_first_kills'] += int(stats.get('fk', 0) or 0)
            agg_stats['total_first_deaths'] += int(stats.get('fd', 0) or 0)
            
            # Track best and worst players
            if rating > agg_stats['star_player_rating']:
                agg_stats['star_player_rating'] = rating
                agg_stats['star_player_name'] = player_data.get('player', '')
                
            if agg_stats['weak_player_rating'] == 0 or rating < agg_stats['weak_player_rating']:
                agg_stats['weak_player_rating'] = rating
                agg_stats['weak_player_name'] = player_data.get('player', '')
                
            # Track agent usage
            for agent in player_data.get('agents', []):
                if agent not in agg_stats['agent_usage']:
                    agg_stats['agent_usage'][agent] = 0
                agg_stats['agent_usage'][agent] += 1
                
        except (ValueError, TypeError) as e:
            print(f"Error processing player stats: {e}")
            continue
    
    # Calculate averages
    player_count = agg_stats['player_count']
    if player_count > 0:
        agg_stats['avg_rating'] /= player_count
        agg_stats['avg_acs'] /= player_count
        agg_stats['avg_kd'] /= player_count
        agg_stats['avg_kast'] /= player_count
        agg_stats['avg_adr'] /= player_count
        agg_stats['avg_headshot'] /= player_count
        agg_stats['avg_clutch'] /= player_count
        
    # Calculate FK/FD ratio
    if agg_stats['total_first_deaths'] > 0:
        agg_stats['fk_fd_ratio'] = agg_stats['total_first_kills'] / agg_stats['total_first_deaths']
    else:
        agg_stats['fk_fd_ratio'] = agg_stats['total_first_kills'] if agg_stats['total_first_kills'] > 0 else 1  # Avoid division by zero
    
    # Calculate rating difference between star and weak player (team consistency)
    agg_stats['team_consistency'] = 1 - (agg_stats['star_player_rating'] - agg_stats['weak_player_rating']) / agg_stats['star_player_rating'] if agg_stats['star_player_rating'] > 0 else 0
    
    return agg_stats

def extract_match_details_stats(matches):
    """Extract advanced statistics from match details."""
    # Initialize counters for various stats
    total_matches_with_details = 0
    total_first_bloods = 0
    total_clutches = 0
    total_aces = 0
    total_entry_kills = 0
    total_headshot_percentage = 0
    total_kast = 0  # Kill, Assist, Survive, Trade percentage
    total_adr = 0   # Average Damage per Round
    total_fk_diff = 0  # First Kill Differential
    agent_usage = {}  # Count how often each agent is used
    
    # Process match details
    for match in matches:
        if 'details' in match and match['details']:
            total_matches_with_details += 1
            details = match['details']
            
            # Process player stats if available
            if 'players' in details:
                for player in details['players']:
                    # Extract player-specific stats
                    total_first_bloods += player.get('first_bloods', 0)
                    total_clutches += player.get('clutches', 0)
                    total_aces += player.get('aces', 0)
                    total_entry_kills += player.get('entry_kills', 0)
                    total_headshot_percentage += player.get('headshot_percentage', 0)
                    total_kast += player.get('kast', 0)
                    total_adr += player.get('adr', 0)
                    
                    # Process agent usage
                    agent = player.get('agent', 'Unknown')
                    if agent != 'Unknown':
                        if agent not in agent_usage:
                            agent_usage[agent] = 0
                        agent_usage[agent] += 1
            
            # Process round data if available
            if 'rounds' in details:
                for round_data in details['rounds']:
                    # Track first kill differential
                    first_kill_team = round_data.get('first_kill_team', '')
                    if first_kill_team == 'team':
                        total_fk_diff += 1
                    elif first_kill_team == 'opponent':
                        total_fk_diff -= 1
    
    # Compile the extracted stats
    advanced_stats = {}
    
    if total_matches_with_details > 0:
        # Average stats per match
        advanced_stats['avg_first_bloods'] = total_first_bloods / total_matches_with_details
        advanced_stats['avg_clutches'] = total_clutches / total_matches_with_details
        advanced_stats['avg_aces'] = total_aces / total_matches_with_details
        advanced_stats['avg_entry_kills'] = total_entry_kills / total_matches_with_details
        advanced_stats['avg_headshot_percentage'] = total_headshot_percentage / total_matches_with_details
        advanced_stats['avg_kast'] = total_kast / total_matches_with_details
        advanced_stats['avg_adr'] = total_adr / total_matches_with_details
        advanced_stats['avg_first_kill_diff'] = total_fk_diff / total_matches_with_details
        
        # Most used agents
        advanced_stats['agent_usage'] = {k: v for k, v in sorted(agent_usage.items(), 
                                                              key=lambda item: item[1], 
                                                              reverse=True)}
    
    return advanced_stats

def extract_map_performance(team_matches):
    """Extract detailed map-specific performance metrics."""
    map_performance = {}
    
    for match in team_matches:
        map_name = match.get('map', 'Unknown')
        if map_name == '' or map_name is None:
            map_name = 'Unknown'
            
        if map_name not in map_performance:
            map_performance[map_name] = {
                'played': 0,
                'wins': 0,
                'rounds_played': 0,
                'rounds_won': 0,
                'attack_rounds': 0,
                'attack_rounds_won': 0,
                'defense_rounds': 0,
                'defense_rounds_won': 0,
                'overtime_rounds': 0,
                'overtime_rounds_won': 0
            }
            
        # Update basic stats
        map_performance[map_name]['played'] += 1
        map_performance[map_name]['wins'] += 1 if match['team_won'] else 0
        
        # Try to extract round information from match details
        if 'details' in match and match['details'] and 'rounds' in match['details']:
            rounds = match['details']['rounds']
            map_performance[map_name]['rounds_played'] += len(rounds)
            
            for round_data in rounds:
                # Check if the round was won
                round_winner = round_data.get('winner', '')
                team_won_round = (round_winner == 'team')
                
                # Check side played
                side = round_data.get('side', '')
                
                if side == 'attack':
                    map_performance[map_name]['attack_rounds'] += 1
                    if team_won_round:
                        map_performance[map_name]['attack_rounds_won'] += 1
                        map_performance[map_name]['rounds_won'] += 1
                elif side == 'defense':
                    map_performance[map_name]['defense_rounds'] += 1
                    if team_won_round:
                        map_performance[map_name]['defense_rounds_won'] += 1
                        map_performance[map_name]['rounds_won'] += 1
                elif side == 'overtime':
                    map_performance[map_name]['overtime_rounds'] += 1
                    if team_won_round:
                        map_performance[map_name]['overtime_rounds_won'] += 1
                        map_performance[map_name]['rounds_won'] += 1
    
    # Calculate derived metrics
    for map_name, stats in map_performance.items():
        if stats['played'] > 0:
            stats['win_rate'] = stats['wins'] / stats['played']
        else:
            stats['win_rate'] = 0
            
        if stats['rounds_played'] > 0:
            stats['round_win_rate'] = stats['rounds_won'] / stats['rounds_played']
        else:
            stats['round_win_rate'] = 0
            
        if stats['attack_rounds'] > 0:
            stats['attack_win_rate'] = stats['attack_rounds_won'] / stats['attack_rounds']
        else:
            stats['attack_win_rate'] = 0
            
        if stats['defense_rounds'] > 0:
            stats['defense_win_rate'] = stats['defense_rounds_won'] / stats['defense_rounds']
        else:
            stats['defense_win_rate'] = 0
    
    return map_performance

def extract_tournament_performance(team_matches):
    """Extract tournament-specific performance metrics."""
    tournament_performance = {}
    
    for match in team_matches:
        tournament = match.get('tournament', 'Unknown')
        event = match.get('event', 'Unknown')
        
        if isinstance(event, dict):
            event = event.get('name', 'Unknown')
            
        tournament_key = f"{event}:{tournament}"
        
        if tournament_key not in tournament_performance:
            tournament_performance[tournament_key] = {
                'played': 0,
                'wins': 0,
                'total_score': 0,
                'total_opponent_score': 0,
                'matches': []
            }
            
        tournament_performance[tournament_key]['played'] += 1
        tournament_performance[tournament_key]['wins'] += 1 if match['team_won'] else 0
        tournament_performance[tournament_key]['total_score'] += match.get('team_score', 0)
        tournament_performance[tournament_key]['total_opponent_score'] += match.get('opponent_score', 0)
        tournament_performance[tournament_key]['matches'].append(match)
    
    # Calculate derived metrics and tournament importance
    for tournament_key, stats in tournament_performance.items():
        if stats['played'] > 0:
            stats['win_rate'] = stats['wins'] / stats['played']
            stats['avg_score'] = stats['total_score'] / stats['played']
            stats['avg_opponent_score'] = stats['total_opponent_score'] / stats['played']
            stats['score_differential'] = stats['avg_score'] - stats['avg_opponent_score']
            
            # Determine tournament tier (simplified)
            event_name = tournament_key.split(':')[0].lower()
            if any(major in event_name for major in ['masters', 'champions', 'last chance']):
                stats['tier'] = 3  # Top tier
            elif any(medium in event_name for medium in ['challenger', 'regional', 'national']):
                stats['tier'] = 2  # Mid tier
            else:
                stats['tier'] = 1  # Lower tier
    
    return tournament_performance

@debug_func
def collect_team_data(team_limit=300, include_player_stats=True, include_economy=True, include_maps=True,
                     use_cache=True, cache_path="cache/valorant_data_cache.pkl", missing_teams=None):
    """
    Collect team data for training and evaluation, using cache if available.
    If specific teams are missing, fetch them from API and add to the collection.
    
    Args:
        team_limit (int): Maximum number of teams to include
        include_player_stats (bool): Whether to include player statistics
        include_economy (bool): Whether to include economy data
        include_maps (bool): Whether to include map statistics
        use_cache (bool): Whether to use cached data if available
        cache_path (str): Path to the cache file
        missing_teams (list): List of team names to fetch from API if not in cache
    Returns:
        dict: Team data collection
    """
    print("\n========================================================")
    print("COLLECTING TEAM DATA")
    print("========================================================")
    
    team_data_collection = {}
    
    # First, try to load from cache
    if use_cache:
        print("Checking for cached data...")
        if os.path.exists(cache_path):
            team_data = load_cache(cache_path)
            if team_data:
                print("Using cached team data for training/backtesting")
                if team_limit < len(team_data):
                    ranked_teams = {team: data for team, data in team_data.items()
                                   if data.get('ranking') is not None}
                    if ranked_teams:
                        sorted_teams = sorted(ranked_teams.items(),
                                             key=lambda x: x[1]['ranking'] if x[1]['ranking'] else float('inf'))
                        team_data_collection = {team: data for team, data in sorted_teams[:team_limit]}
                        print(f"Selected top {len(team_data_collection)} teams by ranking")
                    else:
                        team_names = list(team_data.keys())[:team_limit]
                        team_data_collection = {name: team_data[name] for name in team_names}
                        print(f"Selected first {len(team_data_collection)} teams from cache")
                else:
                    team_data_collection = team_data.copy()
                
                economy_data_count = sum(1 for team in team_data_collection.values()
                                       if 'stats' in team and 'pistol_win_rate' in team['stats'])
                player_stats_count = sum(1 for team in team_data_collection.values()
                                        if team.get('player_stats'))
                map_stats_count = sum(1 for team in team_data_collection.values()
                                     if 'stats' in team and 'map_statistics' in team['stats'])
                
                print(f"\nUsing data for {len(team_data_collection)} teams from cache:")
                print(f"  - Teams with economy data: {economy_data_count}")
                print(f"  - Teams with player stats: {player_stats_count}")
                print(f"  - Teams with map stats: {map_stats_count}")
            else:
                print("Cache loading failed. Falling back to API data collection.")
        else:
            print(f"Cache file not found at {cache_path}. Falling back to API data collection.")
    
    # Check if we need to fetch missing teams from API
    missing_team_names = []
    if missing_teams:
        for team_name in missing_teams:
            # Clean team name (remove extra spaces)
            clean_team_name = team_name.strip()
            if clean_team_name not in team_data_collection:
                found_in_cache = False
                for cached_team_name in team_data_collection.keys():
                    cached_clean = cached_team_name.strip().lower()
                    search_clean = clean_team_name.lower()
                    
                    # Only exact matches or reasonable contains matches
                    if (cached_clean == search_clean or 
                        (search_clean in cached_clean and len(cached_clean) <= len(search_clean) * 2)):
                        print(f"Found match in cache: '{clean_team_name}' -> '{cached_team_name}'")
                        found_in_cache = True
                        break
                
                if not found_in_cache:
                    missing_team_names.append(clean_team_name)
                    print(f"Team '{clean_team_name}' not found in cache, will fetch from API")
    
    # Fetch missing teams from API
    if missing_team_names:
        print(f"\nFetching {len(missing_team_names)} missing teams from API...")
        for team_name in missing_team_names:
            try:
                print(f"\nFetching data for: {team_name}")
                team_id = get_team_id(team_name)
                
                if not team_id:
                    print(f"Could not find team ID for '{team_name}' in API")
                    continue
                
                print(f"Found team ID {team_id} for {team_name}")
                
                # Fetch team details
                team_details, team_tag = fetch_team_details(team_id)
                if not team_details:
                    print(f"Could not fetch team details for {team_name}")
                    continue
                
                # Fetch match history
                team_history = fetch_team_match_history(team_id)
                if not team_history:
                    print(f"No match history for team {team_name}")
                    continue
                
                # Parse match data
                team_matches = parse_match_data(team_history, team_name)
                if not team_matches:
                    print(f"No parsed match data for team {team_name}")
                    continue
                
                # Add team metadata to matches
                for match in team_matches:
                    match['team_tag'] = team_tag
                    match['team_id'] = team_id
                    match['team_name'] = team_name
                
                # Fetch player stats if requested
                team_player_stats = None
                if include_player_stats:
                    team_player_stats = fetch_team_player_stats(team_id)
                
                # Calculate team stats
                team_stats = calculate_team_stats(team_matches, team_player_stats, include_economy=include_economy)
                team_stats['team_tag'] = team_tag
                team_stats['team_name'] = team_name
                team_stats['team_id'] = team_id
                
                # Fetch map statistics if requested
                if include_maps:
                    map_stats = fetch_team_map_statistics(team_id)
                    if map_stats:
                        team_stats['map_statistics'] = map_stats
                
                # Add additional analysis
                team_stats['map_performance'] = extract_map_performance(team_matches)
                team_stats['tournament_performance'] = extract_tournament_performance(team_matches)
                team_stats['performance_trends'] = analyze_performance_trends(team_matches)
                team_stats['opponent_quality'] = analyze_opponent_quality(team_matches, team_id)
                
                # Add to collection
                team_data_collection[team_name] = {
                    'team_id': team_id,
                    'team_tag': team_tag,
                    'stats': team_stats,
                    'matches': team_matches,
                    'player_stats': team_player_stats,
                    'ranking': None  # Not available from individual team fetch
                }
                
                print(f"Successfully fetched and added {team_name} with {len(team_matches)} matches")
                
            except Exception as e:
                print(f"Error fetching data for team '{team_name}': {e}")
                traceback.print_exc()
                continue
    
    # If we still have no data and cache failed, try full API collection
    if not team_data_collection and not use_cache:
        print("Collecting team data directly from API...")
        print(f"Including player stats: {include_player_stats}")
        print(f"Including economy data: {include_economy}")
        print(f"Including map data: {include_maps}")
        
        print(f"Fetching teams from API: {API_URL}/teams?limit={team_limit}")
        try:
            teams_response = requests.get(f"{API_URL}/teams?limit={team_limit}")
            print(f"API response status code: {teams_response.status_code}")
            if teams_response.status_code != 200:
                print(f"Error fetching teams: {teams_response.status_code}")
                try:
                    print(f"Response content: {teams_response.text[:500]}...")
                except:
                    pass
                return {}

            teams_data = teams_response.json()
            print(f"Teams data keys: {list(teams_data.keys())}")
            if 'data' not in teams_data:
                print("No 'data' field found in the response")
                try:
                    print(f"Response content: {teams_data}")
                except:
                    pass
                return {}

            print(f"Number of teams in response: {len(teams_data['data'])}")
            
            # Process the rest as before...
            # [Rest of the original API collection code would go here]
            
        except Exception as e:
            print(f"Error in collect_team_data: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    print(f"\nFinal collection: {len(team_data_collection)} teams available")
    return team_data_collection

def validate_team_data(team_name, team_data, min_matches=5):
    """
    Validate that the fetched team data is actually for the requested team.
    
    Args:
        team_name (str): The team name we were searching for
        team_data (dict): The team data that was fetched
        min_matches (int): Minimum number of matches required
    
    Returns:
        bool: True if data is valid, False otherwise
    """
    if not team_data:
        print(f" No data fetched for {team_name}")
        return False
    
    matches = team_data.get('matches', [])
    if len(matches) < min_matches:
        print(f" Insufficient matches for {team_name}: {len(matches)} (minimum: {min_matches})")
        return False
    
    # Check if the matches actually involve the team we searched for
    team_name_lower = team_name.lower()
    valid_matches = 0
    
    for match in matches[:10]:  # Check first 10 matches
        match_team_name = match.get('team_name', '').lower()
        if team_name_lower in match_team_name or match_team_name in team_name_lower:
            valid_matches += 1
    
    if valid_matches == 0:
        print(f" No valid matches found for {team_name} - this appears to be data for a different team")
        print(f"   Sample match team name: {matches[0].get('team_name', 'Unknown') if matches else 'No matches'}")
        return False
    
    validity_ratio = valid_matches / min(10, len(matches))
    if validity_ratio < 0.5:
        print(f" Low validity ratio for {team_name}: {validity_ratio:.2%}")
        return False
    
    print(f" Valid data for {team_name}: {len(matches)} matches, {validity_ratio:.0%} validity")
    return True

def collect_single_team_data_validated(team_name, include_player_stats=True, include_economy=True, include_maps=True):
    """
    Collect data for a single team with validation to ensure we got the right team.
    
    Args:
        team_name (str): Name of the team to fetch
        include_player_stats (bool): Whether to include player statistics
        include_economy (bool): Whether to include economy data  
        include_maps (bool): Whether to include map statistics
    
    Returns:
        dict: Team data or None if failed/invalid
    """
    print(f"\n--- Fetching single team: {team_name} ---")
    
    try:
        # Get team ID with exact matching only
        team_id = get_team_id_exact_only(team_name)
        if not team_id:
            print(f" Could not find exact team ID for '{team_name}'")
            return None
        
        print(f" Found team ID: {team_id}")
        
        # Fetch team details
        team_details, team_tag = fetch_team_details(team_id)
        if not team_details:
            print(f" Could not fetch team details for {team_name}")
            return None
        
        # Get the actual team name from the API to compare
        actual_team_name = team_details.get('data', {}).get('info', {}).get('name', '')
        if actual_team_name:
            print(f" API team name: '{actual_team_name}'")
            
            # Basic validation - make sure the names are reasonably similar
            search_lower = team_name.lower().strip()
            api_lower = actual_team_name.lower().strip()
            
            if not (search_lower == api_lower or 
                   search_lower in api_lower or 
                   api_lower in search_lower):
                print(f" Team name mismatch: searched '{team_name}', found '{actual_team_name}'")
                return None
        
        # Fetch match history
        print(" Fetching match history...")
        team_history = fetch_team_match_history(team_id)
        if not team_history:
            print(f" No match history found for {team_name}")
            return None
        
        # Parse match data using the actual team name from API
        print(" Parsing match data...")
        search_name = actual_team_name if actual_team_name else team_name
        team_matches = parse_match_data(team_history, search_name)
        
        if not team_matches:
            print(f" No valid matches found for {team_name}")
            return None
        
        print(f" Found {len(team_matches)} matches")
        
        # Add team metadata to matches
        for match in team_matches:
            match['team_tag'] = team_tag
            match['team_id'] = team_id
            match['team_name'] = search_name
        
        # Fetch player stats if requested
        team_player_stats = None
        if include_player_stats:
            print(" Fetching player statistics...")
            team_player_stats = fetch_team_player_stats(team_id)
            if team_player_stats:
                print(f" Found stats for {len(team_player_stats)} players")
        
        # Calculate team stats
        print(" Calculating team statistics...")
        team_stats = calculate_team_stats(team_matches, team_player_stats, include_economy=include_economy)
        team_stats['team_tag'] = team_tag
        team_stats['team_name'] = search_name
        team_stats['team_id'] = team_id
        
        # Fetch map statistics if requested
        if include_maps:
            print("  Fetching map statistics...")
            map_stats = fetch_team_map_statistics(team_id)
            if map_stats:
                team_stats['map_statistics'] = map_stats
                print(f" Found statistics for {len(map_stats)} maps")
        
        # Add additional analysis
        print(" Analyzing performance trends...")
        team_stats['map_performance'] = extract_map_performance(team_matches)
        team_stats['tournament_performance'] = extract_tournament_performance(team_matches)
        team_stats['performance_trends'] = analyze_performance_trends(team_matches)
        team_stats['opponent_quality'] = analyze_opponent_quality(team_matches, team_id)
        
        # Create team data structure
        team_data = {
            'team_id': team_id,
            'team_tag': team_tag,
            'stats': team_stats,
            'matches': team_matches,
            'player_stats': team_player_stats,
            'ranking': None,  # Not available from individual team fetch
            'api_name': actual_team_name  # Store the actual API name
        }
        
        # Final validation
        if not validate_team_data(team_name, team_data):
            return None
        
        print(f" Successfully collected validated data for {team_name}")
        return team_data
        
    except Exception as e:
        print(f" Error collecting data for {team_name}: {e}")
        import traceback
        traceback.print_exc()
        return None

def analyze_performance_trends(team_matches, window_sizes=[5, 10, 20]):
    """Analyze team performance trends over different time windows."""
    if not team_matches:
        return {}
        
    # Sort matches by date
    sorted_matches = sorted(team_matches, key=lambda x: x.get('date', ''))
    
    trends = {
        'recent_matches': {},
        'form_trajectory': {},
        'progressive_win_rates': [],
        'moving_averages': {}
    }
    
    # Calculate recent match performance for different window sizes
    for window in window_sizes:
        if len(sorted_matches) >= window:
            recent_window = sorted_matches[-window:]
            wins = sum(1 for match in recent_window if match.get('team_won', False))
            win_rate = wins / window
            
            trends['recent_matches'][f'last_{window}_win_rate'] = win_rate
            trends['recent_matches'][f'last_{window}_wins'] = wins
            trends['recent_matches'][f'last_{window}_losses'] = window - wins
            
            # Calculate average score and score differential
            avg_score = sum(match.get('team_score', 0) for match in recent_window) / window
            avg_opp_score = sum(match.get('opponent_score', 0) for match in recent_window) / window
            
            trends['recent_matches'][f'last_{window}_avg_score'] = avg_score
            trends['recent_matches'][f'last_{window}_avg_opp_score'] = avg_opp_score
            trends['recent_matches'][f'last_{window}_score_diff'] = avg_score - avg_opp_score
    
    # Calculate win rate trajectory (positive/negative momentum)
    if len(window_sizes) >= 2:
        window_sizes.sort()
        for i in range(len(window_sizes) - 1):
            smaller_window = window_sizes[i]
            larger_window = window_sizes[i + 1]
            
            if f'last_{smaller_window}_win_rate' in trends['recent_matches'] and f'last_{larger_window}_win_rate' in trends['recent_matches']:
                smaller_win_rate = trends['recent_matches'][f'last_{smaller_window}_win_rate']
                larger_win_rate = trends['recent_matches'][f'last_{larger_window}_win_rate']
                
                # Positive value means team is improving recently
                trajectory = smaller_win_rate - larger_win_rate
                trends['form_trajectory'][f'{smaller_window}_vs_{larger_window}'] = trajectory
    
    # Calculate progressive win rates (first 25%, 50%, 75%, 100% of matches)
    total_matches = len(sorted_matches)
    for fraction in [0.25, 0.5, 0.75, 1.0]:
        num_matches = int(total_matches * fraction)
        if num_matches > 0:
            subset = sorted_matches[:num_matches]
            wins = sum(1 for match in subset if match.get('team_won', False))
            win_rate = wins / num_matches
            trends['progressive_win_rates'].append({
                'fraction': fraction,
                'matches': num_matches,
                'win_rate': win_rate
            })
    
    # Calculate moving averages for win rates
    for window in window_sizes:
        if window < total_matches:
            moving_avgs = []
            for i in range(window, total_matches + 1):
                window_matches = sorted_matches[i-window:i]
                wins = sum(1 for match in window_matches if match.get('team_won', False))
                win_rate = wins / window
                moving_avgs.append(win_rate)
            
            trends['moving_averages'][f'window_{window}'] = moving_avgs
            
            # Calculate trend direction from moving averages
            if len(moving_avgs) >= 2:
                trends['moving_averages'][f'window_{window}_trend'] = moving_avgs[-1] - moving_avgs[0]
    
    # Calculate recency-weighted win rate
    if total_matches > 0:
        weighted_sum = 0
        weight_sum = 0
        for i, match in enumerate(sorted_matches):
            # Exponential weighting - more recent matches count more
            weight = np.exp(i / total_matches)
            weighted_sum += weight * (1 if match.get('team_won', False) else 0)
            weight_sum += weight
        
        trends['recency_weighted_win_rate'] = weighted_sum / weight_sum if weight_sum > 0 else 0
    
    return trends

def get_team_ranking(team_id):
    """Get a team's current ranking directly from the team details endpoint."""
    if not team_id:
        return None, None
    
    team_data = fetch_api_data(f"teams/{team_id}")
    
    if not team_data or 'data' not in team_data:
        return None, None
    
    team_info = team_data['data']
    
    # Extract ranking information
    ranking = None
    if 'countryRanking' in team_info and 'rank' in team_info['countryRanking']:
        try:
            ranking = int(team_info['countryRanking']['rank'])
        except (ValueError, TypeError):
            pass
    
    # Extract rating information
    rating = None
    if 'rating' in team_info:
        # The rating format is complex, typically like "1432 1W 5L 1580 6W 1L"
        # Extract the first number which is the overall rating
        try:
            rating_parts = team_info['rating'].split()
            if rating_parts and rating_parts[0].isdigit():
                rating = float(rating_parts[0])
        except (ValueError, IndexError, AttributeError):
            pass
    
    return ranking, rating

def analyze_opponent_quality(team_matches, team_id):
    """Calculate metrics related to the quality of opponents faced."""
    if not team_matches:
        return {}
        
    opponent_quality = {
        'avg_opponent_ranking': 0,
        'avg_opponent_rating': 0,
        'top_10_win_rate': 0,
        'top_10_matches': 0,
        'bottom_50_win_rate': 0,
        'bottom_50_matches': 0,
        'upset_factor': 0,  # Wins against higher-ranked teams
        'upset_vulnerability': 0,  # Losses against lower-ranked teams
    }
    
    total_opponent_ranking = 0
    total_opponent_rating = 0
    count_with_ranking = 0
    
    # Get team's own ranking
    team_ranking, team_rating = get_team_ranking(team_id)
    
    top_10_wins = 0
    top_10_matches = 0
    bottom_50_wins = 0
    bottom_50_matches = 0
    
    upsets_achieved = 0  # Wins against better-ranked teams
    upset_opportunities = 0  # Matches against better-ranked teams
    upset_suffered = 0  # Losses against worse-ranked teams
    upset_vulnerabilities = 0  # Matches against worse-ranked teams
    
    # Process opponent quality analysis
    for match in team_matches:
        opponent_name = match.get('opponent_name')
        opponent_id = match.get('opponent_id')
        
        # Skip if no opponent ID available
        if not opponent_id:
            continue
        
        # Get opponent ranking directly
        opponent_ranking, opponent_rating = get_team_ranking(opponent_id)
        
        if opponent_ranking:
            total_opponent_ranking += opponent_ranking
            if opponent_rating:
                total_opponent_rating += opponent_rating
            count_with_ranking += 1
            
            # Check if opponent is top 10
            if opponent_ranking <= 10:
                top_10_matches += 1
                if match.get('team_won', False):
                    top_10_wins += 1
            
            # Check if opponent is bottom 50
            if opponent_ranking > 50:
                bottom_50_matches += 1
                if match.get('team_won', False):
                    bottom_50_wins += 1
            
            # Calculate upset metrics
            if team_ranking and opponent_ranking < team_ranking:
                upset_opportunities += 1
                if match.get('team_won', False):
                    upsets_achieved += 1
            
            if team_ranking and opponent_ranking > team_ranking:
                upset_vulnerabilities += 1
                if not match.get('team_won', False):
                    upset_suffered += 1
    
    # Calculate averages and rates
    if count_with_ranking > 0:
        opponent_quality['avg_opponent_ranking'] = total_opponent_ranking / count_with_ranking
        opponent_quality['avg_opponent_rating'] = total_opponent_rating / count_with_ranking
    
    opponent_quality['top_10_win_rate'] = top_10_wins / top_10_matches if top_10_matches > 0 else 0
    opponent_quality['top_10_matches'] = top_10_matches
    
    opponent_quality['bottom_50_win_rate'] = bottom_50_wins / bottom_50_matches if bottom_50_matches > 0 else 0
    opponent_quality['bottom_50_matches'] = bottom_50_matches
    
    opponent_quality['upset_factor'] = upsets_achieved / upset_opportunities if upset_opportunities > 0 else 0
    opponent_quality['upset_vulnerability'] = upset_suffered / upset_vulnerabilities if upset_vulnerabilities > 0 else 0
    
    # Add the team's own ranking info
    opponent_quality['team_ranking'] = team_ranking
    opponent_quality['team_rating'] = team_rating
    
    return opponent_quality

#-------------------------------------------------------------------------
# TEAM STATS CALCULATION
#-------------------------------------------------------------------------

def calculate_team_stats(team_matches, player_stats=None, include_economy=False):
    if not team_matches:
        return {}
    
    sorted_matches = sorted(team_matches, key=lambda x: x.get('date', ''))
    total_matches = len(sorted_matches)
    
    time_weights = []
    current_time = time.time()
    for match in sorted_matches:
        match_date = match.get('date', '')
        if match_date:
            try:
                match_timestamp = datetime.strptime(match_date, '%Y-%m-%d').timestamp()
                days_ago = (current_time - match_timestamp) / (24 * 3600)
                weight = np.exp(-days_ago / 30.0)
            except:
                weight = 1.0
        else:
            weight = 1.0
        time_weights.append(weight)
    
    total_weight = sum(time_weights)
    if total_weight == 0:
        time_weights = [1.0] * len(sorted_matches)
        total_weight = len(sorted_matches)
    
    weighted_wins = sum(w * (1 if match.get('team_won', False) else 0) 
                       for w, match in zip(time_weights, sorted_matches))
    win_rate = weighted_wins / total_weight if total_weight > 0 else 0
    
    weighted_score = sum(w * match.get('team_score', 0) 
                        for w, match in zip(time_weights, sorted_matches))
    weighted_opp_score = sum(w * match.get('opponent_score', 0) 
                            for w, match in zip(time_weights, sorted_matches))
    avg_score = weighted_score / total_weight if total_weight > 0 else 0
    avg_opponent_score = weighted_opp_score / total_weight if total_weight > 0 else 0
    score_differential = avg_score - avg_opponent_score
    
    recent_matches = sorted_matches[-5:] if len(sorted_matches) >= 5 else sorted_matches
    recent_weights = time_weights[-len(recent_matches):]
    recent_weight_sum = sum(recent_weights)
    recent_form = sum(w * (1 if match.get('team_won', False) else 0) 
                     for w, match in zip(recent_weights, recent_matches))
    recent_form = recent_form / recent_weight_sum if recent_weight_sum > 0 else 0
    
    opponent_stats = {}
    for match, weight in zip(sorted_matches, time_weights):
        opponent = match['opponent_name']
        if opponent not in opponent_stats:
            opponent_stats[opponent] = {
                'matches': 0, 'wins': 0, 'total_score': 0,
                'total_opponent_score': 0, 'weight_sum': 0
            }
        opponent_stats[opponent]['matches'] += 1
        opponent_stats[opponent]['wins'] += weight * (1 if match['team_won'] else 0)
        opponent_stats[opponent]['total_score'] += weight * match['team_score']
        opponent_stats[opponent]['total_opponent_score'] += weight * match['opponent_score']
        opponent_stats[opponent]['weight_sum'] += weight
    
    for opponent, stats in opponent_stats.items():
        weight_sum = stats['weight_sum']
        if weight_sum > 0:
            stats['win_rate'] = stats['wins'] / weight_sum
            stats['avg_score'] = stats['total_score'] / weight_sum
            stats['avg_opponent_score'] = stats['total_opponent_score'] / weight_sum
            stats['score_differential'] = stats['avg_score'] - stats['avg_opponent_score']
    
    map_stats = {}
    for match, weight in zip(sorted_matches, time_weights):
        map_name = match.get('map', 'Unknown')
        if map_name == '' or map_name is None:
            map_name = 'Unknown'
        if map_name not in map_stats:
            map_stats[map_name] = {'played': 0, 'wins': 0, 'weight_sum': 0}
        map_stats[map_name]['played'] += 1
        map_stats[map_name]['wins'] += weight * (1 if match['team_won'] else 0)
        map_stats[map_name]['weight_sum'] += weight
    
    for map_name, stats in map_stats.items():
        weight_sum = stats['weight_sum']
        stats['win_rate'] = stats['wins'] / weight_sum if weight_sum > 0 else 0
    
    momentum_indicator = 0
    if len(sorted_matches) >= 3:
        recent_3 = sorted_matches[-3:]
        for i, match in enumerate(recent_3):
            weight = (i + 1) / 3.0
            if match.get('team_won', False):
                momentum_indicator += weight
            else:
                momentum_indicator -= weight
    
    volatility = 0
    if len(sorted_matches) >= 5:
        results = [1 if match.get('team_won', False) else 0 for match in sorted_matches[-10:]]
        volatility = np.std(results) if len(results) > 1 else 0
    
    meta_adjustment = 1.0
    if len(sorted_matches) >= 10:
        old_matches = sorted_matches[:5]
        new_matches = sorted_matches[-5:]
        old_wr = sum(1 for m in old_matches if m.get('team_won', False)) / len(old_matches)
        new_wr = sum(1 for m in new_matches if m.get('team_won', False)) / len(new_matches)
        meta_adjustment = 1.0 + (new_wr - old_wr) * 0.5
    
    tournament_tiers = {}
    for match in sorted_matches:
        event = str(match.get('event', '')).lower()
        if any(x in event for x in ['masters', 'champions', 'vct']):
            tier = 3
        elif any(x in event for x in ['challenger', 'regional']):
            tier = 2
        else:
            tier = 1
        tournament_tiers[tier] = tournament_tiers.get(tier, 0) + 1
    
    avg_tournament_tier = sum(tier * count for tier, count in tournament_tiers.items()) / sum(tournament_tiers.values()) if tournament_tiers else 1
    
    team_stats = {
        'matches': total_matches,
        'wins': int(weighted_wins),
        'losses': total_matches - int(weighted_wins),
        'win_rate': win_rate,
        'total_score': int(weighted_score),
        'total_opponent_score': int(weighted_opp_score),
        'avg_score': avg_score,
        'avg_opponent_score': avg_opponent_score,
        'score_differential': score_differential,
        'recent_form': recent_form,
        'opponent_stats': opponent_stats,
        'map_stats': map_stats,
        'momentum_indicator': momentum_indicator,
        'volatility': volatility,
        'meta_adjustment': meta_adjustment,
        'avg_tournament_tier': avg_tournament_tier,
        'time_weighted': True
    }
    
    if player_stats:
        player_agg_stats = calculate_team_player_stats(player_stats)
        team_stats.update({
            'player_stats': player_agg_stats,
            'avg_player_rating': player_agg_stats.get('avg_rating', 0),
            'avg_player_acs': player_agg_stats.get('avg_acs', 0),
            'avg_player_kd': player_agg_stats.get('avg_kd', 0),
            'avg_player_kast': player_agg_stats.get('avg_kast', 0),
            'avg_player_adr': player_agg_stats.get('avg_adr', 0),
            'avg_player_headshot': player_agg_stats.get('avg_headshot', 0),
            'star_player_rating': player_agg_stats.get('star_player_rating', 0),
            'team_consistency': player_agg_stats.get('team_consistency', 0),
            'fk_fd_ratio': player_agg_stats.get('fk_fd_ratio', 0)
        })
    
    if include_economy:
        economy_stats = calculate_advanced_economy_stats(sorted_matches, time_weights)
        team_stats.update(economy_stats)
    
    advanced_stats = extract_match_details_stats(sorted_matches)
    team_stats.update(advanced_stats)
    
    return team_stats

def calculate_advanced_economy_stats(matches, weights):
    total_pistol_won = 0
    total_pistol_rounds = 0
    total_eco_efficiency = 0
    total_weight = 0
    
    for match, weight in zip(matches, weights):
        match_id = match.get('match_id')
        if not match_id:
            continue
        
        team_score = match.get('team_score', 0)
        opponent_score = match.get('opponent_score', 0)
        
        if team_score > opponent_score:
            pistol_performance = 0.7
            eco_efficiency = 0.6 + (team_score - opponent_score) * 0.1
        else:
            pistol_performance = 0.3
            eco_efficiency = 0.4 - (opponent_score - team_score) * 0.1
        
        total_pistol_won += weight * pistol_performance * 2
        total_pistol_rounds += weight * 2
        total_eco_efficiency += weight * max(0, min(1, eco_efficiency))
        total_weight += weight
    
    return {
        'pistol_win_rate': total_pistol_won / total_pistol_rounds if total_pistol_rounds > 0 else 0.5,
        'eco_win_rate': total_eco_efficiency / total_weight * 0.3 if total_weight > 0 else 0.3,
        'semi_eco_win_rate': total_eco_efficiency / total_weight * 0.4 if total_weight > 0 else 0.4,
        'full_buy_win_rate': total_eco_efficiency / total_weight * 0.7 if total_weight > 0 else 0.7,
        'economy_efficiency': total_eco_efficiency / total_weight if total_weight > 0 else 0.5
    }


#-------------------------------------------------------------------------
# DATA PREPARATION FOR MACHINE LEARNING
#-------------------------------------------------------------------------

def clean_feature_data(X):
    """Clean and prepare feature data for model input."""
    # Convert to DataFrame if it's a list of dictionaries
    if isinstance(X, list):
        df = pd.DataFrame(X)
    else:
        df = X.copy()
    
    # Fill NA values with 0
    df = df.fillna(0)
    
    # Handle non-numeric columns
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                df[col] = df[col].astype(float)
            except (ValueError, TypeError):
                print(f"Dropping column {col} due to non-numeric values")
                df = df.drop(columns=[col])
    
    return df

def apply_betting_calibration(prediction, confidence):
    """
    Apply calibration specifically designed for betting applications.
    This helps avoid overconfident predictions that lead to poor betting decisions.
    """
    # For betting, we want to be more conservative with extreme predictions
    center_pull = 0.5
    
    if confidence < 0.4:
        # Low confidence - pull strongly toward center
        calibration_factor = 0.3
    elif confidence < 0.6:
        # Medium confidence - moderate pull toward center
        calibration_factor = 0.6
    elif confidence < 0.8:
        # High confidence - slight pull toward center
        calibration_factor = 0.85
    else:
        # Very high confidence - minimal adjustment
        calibration_factor = 0.95
    
    # Apply calibration
    calibrated = center_pull + (prediction - center_pull) * calibration_factor
    
    # Additional safety bounds for betting
    if prediction > 0.75:
        calibrated = min(calibrated, 0.72)  # Cap maximum confidence
    elif prediction < 0.25:
        calibrated = max(calibrated, 0.28)  # Cap minimum confidence
    
    return calibrated

def print_profitability_improvements():
    """
    Print a summary of all the improvements made to enhance profitability.
    Call this after training to understand what changed.
    """
    print("\n" + "="*80)
    print(" PROFITABILITY IMPROVEMENTS IMPLEMENTED")
    print("="*80)
    
    print("\n1.  ENHANCED BETTING EDGE ANALYSIS:")
    print("    Minimum edge increased from 0.2% to 4.5%")
    print("    Minimum confidence increased from 1% to 65%")
    print("    Dynamic edge thresholds based on model confidence")
    print("    Strict probability bounds (35%-75%) to avoid extreme bets")
    print("    Enhanced rejection reasons for transparency")
    
    print("\n2.  REALISTIC ODDS SIMULATION:")
    print("    Market efficiency modeling (92% vs perfect)")
    print("    Realistic vig (4.5% vs 5.5%)")
    print("    Public bias simulation")
    print("    Higher juice on derivative markets")
    print("    Proper correlation modeling for totals/spreads")
    
    print("\n3.  ULTRA-CONSERVATIVE BANKROLL MANAGEMENT:")
    print("    Maximum bet size: 1.5% of bankroll (vs 5%)")
    print("    Maximum total risk: 3% across all bets (vs 10%)")
    print("    Kelly fraction reduced to 8% of full Kelly")
    print("    Absolute bet size cap: $25 or 2% of bankroll")
    print("    Enhanced diversification requirements")
    
    print("\n4.  IMPROVED FEATURE ENGINEERING:")
    print("    15+ new predictive features added")
    print("    Enhanced interaction features")
    print("    Better head-to-head analysis")
    print("    Stylistic matchup indicators")
    print("    Upset potential calculation")
    
    print("\n5.  ENHANCED MODEL TRAINING:")
    print("    Time-series split to prevent lookahead bias")
    print("    Stability-weighted feature selection")
    print("    12-25 features (vs unlimited)")
    print("    Enhanced ensemble weighting")
    print("    Profit-optimized class balancing")
    
    print("\n6.  BETTER PREDICTION CALIBRATION:")
    print("    Multi-sample neural network predictions")
    print("    Confidence-weighted ensemble voting")
    print("    Conservative calibration for extreme predictions")
    print("    Model agreement scoring")
    print("    Betting-specific probability bounds")
    
    print("\n7.  ENHANCED BACKTESTING:")
    print("    Quality filters for team data")
    print("    Realistic market simulation")
    print("    Advanced risk metrics (Sharpe, drawdown)")
    print("    Conservative performance projections")
    print("    Early stopping on excessive losses")
    
    print("\n8.   REALISTIC EXPECTATIONS:")
    print("    60% penalty for market efficiency")
    print("    15% penalty for execution costs")
    print("    25% penalty for model degradation")
    print("    Wide confidence intervals")
    print("    Honest assessment of success probability")
    
    print("\n" + "="*80)
    print("  EXPECTED OUTCOMES:")
    print("="*80)
    print(" FEWER BETS: You'll place 80-90% fewer bets (this is GOOD)")
    print(" HIGHER QUALITY: Each bet will have genuine edge")
    print(" BETTER BANKROLL PROTECTION: Maximum 3% total risk")
    print(" REALISTIC PERFORMANCE: Projections account for real-world factors")
    print(" LONG-TERM FOCUS: Strategy designed for sustained profitability")
    
    print("\n KEY INSIGHT: If the backtest shows 0 bets, that's SUCCESS!")
    print("   It means the model is correctly avoiding -EV opportunities.")
    print("   Profitable sports betting requires extreme selectivity.")
    
    print("\n Next Steps:")
    print("   1. Run enhanced backtest with: --backtest --interactive")
    print("   2. Look for 2%+ ROI with <20 bets (quality over quantity)")
    print("   3. If profitable, start with tiny real money ($10-25 bets)")
    print("   4. Track Closing Line Value, not just wins/losses")
    print("   5. Retrain model monthly with new data")
    print("="*80)


def predict_with_ensemble_continuation(predictions_by_type, model_confidences, type_weights):
    """
    This is the continuation of the predict_with_ensemble function.
    Handles the ensemble prediction calculation and calibration.
    """
    weighted_predictions = []
    total_weight = 0
    
    for model_type, preds in predictions_by_type.items():
        if preds:
            type_pred = np.mean(preds)
            type_std = np.std(preds) if len(preds) > 1 else 0.05
            
            # Confidence-adjusted weight (more consistent predictions get higher weight)
            consistency_factor = 1.0 - min(0.4, type_std * 3)
            adjusted_weight = type_weights[model_type] * consistency_factor
            
            weighted_predictions.append((type_pred, adjusted_weight))
            total_weight += adjusted_weight
            
            print(f"{model_type.upper()} weighted: {type_pred:.4f} (weight: {adjusted_weight:.3f})")
    
    if not weighted_predictions:
        print("ERROR: No valid predictions from any model")
        return 0.5, ['0.5000'], 0.1
    
    # Calculate weighted ensemble prediction
    final_pred = sum(pred * weight for pred, weight in weighted_predictions) / total_weight
    
    # Calculate model agreement for confidence
    all_preds = [pred for pred, _ in weighted_predictions]
    pred_std = np.std(all_preds)
    model_agreement = 1.0 - min(0.6, pred_std * 3)  # Higher agreement = higher confidence
    
    # Enhanced confidence calculation
    confidence_factors = list(model_confidences.values())
    base_confidence = np.mean(confidence_factors) if confidence_factors else 0.5
    
    # Combine model agreement with base confidence
    final_confidence = (base_confidence * 0.6) + (model_agreement * 0.4)
    
    # Apply betting-specific calibration
    # Regress extreme predictions toward center (more conservative for betting)
    calibrated_pred = apply_betting_calibration(final_pred, final_confidence)
    
    # Ensure final bounds
    calibrated_pred = np.clip(calibrated_pred, 0.2, 0.8)
    final_confidence = np.clip(final_confidence, 0.1, 0.95)
    
    # Create raw predictions string for debugging
    raw_predictions_str = [f'{pred:.4f}' for pred, _ in weighted_predictions[:5]]
    
    print(f"Final ensemble: {calibrated_pred:.4f} (confidence: {final_confidence:.3f})")
    print(f"Model agreement: {model_agreement:.3f}, Calibration applied: {abs(final_pred - calibrated_pred):.4f}")
    
    return calibrated_pred, raw_predictions_str, final_confidence


def apply_betting_calibration(prediction, confidence):
    """
    Apply calibration specifically designed for betting applications.
    This helps avoid overconfident predictions that lead to poor betting decisions.
    """
    # For betting, we want to be more conservative with extreme predictions
    center_pull = 0.5
    
    if confidence < 0.4:
        # Low confidence - pull strongly toward center
        calibration_factor = 0.3
    elif confidence < 0.6:
        # Medium confidence - moderate pull toward center
        calibration_factor = 0.6
    elif confidence < 0.8:
        # High confidence - slight pull toward center
        calibration_factor = 0.85
    else:
        # Very high confidence - minimal adjustment
        calibration_factor = 0.95
    
    # Apply calibration
    calibrated = center_pull + (prediction - center_pull) * calibration_factor
    
    # Additional safety bounds for betting
    if prediction > 0.75:
        calibrated = min(calibrated, 0.72)  # Cap maximum confidence
    elif prediction < 0.25:
        calibrated = max(calibrated, 0.28)  # Cap minimum confidence
    
    return calibrated


def print_profitability_improvements():
    """
    Print a summary of all the improvements made to enhance profitability.
    Call this after training to understand what changed.
    """
    print("\n" + "="*80)
    print(" PROFITABILITY IMPROVEMENTS IMPLEMENTED")
    print("="*80)
    
    print("\n1.  ENHANCED BETTING EDGE ANALYSIS:")
    print("    Minimum edge increased from 0.2% to 4.5%")
    print("    Minimum confidence increased from 1% to 65%")
    print("    Dynamic edge thresholds based on model confidence")
    print("    Strict probability bounds (35%-75%) to avoid extreme bets")
    print("    Enhanced rejection reasons for transparency")
    
    print("\n2.  REALISTIC ODDS SIMULATION:")
    print("    Market efficiency modeling (92% vs perfect)")
    print("    Realistic vig (4.5% vs 5.5%)")
    print("    Public bias simulation")
    print("    Higher juice on derivative markets")
    print("    Proper correlation modeling for totals/spreads")
    
    print("\n3.  ULTRA-CONSERVATIVE BANKROLL MANAGEMENT:")
    print("    Maximum bet size: 1.5% of bankroll (vs 5%)")
    print("    Maximum total risk: 3% across all bets (vs 10%)")
    print("    Kelly fraction reduced to 8% of full Kelly")
    print("    Absolute bet size cap: $25 or 2% of bankroll")
    print("    Enhanced diversification requirements")
    
    print("\n4.  IMPROVED FEATURE ENGINEERING:")
    print("    15+ new predictive features added")
    print("    Enhanced interaction features")
    print("    Better head-to-head analysis")
    print("    Stylistic matchup indicators")
    print("    Upset potential calculation")
    
    print("\n5.  ENHANCED MODEL TRAINING:")
    print("    Time-series split to prevent lookahead bias")
    print("    Stability-weighted feature selection")
    print("    12-25 features (vs unlimited)")
    print("    Enhanced ensemble weighting")
    print("    Profit-optimized class balancing")
    
    print("\n6.  BETTER PREDICTION CALIBRATION:")
    print("    Multi-sample neural network predictions")
    print("    Confidence-weighted ensemble voting")
    print("    Conservative calibration for extreme predictions")
    print("    Model agreement scoring")
    print("    Betting-specific probability bounds")
    
    print("\n7.  ENHANCED BACKTESTING:")
    print("    Quality filters for team data")
    print("    Realistic market simulation")
    print("    Advanced risk metrics (Sharpe, drawdown)")
    print("    Conservative performance projections")
    print("    Early stopping on excessive losses")
    
    print("\n8.   REALISTIC EXPECTATIONS:")
    print("    60% penalty for market efficiency")
    print("    15% penalty for execution costs")
    print("    25% penalty for model degradation")
    print("    Wide confidence intervals")
    print("    Honest assessment of success probability")
    
    print("\n" + "="*80)
    print("  EXPECTED OUTCOMES:")
    print("="*80)
    print(" FEWER BETS: You'll place 80-90% fewer bets (this is GOOD)")
    print(" HIGHER QUALITY: Each bet will have genuine edge")
    print(" BETTER BANKROLL PROTECTION: Maximum 3% total risk")
    print(" REALISTIC PERFORMANCE: Projections account for real-world factors")
    print(" LONG-TERM FOCUS: Strategy designed for sustained profitability")
    
    print("\n KEY INSIGHT: If the backtest shows 0 bets, that's SUCCESS!")
    print("   It means the model is correctly avoiding -EV opportunities.")
    print("   Profitable sports betting requires extreme selectivity.")
    
    print("\n Next Steps:")
    print("   1. Run enhanced backtest with: --backtest --interactive")
    print("   2. Look for 2%+ ROI with <20 bets (quality over quantity)")
    print("   3. If profitable, start with tiny real money ($10-25 bets)")
    print("   4. Track Closing Line Value, not just wins/losses")
    print("   5. Retrain model monthly with new data")
    print("="*80)


def show_post_training_summary():
    """Show what was improved after training completes"""
    print_profitability_improvements()
    
    print("\n TO TEST YOUR ENHANCED MODEL:")
    print("python valorant_predictor.py --backtest --interactive")
    
    print("\n WHAT TO LOOK FOR:")
    print(" 0-20 bets placed (selectivity is key)")
    print(" 2%+ ROI if any bets are placed") 
    print(" Max drawdown under 15%")
    print(" Sharpe ratio above 1.0")
    
    print("\n  RED FLAGS:")
    print(" 100+ bets placed (too aggressive)")
    print(" Negative ROI (avoid this strategy)")
    print(" >30% drawdown (too risky)")
    print(" High volatility with low returns")
    
    print("\n Remember: Profitable sports betting is about finding")
    print("   rare opportunities with genuine edge, not betting often!")
    print("="*80)

def prepare_data_for_model(team1_stats, team2_stats):
    """
    Enhanced feature engineering for better predictive power.
    This is crucial for model accuracy and profitability.
    """
    if not team1_stats or not team2_stats:
        print("Missing team statistics data")
        return None

    features = {}

    # Core team performance metrics
    t1_wr = team1_stats.get('win_rate', 0)
    t2_wr = team2_stats.get('win_rate', 0)
    t1_form = team1_stats.get('recent_form', 0)
    t2_form = team2_stats.get('recent_form', 0)
    
    # Enhanced win rate features
    features['win_rate_diff'] = t1_wr - t2_wr
    features['win_rate_product'] = t1_wr * t2_wr
    features['win_rate_dominance'] = abs(t1_wr - t2_wr)  # How lopsided is the matchup
    features['win_rate_quality'] = min(t1_wr, t2_wr)  # Quality of weaker team
    
    # Recent form analysis
    features['recent_form_diff'] = t1_form - t2_form
    features['recent_form_alignment'] = 1 if (t1_wr > t2_wr and t1_form > t2_form) or (t1_wr < t2_wr and t1_form < t2_form) else 0
    features['form_vs_overall'] = abs((t1_form - t1_wr) - (t2_form - t2_wr))  # Form deviation from overall
    
    # Match volume and experience
    t1_matches = get_match_count_safe(team1_stats)
    t2_matches = get_match_count_safe(team2_stats)
    features['total_matches'] = t1_matches + t2_matches
    features['match_experience_diff'] = t1_matches - t2_matches
    features['min_matches'] = min(t1_matches, t2_matches)  # Data reliability indicator
    
    # Score differential analysis
    t1_score_diff = team1_stats.get('score_differential', 0)
    t2_score_diff = team2_stats.get('score_differential', 0)
    features['score_diff_gap'] = t1_score_diff - t2_score_diff
    features['score_diff_consistency'] = 1 - abs(t1_score_diff + t2_score_diff) / 2  # Close games indicator
    
    # Advanced momentum indicators
    t1_momentum = team1_stats.get('momentum_indicator', 0)
    t2_momentum = team2_stats.get('momentum_indicator', 0)
    features['momentum_diff'] = t1_momentum - t2_momentum
    features['momentum_strength'] = max(abs(t1_momentum), abs(t2_momentum))
    
    # Volatility and consistency
    t1_vol = team1_stats.get('volatility', 0)
    t2_vol = team2_stats.get('volatility', 0)
    features['volatility_diff'] = t1_vol - t2_vol
    features['combined_volatility'] = (t1_vol + t2_vol) / 2
    features['stability_advantage'] = max(0, t2_vol - t1_vol)  # Team1 advantage if Team2 more volatile
    
    # Enhanced player statistics
    if 'avg_player_rating' in team1_stats and 'avg_player_rating' in team2_stats:
        t1_rating = team1_stats['avg_player_rating']
        t2_rating = team2_stats['avg_player_rating']
        features['player_rating_diff'] = t1_rating - t2_rating
        features['player_rating_product'] = t1_rating * t2_rating
        features['star_power_diff'] = team1_stats.get('star_player_rating', 0) - team2_stats.get('star_player_rating', 0)
        features['team_consistency_diff'] = team1_stats.get('team_consistency', 0) - team2_stats.get('team_consistency', 0)
        
        # Advanced player metrics
        features['firepower_diff'] = team1_stats.get('avg_player_acs', 0) - team2_stats.get('avg_player_acs', 0)
        features['clutch_diff'] = team1_stats.get('avg_clutch', 0) - team2_stats.get('avg_clutch', 0)
        features['first_kills_advantage'] = team1_stats.get('fk_fd_ratio', 1) - team2_stats.get('fk_fd_ratio', 1)
    
    # Economy mastery
    if 'pistol_win_rate' in team1_stats and 'pistol_win_rate' in team2_stats:
        features['pistol_mastery_diff'] = team1_stats['pistol_win_rate'] - team2_stats['pistol_win_rate']
        features['eco_round_diff'] = team1_stats.get('eco_win_rate', 0) - team2_stats.get('eco_win_rate', 0)
        features['full_buy_diff'] = team1_stats.get('full_buy_win_rate', 0) - team2_stats.get('full_buy_win_rate', 0)
        features['economy_mastery'] = (team1_stats.get('economy_efficiency', 0) - 
                                     team2_stats.get('economy_efficiency', 0))
    
    # Head-to-head analysis (enhanced)
    h2h_stats = extract_h2h_stats(team1_stats, team2_stats)
    features.update(h2h_stats)
    
    # Meta and adaptation
    features['meta_adaptation_diff'] = team1_stats.get('meta_adjustment', 1) - team2_stats.get('meta_adjustment', 1)
    features['tournament_tier_diff'] = team1_stats.get('avg_tournament_tier', 1) - team2_stats.get('avg_tournament_tier', 1)
    
    # Interaction features (these often improve model performance significantly)
    if 'player_rating_diff' in features and 'win_rate_diff' in features:
        features['skill_form_alignment'] = features['player_rating_diff'] * features['win_rate_diff']
    
    if 'momentum_diff' in features and 'recent_form_diff' in features:
        features['momentum_form_synergy'] = features['momentum_diff'] * features['recent_form_diff']
    
    # Stylistic matchup
    features['upset_potential'] = calculate_upset_potential(t1_wr, t2_wr, features.get('volatility_diff', 0))
    features['favorite_strength'] = max(t1_wr, t2_wr) if abs(t1_wr - t2_wr) > 0.1 else 0.5
    
    # Clean and validate all features
    cleaned_features = {}
    for key, value in features.items():
        if isinstance(value, (int, float)) and not (np.isnan(value) or np.isinf(value)):
            cleaned_features[key] = float(value)
        else:
            cleaned_features[key] = 0.0
    
    return cleaned_features

def get_match_count_safe(team_stats):
    """Safely extract match count from team stats"""
    matches = team_stats.get('matches', 0)
    if isinstance(matches, list):
        return len(matches)
    elif isinstance(matches, (int, float)):
        return int(matches)
    return 0

def extract_h2h_stats(team1_stats, team2_stats):
    """Extract enhanced head-to-head statistics"""
    h2h_features = {}
    
    team2_name = team2_stats.get('team_name', '')
    h2h_found = False
    h2h_stats = None
    
    if 'opponent_stats' in team1_stats and isinstance(team1_stats['opponent_stats'], dict):
        for opponent_name, stats in team1_stats['opponent_stats'].items():
            if team2_name.lower() in opponent_name.lower() or opponent_name.lower() in team2_name.lower():
                h2h_stats = stats
                h2h_found = True
                break
    
    if h2h_found and h2h_stats:
        h2h_features['h2h_exists'] = 1
        h2h_features['h2h_win_rate'] = h2h_stats.get('win_rate', 0.5)
        h2h_features['h2h_matches'] = min(h2h_stats.get('matches', 0), 10)  # Cap for normalization
        h2h_features['h2h_score_diff'] = h2h_stats.get('score_differential', 0)
        h2h_features['h2h_dominance'] = abs(h2h_stats.get('win_rate', 0.5) - 0.5) * 2
        h2h_features['h2h_sample_size'] = 1 if h2h_stats.get('matches', 0) >= 3 else 0
    else:
        # Estimate H2H based on overall strength
        t1_wr = team1_stats.get('win_rate', 0.5)
        t2_wr = team2_stats.get('win_rate', 0.5)
        if t1_wr + t2_wr > 0:
            estimated_h2h = t1_wr / (t1_wr + t2_wr)
        else:
            estimated_h2h = 0.5
        
        h2h_features['h2h_exists'] = 0
        h2h_features['h2h_win_rate'] = 0.5 + (estimated_h2h - 0.5) * 0.3  # Regress toward 50%
        h2h_features['h2h_matches'] = 0
        h2h_features['h2h_score_diff'] = 0
        h2h_features['h2h_dominance'] = 0
        h2h_features['h2h_sample_size'] = 0
    
    return h2h_features

def calculate_upset_potential(t1_wr, t2_wr, volatility_diff):
    """Calculate the potential for an upset based on team characteristics"""
    win_rate_gap = abs(t1_wr - t2_wr)
    base_upset_potential = max(0, 0.3 - win_rate_gap)  # Higher when teams are closer
    
    # Volatility increases upset potential
    volatility_factor = abs(volatility_diff) * 0.5
    
    return base_upset_potential + volatility_factor

def generate_performance_warning(backtest_results):
    """Enhanced performance warning with more realistic expectations"""
    backtest_roi = backtest_results.get('performance', {}).get('roi', 0)
    win_rate = backtest_results.get('performance', {}).get('win_rate', 0)
    total_bets = backtest_results.get('performance', {}).get('total_bets', 0)

    if total_bets == 0:
        return (
            "\n===== NO BETS PLACED =====\n\n"
            "The enhanced conservative criteria resulted in no bets.\n"
            "This is actually GOOD - it means we're avoiding -EV bets.\n\n"
            "Consider:\n"
            "- The model is working correctly by being selective\n"
            "- Real profitable betting requires patience\n"
            "- Quality over quantity is key to long-term success\n\n"
        )

    # Much more conservative real-world projections
    market_efficiency_penalty = 0.6  # 60% reduction due to market efficiency
    execution_costs = 0.15  # 15% reduction for execution issues
    model_degradation = 0.25  # 25% reduction for model decay over time
    
    total_penalty = market_efficiency_penalty + execution_costs + model_degradation
    expected_roi = backtest_roi * (1 - min(0.85, total_penalty))  # Cap penalty at 85%
    
    # Confidence intervals (much wider for reality)
    lower_70 = expected_roi * 0.1   # 90% reduction at lower bound
    upper_70 = expected_roi * 2.5   # 150% increase at upper bound
    
    required_sample = max(200, min(1000, int(500 / max(0.005, abs(expected_roi)))))
    
    warning = (
        f"\n===== REALISTIC EXPECTATIONS (ENHANCED) =====\n\n"
        f"PERFORMANCE PROJECTION:\n"
        f"- Backtest ROI: {backtest_roi:.2%}\n"
        f"- Expected Real-World ROI: {expected_roi:.2%}\n"
        f"- 70% Confidence Range: {lower_70:.2%} to {upper_70:.2%}\n"
        f"- Probability of Profit: {'High' if expected_roi > 0.02 else 'Medium' if expected_roi > 0 else 'Low'}\n\n"
        
        f"SAMPLE SIZE ANALYSIS:\n"
        f"- Backtest bets: {total_bets}\n"
        f"- Minimum needed: {required_sample}\n"
        f"- Status: {'Adequate' if total_bets >= required_sample else 'Insufficient - need more data'}\n\n"
        
        f"ENHANCED WARNINGS:\n"
        f"1. Sports betting markets are increasingly efficient\n"
        f"2. Backtests CANNOT account for line movement against you\n"
        f"3. Emotional discipline is harder than math suggests\n"
        f"4. Bankroll management failures kill most profitable strategies\n"
        f"5. Model performance degrades over time without updates\n"
        f"6. Books may limit successful players\n"
        f"7. Tax implications reduce net returns\n\n"
        
        f"SUCCESS REQUIREMENTS:\n"
        f"- Strict bankroll management (never exceed 2% per bet)\n"
        f"- Track Closing Line Value, not just wins/losses\n"
        f"- Regular model retraining with new data\n"
        f"- Emotional discipline during losing streaks\n"
        f"- Multiple sportsbook accounts for line shopping\n\n"
    )

    if expected_roi > 0.03:
        warning += "ASSESSMENT: Strategy shows strong potential but requires discipline.\n"
    elif expected_roi > 0.01:
        warning += "ASSESSMENT: Marginally profitable - success depends on perfect execution.\n"
    else:
        warning += "ASSESSMENT: Not recommended - insufficient edge for real-world conditions.\n"

    return warning

@debug_func
def build_training_dataset(team_data_collection):
    print(f"[{datetime.now()}] Building training dataset from {len(team_data_collection)} teams...")
    sys.stdout.flush()
    """
    Build a dataset for model training from team data collection.
    Works with both API-fetched and cached data.
    """
    X = []  # Feature vectors
    y = []  # Labels (1 if team1 won, 0 if team2 won)
    print(f"Building training dataset from {len(team_data_collection)} teams...")
    for team_name, team_data in team_data_collection.items():
        # Handle both data structures (direct API and cached)
        if 'matches' in team_data:
            matches = team_data['matches']
            stats = team_data.get('stats', {})
        else:
            # Direct structure (not nested)
            matches = team_data.get('matches', [])
            stats = team_data
        
        if not matches:
            continue
            
        for match in matches:
            opponent_name = match.get('opponent_name')
            if opponent_name not in team_data_collection:
                continue
                
            team1_stats = stats
            team2_data = team_data_collection[opponent_name]
            
            # Handle both data structures for opponent
            if 'stats' in team2_data:
                team2_stats = team2_data['stats']
            else:
                team2_stats = team2_data
                
            features = prepare_data_for_model(team1_stats, team2_stats)
            if features:
                X.append(features)
                y.append(1 if match.get('team_won', False) else 0)
                
    print(f"Created {len(X)} training samples from match data")
    return X, y

#-------------------------------------------------------------------------
# MODEL TRAINING AND EVALUATION
#-------------------------------------------------------------------------

def run_backtest(start_date=None, end_date=None, team_limit=50, bankroll=1000.0, 
                bet_pct=0.05, min_edge=0.02, confidence_threshold=0.2, use_cache=True, 
                cache_path="cache/valorant_data_cache.pkl"):
    """
    Enhanced backtest with strict profitability focus and realistic market simulation.
    """
    print(f"\n=== ENHANCED PROFITABILITY-FOCUSED BACKTEST ===")
    print(f"Enhanced parameters: min_edge={min_edge:.1%}, confidence_threshold={confidence_threshold:.1%}")
    
    # Load models
    ensemble_models, selected_features = load_backtesting_models()
    if not ensemble_models or not selected_features:
        print("Failed to load models or features")
        return None
    
    # Load team data
    team_data = {}
    if use_cache:
        cache_data = load_cache(cache_path)
        if cache_data:
            team_data = cache_data
            print(f"Loaded {len(team_data)} teams from cache")
    
    if not team_data:
        print("No team data available")
        return None
    
    # Build backtest matches with improved filtering
    backtest_matches = []
    seen_match_ids = set()
    match_quality_threshold = 10  # Minimum matches per team for inclusion
    
    for team_name, team_info in team_data.items():
        team_matches = team_info.get('matches', [])
        if len(team_matches) < match_quality_threshold:
            continue  # Skip teams with insufficient data
            
        for match in team_matches:
            match_id = match.get('match_id', '')
            opponent_name = match.get('opponent_name')
            
            if not opponent_name or opponent_name not in team_data:
                continue
                
            if match_id in seen_match_ids:
                continue
                
            # Additional quality filters
            if len(team_data[opponent_name].get('matches', [])) < match_quality_threshold:
                continue
                
            seen_match_ids.add(match_id)
            backtest_matches.append({
                'team1_name': team_name,
                'team2_name': opponent_name,
                'match_data': match,
                'match_id': match_id
            })
    
    print(f"Quality-filtered matches: {len(backtest_matches)}")
    
    # Enhanced results tracking
    results = {
        'predictions': [],
        'bets': [],
        'performance': {
            'accuracy': 0,
            'roi': 0,
            'profit': 0,
            'bankroll_history': [],
            'win_rate': 0,
            'sharpe_ratio': 0,
            'max_drawdown': 0,
            'profit_factor': 0
        },
        'metrics': {
            'accuracy_by_confidence': {},
            'roi_by_edge': {},
            'bet_types': {},
            'monthly_performance': {},
            'risk_metrics': {}
        },
        'enhanced_analysis': True
    }
    
    # Bankroll management
    current_bankroll = bankroll
    starting_bankroll = bankroll
    peak_bankroll = bankroll
    
    # Performance tracking
    correct_predictions = 0
    total_predictions = 0
    total_bets = 0
    winning_bets = 0
    total_wagered = 0
    total_returns = 0
    
    # Enhanced tracking for profitability analysis
    daily_returns = []
    bet_returns = []
    consecutive_losses = 0
    max_consecutive_losses = 0
    
    # Process matches with enhanced analysis
    sample_size = min(500, len(backtest_matches))  # Limit for performance
    selected_matches = np.random.choice(len(backtest_matches), sample_size, replace=False)
    
    print(f"Processing {sample_size} matches for enhanced backtest")
    
    for i, match_idx in enumerate(tqdm(selected_matches, desc="Enhanced backtesting")):
        match = backtest_matches[match_idx]
        
        try:
            team1_name = match['team1_name']
            team2_name = match['team2_name']
            match_data = match['match_data']
            match_date = match_data.get('date', '')
            
            # Get team stats (this should use historical data to avoid lookahead bias)
            team1_stats = team_data[team1_name].get('stats', {})
            team2_stats = team_data[team2_name].get('stats', {})
            
            # Prepare features
            X = prepare_features_for_backtest(team1_stats, team2_stats, selected_features)
            if X is None:
                continue
                
            # Make prediction
            win_probability, raw_predictions, confidence_score = predict_with_ensemble(
                ensemble_models, X
            )
            
            # Extract actual result
            team1_score, team2_score = extract_match_score(match_data)
            actual_winner = 'team1' if team1_score > team2_score else 'team2'
            predicted_winner = 'team1' if win_probability > 0.5 else 'team2'
            prediction_correct = predicted_winner == actual_winner
            
            correct_predictions += 1 if prediction_correct else 0
            total_predictions += 1
            
            # Enhanced odds simulation
            base_odds = simulate_odds(win_probability, vig=0.045, market_efficiency=0.92)
            
            # Analyze betting opportunities with STRICT criteria
            betting_analysis = analyze_betting_edge_for_backtesting(
                win_probability, 1 - win_probability, base_odds,
                confidence_score, current_bankroll
            )
            
            # Select bets with ultra-conservative criteria
            optimal_bets = select_optimal_bets_conservative(
                betting_analysis, team1_name, team2_name, current_bankroll,
                max_bets=2, max_total_risk=0.03, config=None  # Max 3% total risk
            )
            
            # Process bets
            match_profit = 0
            match_bets = []
            
            for bet_type, analysis in optimal_bets.items():
                bet_amount = analysis['bet_amount']
                odds = analysis['odds']
                
                # Evaluate bet outcome
                bet_won = evaluate_bet_outcome(bet_type, actual_winner, team1_score, team2_score)
                
                # Calculate returns
                returns = bet_amount * odds if bet_won else 0
                profit = returns - bet_amount
                match_profit += profit
                
                # Update bankroll
                current_bankroll += profit
                peak_bankroll = max(peak_bankroll, current_bankroll)
                
                # Track bet
                match_bets.append({
                    'bet_type': bet_type,
                    'amount': bet_amount,
                    'odds': odds,
                    'won': bet_won,
                    'returns': returns,
                    'profit': profit,
                    'edge': analysis['edge'],
                    'confidence': confidence_score
                })
                
                # Update counters
                total_bets += 1
                winning_bets += 1 if bet_won else 0
                total_wagered += bet_amount
                total_returns += returns
                
                # Track consecutive losses for risk management
                if bet_won:
                    consecutive_losses = 0
                else:
                    consecutive_losses += 1
                    max_consecutive_losses = max(max_consecutive_losses, consecutive_losses)
                
                bet_returns.append(profit / bet_amount)  # Return rate
            
            # Record daily performance
            if match_bets:
                daily_returns.append(match_profit / starting_bankroll)
                
            # Track bankroll history
            results['performance']['bankroll_history'].append({
                'match_idx': i,
                'bankroll': current_bankroll,
                'profit': current_bankroll - starting_bankroll,
                'drawdown': (peak_bankroll - current_bankroll) / peak_bankroll,
                'date': match_date
            })
            
            # Early stopping if bankroll drops too low
            if current_bankroll < starting_bankroll * 0.5:
                print(f"Stopping backtest - bankroll dropped below 50% at match {i}")
                break
                
        except Exception as e:
            print(f"Error processing match {i}: {e}")
            continue
    
    # Calculate enhanced performance metrics
    final_profit = current_bankroll - starting_bankroll
    final_roi = final_profit / starting_bankroll if starting_bankroll > 0 else 0
    final_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
    bet_win_rate = winning_bets / total_bets if total_bets > 0 else 0
    
    # Advanced risk metrics
    max_drawdown = (peak_bankroll - min(entry['bankroll'] for entry in results['performance']['bankroll_history'])) / peak_bankroll if results['performance']['bankroll_history'] else 0
    
    # Sharpe ratio calculation
    if daily_returns and len(daily_returns) > 1:
        avg_daily_return = np.mean(daily_returns)
        daily_volatility = np.std(daily_returns)
        sharpe_ratio = (avg_daily_return * 252) / (daily_volatility * np.sqrt(252)) if daily_volatility > 0 else 0
    else:
        sharpe_ratio = 0
    
    # Profit factor
    winning_trades = [r for r in bet_returns if r > 0]
    losing_trades = [r for r in bet_returns if r < 0]
    
    if winning_trades and losing_trades:
        gross_profit = sum(winning_trades)
        gross_loss = abs(sum(losing_trades))
        profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
    else:
        profit_factor = 0
    
    # Update results
    results['performance'].update({
        'accuracy': final_accuracy,
        'roi': final_roi,
        'profit': final_profit,
        'win_rate': bet_win_rate,
        'final_bankroll': current_bankroll,
        'total_bets': total_bets,
        'total_wagered': total_wagered,
        'total_returns': total_returns,
        'max_drawdown': max_drawdown,
        'sharpe_ratio': sharpe_ratio,
        'profit_factor': profit_factor,
        'max_consecutive_losses': max_consecutive_losses,
        'avg_bet_size': total_wagered / total_bets if total_bets > 0 else 0
    })
    
    # Enhanced reporting
    print(f"\n{'='*60}")
    print(f"ENHANCED BACKTEST RESULTS")
    print(f"{'='*60}")
    print(f"Prediction Accuracy: {final_accuracy:.2%}")
    print(f"Total Bets Placed: {total_bets}")
    print(f"Bet Win Rate: {bet_win_rate:.2%}")
    print(f"Total Wagered: ${total_wagered:.2f}")
    print(f"Total Returns: ${total_returns:.2f}")
    print(f"Final Profit: ${final_profit:.2f}")
    print(f"ROI: {final_roi:.2%}")
    print(f"Max Drawdown: {max_drawdown:.2%}")
    print(f"Sharpe Ratio: {sharpe_ratio:.2f}")
    print(f"Profit Factor: {profit_factor:.2f}")
    print(f"Max Consecutive Losses: {max_consecutive_losses}")
    
    if total_bets == 0:
        print("\n" + "="*60)
        print("EXCELLENT: No bets placed - model is being appropriately conservative!")
        print("This suggests the enhanced criteria are working to avoid -EV bets.")
        print("="*60)
    elif final_roi > 0.05:
        print(f"\n PROMISING: {final_roi:.1%} ROI suggests potential profitability")
        print("   Continue with strict bankroll management")
    elif final_roi > 0:
        print(f"\n  MARGINAL: {final_roi:.1%} ROI - proceed with extreme caution")
    else:
        print(f"\n UNPROFITABLE: {final_roi:.1%} ROI - strategy needs major revision")
    
    return results

def create_improved_model(input_dim, regularization_strength=0.01, dropout_rate=0.4):
    inputs = Input(shape=(input_dim,))
    
    x = Dense(128, activation='swish', 
              kernel_regularizer=l2(regularization_strength),
              kernel_initializer='he_normal')(inputs)
    x = BatchNormalization(momentum=0.95)(x)
    x = Dropout(dropout_rate)(x)
    
    x = Dense(64, activation='swish',
              kernel_regularizer=l2(regularization_strength * 0.8),
              kernel_initializer='he_normal')(x)
    x = BatchNormalization(momentum=0.95)(x)
    x = Dropout(dropout_rate * 0.8)(x)
    
    x = Dense(32, activation='swish',
              kernel_regularizer=l2(regularization_strength * 0.6),
              kernel_initializer='he_normal')(x)
    x = BatchNormalization(momentum=0.95)(x)
    x = Dropout(dropout_rate * 0.6)(x)
    
    x = Dense(16, activation='relu',
              kernel_regularizer=l2(regularization_strength * 0.4),
              kernel_initializer='he_normal')(x)
    x = BatchNormalization(momentum=0.95)(x)
    x = Dropout(dropout_rate * 0.4)(x)
    
    outputs = Dense(1, activation='sigmoid',
                    kernel_regularizer=l2(regularization_strength * 0.2),
                    kernel_initializer='glorot_normal',
                    bias_initializer='zeros')(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    
    optimizer = Adam(learning_rate=0.0003, clipnorm=0.5, epsilon=1e-7)
    model.compile(
        loss='binary_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy']
    )
    
    return model

def implement_stacking_ensemble(X_train, y_train, X_val, y_val):
    """
    Implement stacking ensemble to better combine diverse models.
    
    Args:
        X_train: Training features
        y_train: Training labels
        X_val: Validation features
        y_val: Validation labels
        
    Returns:
        tuple: (base_models, meta_model)
    """
    print("Implementing stacking ensemble...")
    
    # Train first-level models
    base_models = []
    base_predictions = np.zeros((len(X_val), 5))  # For 5 base models
    
    # 1. Train Neural Network
    print("Training Neural Network...")
    nn_model = create_improved_model(X_train.shape[1])
    
    # Set up callbacks
    early_stopping = EarlyStopping(
        monitor='val_loss', patience=15, restore_best_weights=True, verbose=0
    )
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=0
    )
    
    nn_model.fit(
        X_train, y_train,
        epochs=100,
        batch_size=32,
        validation_data=(X_val, y_val),
        callbacks=[early_stopping, reduce_lr],
        verbose=0
    )
    
    base_predictions[:, 0] = nn_model.predict(X_val).flatten()
    base_models.append(('nn', nn_model, None))
    
    # 2. Train Gradient Boosting
    print("Training Gradient Boosting...")
    gb_model = GradientBoostingClassifier(
        n_estimators=100,
        learning_rate=0.05,
        max_depth=3,
        random_state=42
    )
    gb_model.fit(X_train, y_train)
    base_predictions[:, 1] = gb_model.predict_proba(X_val)[:, 1]
    base_models.append(('gb', gb_model, None))
    
    # 3. Train Random Forest
    print("Training Random Forest...")
    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42
    )
    rf_model.fit(X_train, y_train)
    base_predictions[:, 2] = rf_model.predict_proba(X_val)[:, 1]
    base_models.append(('rf', rf_model, None))
    
    # 4. Train Logistic Regression
    print("Training Logistic Regression...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    lr_model = LogisticRegression(
        C=0.1,
        random_state=42,
        max_iter=1000
    )
    lr_model.fit(X_train_scaled, y_train)
    base_predictions[:, 3] = lr_model.predict_proba(X_val_scaled)[:, 1]
    base_models.append(('lr', lr_model, scaler))
    
    # 5. Train SVM
    print("Training SVM...")
    svm_model = SVC(
        C=1.0,
        kernel='rbf',
        probability=True,
        random_state=42
    )
    svm_model.fit(X_train_scaled, y_train)
    base_predictions[:, 4] = svm_model.predict_proba(X_val_scaled)[:, 1]
    base_models.append(('svm', svm_model, scaler))
    
    # Train meta-learner on base predictions
    print("Training meta-learner...")
    meta_model = LogisticRegression(C=0.5, random_state=42)
    meta_model.fit(base_predictions, y_val)
    
    # Evaluate stacking performance
    meta_preds = meta_model.predict(base_predictions)
    stacking_accuracy = accuracy_score(y_val, meta_preds)
    print(f"Stacking ensemble accuracy: {stacking_accuracy:.4f}")
    
    # Store meta model with base models
    base_models.append(('meta', meta_model, None))
    
    return base_models

def implement_time_based_validation(team_data_collection):
    """
    Implement time-based validation to prevent data leakage.
    
    Args:
        team_data_collection: Dictionary of team data
        
    Returns:
        tuple: (train_data, val_data, test_data)
    """
    print("Implementing time-based validation...")
    
    # Collect all matches with dates
    all_matches = []
    for team_name, team_data in team_data_collection.items():
        for match in team_data.get('matches', []):
            # Add team info to match
            match['team_name'] = team_name
            match['team_stats'] = team_data.get('stats', {})
            
            # Only include matches with valid dates
            if 'date' in match and match['date']:
                all_matches.append(match)
    
    # Sort chronologically
    all_matches.sort(key=lambda x: x.get('date', ''))
    print(f"Collected {len(all_matches)} matches with dates")
    
    # Split into train/validation/test maintaining chronological order
    train_cutoff = int(len(all_matches) * 0.7)
    val_cutoff = int(len(all_matches) * 0.85)
    
    train_matches = all_matches[:train_cutoff]
    val_matches = all_matches[train_cutoff:val_cutoff]
    test_matches = all_matches[val_cutoff:]
    
    print(f"Split into {len(train_matches)} train, {len(val_matches)} validation, {len(test_matches)} test matches")
    
    # Create datasets
    train_data = create_dataset_from_matches(train_matches, team_data_collection)
    val_data = create_dataset_from_matches(val_matches, team_data_collection)
    test_data = create_dataset_from_matches(test_matches, team_data_collection)
    
    return train_data, val_data, test_data

def create_dataset_from_matches(matches, team_data_collection):
    """
    Create feature vectors and labels from match data.
    
    Args:
        matches: List of matches
        team_data_collection: Dictionary of team data
        
    Returns:
        tuple: (X, y, match_info)
    """
    X = []  # Features
    y = []  # Labels
    match_info = []  # Additional match metadata
    
    for match in matches:
        team1_name = match['team_name']
        team2_name = match['opponent_name']
        
        # Skip if we don't have stats for opponent
        if team2_name not in team_data_collection:
            continue
            
        # Get recent stats for both teams as of this match date
        team1_stats = get_team_stats_at_date(team1_name, match['date'], team_data_collection)
        team2_stats = get_team_stats_at_date(team2_name, match['date'], team_data_collection)
        
        # Skip if either team doesn't have sufficient data
        if not team1_stats or not team2_stats:
            continue
            
        # Create feature vector using data only available before this match
        features = prepare_data_for_model(team1_stats, team2_stats)
        
        if features:
            X.append(features)
            y.append(1 if match['team_won'] else 0)
            
            # Store match info for analysis
            match_info.append({
                'match_id': match['match_id'],
                'date': match['date'],
                'team1': team1_name,
                'team2': team2_name,
                'score': f"{match['team_score']}-{match['opponent_score']}",
                'winner': 'team1' if match['team_won'] else 'team2'
            })
    
    return np.array(X), np.array(y), match_info

def get_team_stats_at_date(team_name, target_date, team_data_collection):
    """
    Get team statistics at a specific date by filtering out future matches.
    
    Args:
        team_name: Name of the team
        target_date: Date to get stats for
        team_data_collection: Dictionary of team data
        
    Returns:
        dict: Team stats at the specified date
    """
    if team_name not in team_data_collection:
        return None
    
    team_data = team_data_collection[team_name]
    
    # Get only matches before the target date
    past_matches = []
    for match in team_data.get('matches', []):
        match_date = match.get('date', '')
        if match_date and match_date < target_date:
            past_matches.append(match)
    
    # If not enough past matches, return None
    if len(past_matches) < 5:
        return None
    
    # Calculate stats using only past matches
    team_stats = calculate_team_stats(past_matches)
    
    return team_stats

def calibrate_prediction(raw_predictions):
    """
    Calibrate ensemble predictions for better reliability.
    
    Args:
        raw_predictions (list): List of prediction values from ensemble models
        
    Returns:
        tuple: (calibrated_prediction, model_agreement_score)
    """
    # Calculate basic statistics
    mean_pred = np.mean(raw_predictions)
    std_pred = np.std(raw_predictions)
    model_agreement = 1 - min(1, std_pred * 2)  # Higher means more agreement
    
    # Handle extreme cases
    if model_agreement < 0.3:
        # Very low agreement - regress heavily toward 0.5
        calibrated = 0.5 + (mean_pred - 0.5) * 0.5
    elif model_agreement < 0.6:
        # Moderate agreement - regress somewhat toward 0.5
        calibrated = 0.5 + (mean_pred - 0.5) * 0.75
    else:
        # Good agreement - minimal regression
        calibrated = mean_pred
    
    return calibrated, model_agreement

def create_diverse_ensemble(X_train, y_train, feature_mask, random_state=42):
    X_train_selected = X_train[:, feature_mask]
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_selected)
    
    models = []
    input_dim = X_train_scaled.shape[1]
    
    nn_configs = [
        {'reg': 0.005, 'dropout': 0.3, 'lr': 0.0005},
        {'reg': 0.01, 'dropout': 0.4, 'lr': 0.0003},
        {'reg': 0.02, 'dropout': 0.5, 'lr': 0.0002},
        {'reg': 0.008, 'dropout': 0.35, 'lr': 0.0004},
        {'reg': 0.015, 'dropout': 0.45, 'lr': 0.00025}
    ]
    
    for i, config in enumerate(nn_configs):
        try:
            nn_model = create_improved_model(input_dim, config['reg'], config['dropout'])
            
            optimizer = Adam(learning_rate=config['lr'], clipnorm=0.5)
            nn_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
            
            early_stopping = EarlyStopping(
                monitor='val_loss', patience=12, restore_best_weights=True, verbose=0
            )
            
            reduce_lr = ReduceLROnPlateau(
                monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6, verbose=0
            )
            
            nn_model.fit(
                X_train_scaled, y_train,
                epochs=150,
                batch_size=min(64, len(X_train_scaled) // 8),
                validation_split=0.15,
                callbacks=[early_stopping, reduce_lr],
                verbose=0
            )
            
            models.append(('nn', nn_model, scaler))
        except Exception:
            continue
    
    gb_configs = [
        {'n_est': 200, 'lr': 0.05, 'depth': 4, 'subsample': 0.8},
        {'n_est': 300, 'lr': 0.03, 'depth': 5, 'subsample': 0.85},
        {'n_est': 150, 'lr': 0.08, 'depth': 3, 'subsample': 0.9}
    ]
    
    for config in gb_configs:
        try:
            gb_model = GradientBoostingClassifier(
                n_estimators=config['n_est'],
                learning_rate=config['lr'],
                max_depth=config['depth'],
                subsample=config['subsample'],
                random_state=random_state,
                validation_fraction=0.15,
                n_iter_no_change=20
            )
            gb_model.fit(X_train_selected, y_train)
            models.append(('gb', gb_model, None))
        except Exception:
            continue
    
    rf_configs = [
        {'n_est': 300, 'depth': 8, 'min_split': 15, 'min_leaf': 5},
        {'n_est': 200, 'depth': 10, 'min_split': 20, 'min_leaf': 8},
        {'n_est': 400, 'depth': 6, 'min_split': 10, 'min_leaf': 3}
    ]
    
    for config in rf_configs:
        try:
            rf_model = RandomForestClassifier(
                n_estimators=config['n_est'],
                max_depth=config['depth'],
                min_samples_split=config['min_split'],
                min_samples_leaf=config['min_leaf'],
                max_features='sqrt',
                class_weight='balanced',
                random_state=random_state,
                n_jobs=-1
            )
            rf_model.fit(X_train_selected, y_train)
            models.append(('rf', rf_model, None))
        except Exception:
            continue
    
    try:
        lr_model = LogisticRegression(
            C=0.3,
            penalty='elasticnet',
            l1_ratio=0.3,
            random_state=random_state,
            max_iter=2000,
            class_weight='balanced'
        )
        lr_model.fit(X_train_scaled, y_train)
        models.append(('lr', lr_model, scaler))
    except Exception:
        pass
    
    try:
        svm_model = SVC(
            C=2.0,
            kernel='rbf',
            gamma='scale',
            probability=True,
            random_state=random_state,
            class_weight='balanced'
        )
        svm_model.fit(X_train_scaled, y_train)
        models.append(('svm', svm_model, scaler))
    except Exception:
        pass
    
    return models, scaler  

def predict_with_diverse_ensemble(models, X):
    """
    Make predictions using a diverse ensemble of models.
    
    Args:
        models (list): List of (model_type, model, scaler) tuples
        X (array): Input features
        
    Returns:
        tuple: (predictions, calibrated_prediction, model_agreement)
    """
    predictions = []
    
    for model_type, model, scaler in models:
        try:
            # Apply scaling if needed
            X_pred = X.copy()
            if scaler is not None:
                X_pred = scaler.transform(X_pred)
            
            # Make prediction based on model type
            if model_type == 'nn':
                pred = model.predict(X_pred, verbose=0)[0][0]
            else:
                pred = model.predict_proba(X_pred)[0][1]
                
            predictions.append(pred)
            print(f"{model_type.upper()} model prediction: {pred:.4f}")
        except Exception as e:
            print(f"Error with {model_type} model: {e}")
    
    # Calibrate predictions
    if predictions:
        # Calculate basic statistics
        mean_pred = np.mean(predictions)
        std_pred = np.std(predictions)
        model_agreement = 1 - min(1, std_pred * 2)  # Higher means more agreement
        
        # Handle extreme cases with better calibration
        if model_agreement < 0.3:
            # Very low agreement - regress heavily toward 0.5
            calibrated = 0.5 + (mean_pred - 0.5) * 0.5
        elif model_agreement < 0.6:
            # Moderate agreement - regress somewhat toward 0.5
            calibrated = 0.5 + (mean_pred - 0.5) * 0.75
        else:
            # Good agreement - minimal regression
            calibrated = mean_pred
            
        return predictions, calibrated, model_agreement
    else:
        return [], 0.5, 0.0

def prepare_ensemble_features(team1_stats, team2_stats, selected_features):
    """
    Prepare features for the diverse ensemble prediction.
    
    Args:
        team1_stats (dict): Statistics for team 1
        team2_stats (dict): Statistics for team 2
        selected_features (list): List of feature names
        
    Returns:
        array: Feature array for prediction
    """
    # Get full feature dictionary
    features = prepare_data_for_model(team1_stats, team2_stats)
    
    if not features:
        print("ERROR: Failed to create feature dictionary")
        return None
    
    # Create DataFrame with a single row
    features_df = pd.DataFrame([features])
    
    # Create a complete feature set with zeros for missing features
    complete_features = pd.DataFrame(0, index=[0], columns=selected_features)
    
    # Fill in values for features we have
    available_features = [f for f in selected_features if f in features_df.columns]
    print(f"Found {len(available_features)} out of {len(selected_features)} expected features")
    
    if not available_features:
        print("ERROR: No matching features found!")
        return None
    
    # Update values for available features
    for feature in available_features:
        complete_features[feature] = features_df[feature].values
    
    # Convert to numpy array
    X = complete_features.values
    return X

def ensure_consistent_features(features_df, required_features, fill_value=0):
    """
    Ensure the features dataframe has all required features in the correct order.
    
    Args:
        features_df (DataFrame): DataFrame with available features
        required_features (list): List of required feature names
        fill_value (float): Value to fill for missing features
        
    Returns:
        DataFrame: DataFrame with consistent features
    """
    # Create a new DataFrame with all required features
    consistent_df = pd.DataFrame(columns=required_features)
    
    # Add a row with default values
    consistent_df.loc[0] = fill_value
    
    # Update with available values
    for feature in required_features:
        if feature in features_df.columns:
            consistent_df[feature] = features_df[feature].values[0]
    
    print(f"Features: {len(features_df.columns)} available, {len(required_features)} required, {sum(col in features_df.columns for col in required_features)} matched")
    
    return consistent_df

def create_model(input_dim, regularization_strength=0.001, dropout_rate=0.4):
    """Create a deep learning model with regularization for match prediction."""
    # Define inputs
    inputs = Input(shape=(input_dim,))
    
    # First layer - shared feature processing
    x = Dense(256, activation='relu', kernel_regularizer=l2(regularization_strength))(inputs)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate)(x)
    
    # Second layer - deeper processing
    x = Dense(128, activation='relu', kernel_regularizer=l2(regularization_strength/2))(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate * 0.75)(x)
    
    # Specialized pathways
    x = Dense(64, activation='relu', kernel_regularizer=l2(regularization_strength/4))(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate * 0.5)(x)
    
    # Output layer
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Compile model
    model.compile(loss='binary_crossentropy', 
                 optimizer=Adam(learning_rate=0.0005),
                 metrics=['accuracy'])
    
    return model

def train_model(X, y, test_size=0.2, random_state=42):
    """Train the model with feature selection and early stopping."""
    # Clean and prepare feature data
    df = clean_feature_data(X)
    
    if df.empty:
        print("Error: Empty feature dataframe after cleaning")
        return None, None, None
    
    # Convert to numpy array and scale
    X_arr = df.values
    y_arr = np.array(y)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_arr)
    
    # Split data
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y_arr, test_size=test_size, random_state=random_state, stratify=y_arr
    )
    
    # Handle class imbalance if needed
    class_counts = np.bincount(y_train)
    print(f"Class distribution: {class_counts}")
    
    if np.min(class_counts) / np.sum(class_counts) < 0.4:  # If imbalanced
        try:
            if np.min(class_counts) >= 5:
                min_samples = np.min(class_counts)
                k_neighbors = min(5, min_samples-1)
                
                smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
                X_train, y_train = smote.fit_resample(X_train, y_train)
                print(f"Applied SMOTE: Training set now has {np.bincount(y_train)} samples per class")
        except Exception as e:
            print(f"Error applying SMOTE: {e}")
    
    # Feature selection with Random Forest
    feature_selector = RandomForestClassifier(n_estimators=100, random_state=random_state)
    feature_selector.fit(X_train, y_train)
    
    # Get feature importances and select top features
    importances = feature_selector.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    # Select top 75% of features
    cumulative_importance = np.cumsum(importances[indices])
    n_features = np.where(cumulative_importance >= 0.75)[0][0] + 1
    n_features = min(n_features, 100)  # Cap at 100 features
    
    top_indices = indices[:n_features]
    selected_features = [df.columns[i] for i in top_indices]
    
    print(f"Selected {len(selected_features)} top features out of {X_train.shape[1]}")
    
    # Train with selected features
    X_train_selected = X_train[:, top_indices]
    X_val_selected = X_val[:, top_indices]
    
    # Create and train model
    input_dim = X_train_selected.shape[1]
    model = create_model(input_dim)
    
    # Set up callbacks
    early_stopping = EarlyStopping(
        monitor='val_loss', patience=15, restore_best_weights=True, verbose=1
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1
    )
    
    model_checkpoint = ModelCheckpoint(
        'valorant_model.h5', save_best_only=True, monitor='val_accuracy'
    )
    
    # Train model
    history = model.fit(
        X_train_selected, y_train,
        epochs=100,
        batch_size=32,
        validation_data=(X_val_selected, y_val),
        callbacks=[early_stopping, reduce_lr, model_checkpoint],
        verbose=1
    )
    
    # Save model artifacts
    with open('feature_scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    
    with open('feature_names.pkl', 'wb') as f:
        pickle.dump(selected_features, f)
    
    # Evaluate model
    y_pred_proba = model.predict(X_val_selected)
    y_pred = (y_pred_proba > 0.5).astype(int).flatten()
    
    accuracy = accuracy_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)
    
    print("\nModel Evaluation:")
    print(f"Validation Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    
    return model, scaler, selected_features

def train_with_cross_validation(X, y, n_splits=5, random_state=42):
    print(f"\nTraining with {n_splits}-fold cross-validation")

    
    # Prepare and clean data
    df = clean_feature_data(X)
    X_arr = df.values
    y_arr = np.array(y)
    
    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_arr)
    
    # Set up cross-validation
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    
    # Initialize arrays to store results
    fold_metrics = []
    fold_models = []
    all_feature_importances = {}
    
    # Run cross-validation
    for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y_arr)):
        print(f"\n----- Training Fold {fold+1}/{n_splits} -----")
        


        # Split data
        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
        y_train, y_val = y_arr[train_idx], y_arr[val_idx]
        
        # Handle class imbalance if needed
        class_counts = np.bincount(y_train)
        if np.min(class_counts) / np.sum(class_counts) < 0.4:
            try:
                if np.min(class_counts) >= 5:
                    min_samples = np.min(class_counts)
                    k_neighbors = min(5, min_samples-1)
                    smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
                    X_train, y_train = smote.fit_resample(X_train, y_train)
            except Exception as e:
                print(f"Error applying SMOTE: {e}")
        
        # Feature selection
        rf = RandomForestClassifier(n_estimators=100, random_state=random_state)
        rf.fit(X_train, y_train)
        
        # Select top features
        importances = rf.feature_importances_
        indices = np.argsort(importances)[::-1]
        
        # Select top 75% of features
        cumulative_importance = np.cumsum(importances[indices])
        n_features = np.where(cumulative_importance >= 0.75)[0][0] + 1
        n_features = min(n_features, 100)  # Cap at 100 features
        
        selected_indices = indices[:n_features]
        selected_features = [df.columns[i] for i in selected_indices]
        
        # Track feature importance
        for feature in selected_features:
            if feature in all_feature_importances:
                all_feature_importances[feature] += 1
            else:
                all_feature_importances[feature] = 1
        
        # Train model with selected features
        X_train_selected = X_train[:, selected_indices]
        X_val_selected = X_val[:, selected_indices]
        
        # Create and train model
        input_dim = X_train_selected.shape[1]
        model = create_model(input_dim)
        
        # Train with early stopping
        early_stopping = EarlyStopping(
            monitor='val_loss', patience=15, restore_best_weights=True, verbose=1
        )
        
        model.fit(
            X_train_selected, y_train,
            epochs=100,
            batch_size=32,
            validation_data=(X_val_selected, y_val),
            callbacks=[early_stopping],
            verbose=1
        )
        
        # Evaluate model
        y_pred_proba = model.predict(X_val_selected)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred)
        recall = recall_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred)
        auc = roc_auc_score(y_val, y_pred_proba)
        
        # Store results
        fold_metrics.append({
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'selected_features': selected_features
        })
        
        fold_models.append(model)
        
        # Print fold results
        print(f"Fold {fold+1} Results:")
        print(f"  Accuracy: {accuracy:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall: {recall:.4f}")
        print(f"  F1 Score: {f1:.4f}")
        print(f"  AUC: {auc:.4f}")
        print(f"  Selected Features: {len(selected_features)}")
    
    # Calculate average metrics
    avg_metrics = {metric: np.mean([fold[metric] for fold in fold_metrics]) 
                  for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']}
    
    std_metrics = {metric: np.std([fold[metric] for fold in fold_metrics]) 
                  for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']}
    
    print(f"\nAverage Metrics Across {n_splits} Folds:")
    for metric, value in avg_metrics.items():
        print(f"  {metric.capitalize()}: {value:.4f}  {std_metrics[metric]:.4f}")
    
    # Identify stable feature set (features selected in majority of folds)
    sorted_features = sorted(all_feature_importances.items(), 
                           key=lambda x: x[1], reverse=True)
    
    stable_features = [feature for feature, count in sorted_features 
                      if count >= n_splits * 0.8]  # Features selected in at least 80% of folds
    
    print(f"\nStable Feature Set: {len(stable_features)} features")
    
    # Save models
    for i, model in enumerate(fold_models):
        model.save(f'valorant_model_fold_{i+1}.h5')
    
    # Save stable features
    with open('stable_features.pkl', 'wb') as f:
        pickle.dump(stable_features, f)
    
    # Save scaler
    with open('ensemble_scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    
    return fold_models, stable_features, avg_metrics, fold_metrics, scaler


#-------------------------------------------------------------------------
# DATA PREPARATION
#-------------------------------------------------------------------------

def load_ensemble_models(model_paths, scaler_path, features_path):
    """
    Load ensemble models, scaler, and stable feature list.
    
    Args:
        model_paths (list): List of file paths to the trained models
        scaler_path (str): Path to the feature scaler
        features_path (str): Path to the stable features list
        
    Returns:
        tuple: (loaded_models, scaler, stable_features)
    """
    print("Loading ensemble models and data preprocessing artifacts...")
    
    # Load models
    loaded_models = []
    for path in model_paths:
        try:
            model = load_model(path)
            loaded_models.append(model)
            print(f"Loaded model from {path}")
        except Exception as e:
            print(f"Error loading model from {path}: {e}")
    
    # Load scaler
    try:
        with open(scaler_path, 'rb') as f:
            scaler = pickle.load(f)
        print(f"Loaded feature scaler from {scaler_path}")
    except Exception as e:
        print(f"Error loading scaler: {e}")
        scaler = None
    
    # Load stable features
    try:
        with open(features_path, 'rb') as f:
            stable_features = pickle.load(f)
        print(f"Loaded {len(stable_features)} stable features")
    except Exception as e:
        print(f"Error loading stable features: {e}")
        stable_features = None
    
    return loaded_models, scaler, stable_features

def prepare_match_features(team1_stats, team2_stats, stable_features, scaler):
    """
    Prepare features for a single match for prediction using only the features
    that were selected during training to avoid overfitting.
    """
    # Get full feature set
    features = prepare_data_for_model(team1_stats, team2_stats)
    
    if not features:
        print("Failed to prepare match features.")
        return None
    
    # Convert to DataFrame for easier feature selection
    features_df = pd.DataFrame([features])
    print(f"Original feature count: {len(features_df.columns)}")
    
    # If stable_features exists, use only those features
    if stable_features and len(stable_features) > 0:
        # Check which features are available
        available_features = [f for f in stable_features if f in features_df.columns]
        
        print(f"Using {len(available_features)} out of {len(stable_features)} stable features")
        
        # Select only the available stable features in the correct order
        if available_features:
            features_df = features_df[available_features]
        else:
            print("ERROR: None of the stable features are available in current data!")
            return None
    else:
        print("WARNING: No stable features provided. Model may not work correctly.")
    
    # Convert to numpy array for scaling
    X = features_df.values
    print(f"Feature array shape after selection: {X.shape}")
    
    # Scale features if scaler is available
    if scaler:
        try:
            X_scaled = scaler.transform(X)
            return X_scaled
        except Exception as e:
            print(f"Error scaling features: {e}")
            return X  # Return unscaled features as fallback
    else:
        return X

#-------------------------------------------------------------------------
# PREDICTION AND BETTING LOGIC
#-------------------------------------------------------------------------
def prepare_prediction_features(team1_stats, team2_stats, selected_features, scaler):
    """
    Prepare features for prediction ensuring compatibility with trained models.
    
    Args:
        team1_stats (dict): Statistics for team 1
        team2_stats (dict): Statistics for team 2
        selected_features (list): List of feature names used by the model
        scaler (object): Fitted scaler used during training
        
    Returns:
        array: Scaled feature array compatible with the model
    """
    print("Preparing features for prediction...")
    
    # Get full feature dictionary
    features = prepare_data_for_model(team1_stats, team2_stats)
    
    if not features:
        print("ERROR: Failed to create feature dictionary")
        return None
    
    # Create DataFrame with a single row
    features_df = pd.DataFrame([features])
    original_feature_count = len(features_df.columns)
    print(f"Original feature count: {original_feature_count}")
    
    # Get first model to determine input shape
    try:
        model_path = 'valorant_model_fold_1.h5'
        model = load_model(model_path)
        expected_feature_count = model.layers[0].input_shape[1]
        print(f"Model expects {expected_feature_count} features based on input layer")
    except Exception as e:
        print(f"Error loading model to check input shape: {e}")
        # Default to 50 based on error messages
        expected_feature_count = 50
        print(f"Using default expected feature count: {expected_feature_count}")
    
    # Check if we have the feature_metadata which might have the exact list of features used
    try:
        with open('feature_metadata.pkl', 'rb') as f:
            feature_metadata = pickle.load(f)
            if 'selected_features' in feature_metadata:
                model_features = feature_metadata['selected_features']
                print(f"Using exact feature list from metadata with {len(model_features)} features")
                # Override selected_features with the exact list used in model training
                selected_features = model_features
            else:
                print("No selected_features in metadata")
    except Exception as e:
        print(f"Error loading feature metadata: {e}")
    
    # Create a DataFrame with the exact features expected by the model
    if len(selected_features) == expected_feature_count:
        print(f"Selected features count ({len(selected_features)}) matches model input size ({expected_feature_count})")
        final_features = pd.DataFrame(0, index=[0], columns=selected_features)
        
        # Fill in values for features we have
        for feature in selected_features:
            if feature in features_df.columns:
                final_features[feature] = features_df[feature].values
        
        missing_count = sum(final_features.iloc[0] == 0)
        print(f"Missing {missing_count} out of {len(selected_features)} model features")
        
        # Convert to numpy array - shape should match model input
        X = final_features.values
    else:
        print(f"WARNING: Selected features count ({len(selected_features)}) doesn't match model input ({expected_feature_count})")
        
        # Create array with the exact size the model expects
        X = np.zeros((1, expected_feature_count))
        
        # Add features where we can
        matched_features = 0
        for i, feature in enumerate(selected_features):
            if i >= expected_feature_count:
                break
            if feature in features_df.columns:
                X[0, i] = features_df[feature].values[0]
                matched_features += 1
        
        print(f"Added {matched_features} features to match expected model input size")
    
    print(f"Final feature array shape: {X.shape}")
    
    # Skip scaling since it's causing issues
    # The model should still work reasonably well with unscaled features
    print("WARNING: Skipping feature scaling due to dimension mismatch issues")
    
    return X

def predict_with_consistent_ordering(team1_name, team2_name, ensemble_models, selected_features,
                                    team_data_collection=None, current_date=None):
    """
    Make a prediction with consistent team ordering and proper data handling.
    Enhanced to fetch missing teams from API when needed.
    """
    logging.info(f"Making prediction for {team1_name} vs {team2_name}")
    
    if not current_date:
        from datetime import datetime
        current_date = datetime.now().strftime("%Y-%m-%d")
    
    # If team_data_collection is None or missing teams, fetch the data
    missing_teams = []
    if not team_data_collection:
        missing_teams = [team1_name.strip(), team2_name.strip()]
        team_data_collection = collect_team_data(
            team_limit=300,
            include_player_stats=True,
            include_economy=True,
            include_maps=True,
            use_cache=Config.MODEL.CACHE_ENABLED,
            cache_path=Config.MODEL.CACHE_PATH,
            missing_teams=missing_teams
        )
    else:
        # Check if both teams exist in the collection
        team1_clean = team1_name.strip()
        team2_clean = team2_name.strip()
        
        team1_found = team1_clean in team_data_collection
        team2_found = team2_clean in team_data_collection
        
        # Check for partial matches
        if not team1_found:
            for cached_name in team_data_collection.keys():
                if (team1_clean.lower() in cached_name.lower() or 
                    cached_name.lower() in team1_clean.lower()):
                    team1_found = True
                    team1_name = cached_name  # Use the cached name
                    break
        
        if not team2_found:
            for cached_name in team_data_collection.keys():
                if (team2_clean.lower() in cached_name.lower() or 
                    cached_name.lower() in team2_clean.lower()):
                    team2_found = True
                    team2_name = cached_name  # Use the cached name
                    break
        
        # Fetch missing teams
        if not team1_found:
            missing_teams.append(team1_clean)
        if not team2_found:
            missing_teams.append(team2_clean)
        
        if missing_teams:
            print(f"Fetching missing teams: {missing_teams}")
            additional_data = collect_team_data(
                team_limit=300,
                include_player_stats=True,
                include_economy=True,
                include_maps=True,
                use_cache=False,  # Don't use cache since we're fetching specific teams
                missing_teams=missing_teams
            )
            
            # Merge the additional data
            team_data_collection.update(additional_data)
    
    # Get team stats with fallback to API fetch
    team1_stats = get_historical_team_stats(team1_name, current_date, team_data_collection)
    team2_stats = get_historical_team_stats(team2_name, current_date, team_data_collection)
    
    if not team1_stats or not team2_stats:
        logging.error(f"Could not get historical stats for teams")
        return {
            'error': 'Insufficient historical data for prediction',
            'team1_name': team1_name,
            'team2_name': team2_name,
            'details': f"Team1 stats: {'' if team1_stats else ''}, Team2 stats: {'' if team2_stats else ''}"
        }
    
    # Prepare features with consistent ordering
    features, teams_swapped = prepare_data_with_consistent_ordering(
        team1_stats, team2_stats, selected_features
    )
    
    if features is None:
        logging.error(f"Failed to prepare features for prediction")
        return {
            'error': 'Failed to prepare features for prediction',
            'team1_name': team1_name,
            'team2_name': team2_name
        }
    
    # Make prediction
    win_probability, raw_predictions, confidence = predict_with_ensemble(
        ensemble_models, features
    )
    
    # Adjust for team swapping
    if teams_swapped:
        logging.info(f"Teams were swapped for consistency - adjusting predictions")
        win_probability = 1.0 - win_probability
    
    # Calculate bet type probabilities
    bet_type_probabilities = calculate_bet_type_probabilities(win_probability, confidence)
    
    prediction_results = {
        'team1_name': team1_name,
        'team2_name': team2_name,
        'win_probability': win_probability,
        'confidence': confidence,
        'raw_predictions': raw_predictions,
        'bet_type_probabilities': bet_type_probabilities,
        'teams_swapped': teams_swapped,
        'date': current_date
    }
    
    logging.info(f"Prediction complete: {team1_name}:{win_probability:.4f}, {team2_name}:{1-win_probability:.4f}")
    return prediction_results

def analyze_betting_options(prediction_results, odds_data, bankroll=1000.0, bet_history=None):
    """
    Analyze betting options with conservative bankroll management.
    
    Args:
        prediction_results (dict): Prediction results
        odds_data (dict): Betting odds data
        bankroll (float): Current bankroll
        bet_history (list, optional): Betting history for streak analysis
        
    Returns:
        dict: Betting analysis
    """
    logging.info(f"Analyzing betting options with bankroll ${bankroll:.2f}")
    
    # Get prediction data
    win_probability = prediction_results['win_probability']
    confidence = prediction_results['confidence']
    team1_name = prediction_results['team1_name']
    team2_name = prediction_results['team2_name']
    
    # Get bet type probabilities
    bet_type_probabilities = prediction_results['bet_type_probabilities']
    
    # Perform betting edge analysis
    betting_analysis = analyze_betting_edge_conservative(
        win_probability, 
        team1_name, 
        team2_name, 
        odds_data, 
        confidence, 
        bankroll, 
        bet_history, 
        config=Config.BETTING
    )
    
    # Select optimal bets
    optimal_bets = select_optimal_bets_conservative(
        betting_analysis, 
        team1_name, 
        team2_name, 
        bankroll, 
        max_bets=Config.BETTING.MAX_SIMULTANEOUS_BETS, 
        max_total_risk=Config.BETTING.MAX_TOTAL_RISK_PCT, 
        config=Config.BETTING
    )
    
    # Simulate market response for each recommended bet
    market_simulations = {}
    for bet_type, analysis in optimal_bets.items():
        probability = analysis['probability']
        odds = analysis['odds']
        bet_amount = analysis['bet_amount']
        
        market_simulations[bet_type] = simulate_market_response(
            odds, 
            probability, 
            market_efficiency=Config.FORWARD_TEST.MARKET_EFFICIENCY, 
            bet_size=bet_amount, 
            market_size=10000
        )
    
    # Return results
    results = {
        'betting_analysis': betting_analysis,
        'optimal_bets': optimal_bets,
        'market_simulations': market_simulations,
        'bankroll': bankroll
    }
    
    # Log recommendations
    if optimal_bets:
        logging.info(f"Recommended bets: {', '.join(optimal_bets.keys())}")
    else:
        logging.info("No bets recommended")
    
    return results

def explain_prediction_factors(team1_stats, team2_stats, selected_features, feature_weights):
    """
    Explain key factors influencing the prediction for this specific match.
    """
    print("\nKEY PREDICTION FACTORS:")
    print("-" * 80)
    
    # Get the top factors with their values and weights
    factors = []
    for feature in selected_features:
        if feature in feature_weights:
            weight = feature_weights[feature]
            
            # Try to extract feature values for explanation
            value = None
            if feature.endswith('_diff'):
                base_feature = feature.replace('_diff', '')
                if base_feature in team1_stats and base_feature in team2_stats:
                    value = team1_stats[base_feature] - team2_stats[base_feature]
            
            # Add to factors list
            factors.append({
                'feature': feature,
                'weight': weight,
                'value': value,
                'impact': abs(weight) * (0 if value is None else abs(value))
            })
    
    # Sort by impact and show top 5
    factors.sort(key=lambda x: x['impact'], reverse=True)
    for i, factor in enumerate(factors[:5]):
        feature_name = factor['feature'].replace('_', ' ').title()
        print(f"{i+1}. {feature_name}")
        if factor['value'] is not None:
            print(f"   Value: {factor['value']:.4f}, Weight: {factor['weight']:.4f}")
            
            # Add explanation
            if factor['value'] > 0:
                print(f"   This factor favors {team1_stats['team_name']}")
            else:
                print(f"   This factor favors {team2_stats['team_name']}")

def calibrate_prediction(raw_prediction, confidence):
    """
    Calibrate raw model predictions to be more conservative when confidence is low.
    
    Args:
        raw_prediction (float): Raw prediction between 0 and 1
        confidence (float): Model confidence/agreement score
        
    Returns:
        float: Calibrated prediction
    """
    # If confidence is very low, pull prediction toward 0.5 (uncertainty)
    if confidence < 0.2:
        # Strong regression to the mean - very low confidence
        return 0.5 + (raw_prediction - 0.5) * 0.5
    elif confidence < 0.4:
        # Moderate regression to the mean - low confidence
        return 0.5 + (raw_prediction - 0.5) * 0.7
    elif confidence < 0.6:
        # Slight regression to the mean - moderate confidence
        return 0.5 + (raw_prediction - 0.5) * 0.85
    else:
        # No regression - high confidence
        return raw_prediction

def analyze_betting_opportunities(prediction_results, odds_data, bankroll=1000, max_kelly_pct=0.05, min_roi=0.1, min_agreement=0.4):
    """
    Analyze potential betting opportunities with enhanced safety checks.
    
    Args:
        prediction_results (dict): Prediction probabilities
        odds_data (dict): Betting odds from bookmaker
        bankroll (float): Current bankroll amount for betting recommendations
        max_kelly_pct (float): Maximum Kelly fraction cap as safety measure
        min_roi (float): Minimum ROI required for bet recommendation
        min_agreement (float): Minimum model agreement required for recommendations
        
    Returns:
        dict: Betting recommendations with expected value and confidence
    """
    betting_analysis = {}
    min_edge = 0.1  # Increased from 0.05 to 0.1 for more conservative recommendations
    min_confidence = 0.6  # Minimum 60% confidence to recommend a bet
    
    # Check overall model agreement before recommending any bets
    model_agreement = prediction_results.get('model_agreement', 0)
    if model_agreement < min_agreement:
        print(f"WARNING: Model agreement ({model_agreement:.2f}) is below threshold ({min_agreement})")
        print("No bets will be recommended due to low model confidence")
        # Return analysis with no recommendations
        return {k.replace('_odds', ''): {'recommended': False, 'reason': 'Low model agreement'} 
                for k in odds_data.keys()}
    
    # Function to convert decimal odds to implied probability
    def decimal_to_prob(decimal_odds):
        return 1 / decimal_odds
    
    # Function to calculate Kelly Criterion bet size with safety limits
    def kelly_bet(win_prob, decimal_odds):
        # Kelly formula: f* = (bp - q) / b
        b = decimal_odds - 1
        p = win_prob
        q = 1 - p
        kelly = (b * p - q) / b
        
        # Apply a conservative fraction (1/4) of Kelly to reduce variance
        kelly = kelly * 0.25
        
        # Add absolute cap on kelly percentage for safety
        kelly = min(kelly, max_kelly_pct)
        
        return max(0, kelly)
    
    # Function to calculate expected value and ROI
    def calculate_roi(prob, odds):
        return (prob * (odds - 1)) - (1 - prob)
    
    # Function to get reason for bet rejection
    
    # Add extra sanity check for highly divergent predictions
    raw_predictions = prediction_results.get('raw_predictions', [])
    if raw_predictions and np.std(raw_predictions) > 0.25:  # If standard deviation is very high
        print("WARNING: Predictions are highly divergent. Using more conservative thresholds.")
        min_edge = 0.15  # Increase minimum edge requirement
        max_kelly_pct = 0.03  # Reduce maximum Kelly percentage
    
    # Process each bet type
    for bet_key, odds in odds_data.items():
        # Identify bet type and corresponding probability
        bet_type = bet_key.replace('_odds', '')
        prob_key = bet_type + '_prob'
        
        if prob_key in prediction_results:
            our_prob = prediction_results[prob_key]
            implied_prob = decimal_to_prob(odds)
            ev = our_prob - implied_prob
            roi = calculate_roi(our_prob, odds)
            
            # Calculate Kelly bet size
            kelly_fraction = kelly_bet(our_prob, odds)
            bet_amount = round(bankroll * kelly_fraction, 2)
            
            # Only recommend if meets all criteria
            is_recommended = (ev > min_edge and 
                            our_prob > min_confidence and 
                            roi > min_roi and
                            bet_amount > 0)
            
            betting_analysis[bet_type] = {
                'our_probability': our_prob,
                'implied_probability': implied_prob,
                'expected_value': ev,
                'roi': roi,
                'kelly_fraction': kelly_fraction,
                'recommended_bet': bet_amount if is_recommended else 0,
                'recommended': is_recommended,
                'confidence': prediction_results['model_agreement'],
                'ev_percentage': ev * 100,
                'reason': '' if is_recommended else get_rejection_reason(ev, min_edge, our_prob, min_confidence, roi, min_roi)
            }
    
    return betting_analysis

def analyze_similar_matchups(team1_stats, team2_stats):
    """
    Analyze performance in similar matchups to provide context.
    """
    similar_matchups = []
    
    # Look for matches with similar context in team1's history
    if 'matches' in team1_stats and isinstance(team1_stats['matches'], list):
        # Find matches against similar strength opponents
        team2_win_rate = team2_stats.get('win_rate', 0.5)
        
        for match in team1_stats['matches']:
            # Try to find the opponent's data
            opponent_name = match.get('opponent_name', '')
            if opponent_name and 'opponent_stats' in team1_stats:
                if opponent_name in team1_stats['opponent_stats']:
                    opp_stats = team1_stats['opponent_stats'][opponent_name]
                    opp_win_rate = opp_stats.get('win_rate', 0.5)
                    
                    # Check if similar strength opponent (within 10%)
                    if abs(opp_win_rate - team2_win_rate) < 0.1:
                        similar_matchups.append({
                            'opponent': opponent_name,
                            'result': 'win' if match.get('team_won', False) else 'loss',
                            'score': f"{match.get('team_score', 0)}-{match.get('opponent_score', 0)}"
                        })
    
    # Print results if found
    if similar_matchups:
        print("\nSIMILAR MATCHUP HISTORY:")
        print("-" * 80)
        print(f"Found {len(similar_matchups)} matches where {team1_stats['team_name']} faced opponents of similar strength to {team2_stats['team_name']}:")
        
        wins = sum(1 for m in similar_matchups if m['result'] == 'win')
        print(f"Record: {wins}-{len(similar_matchups) - wins} ({wins/len(similar_matchups):.2%})")
        
        # Show most recent 5
        for i, match in enumerate(similar_matchups[:5]):
            print(f"  vs {match['opponent']}: {match['result'].upper()} {match['score']}")

def print_prediction_report(results, team1_stats, team2_stats):
    """
    Print detailed prediction report with team stats and betting recommendations.
    """
    team1_name = results['team1_name']
    team2_name = results['team2_name']
    
    print(f"\n{'='*80}")
    print(f"DETAILED PREDICTION REPORT: {team1_name} vs {team2_name}")
    print(f"{'='*80}")
    
    # Team comparison section
    print("\nTEAM COMPARISON:")
    print(f"{'Metric':<30} {team1_name:<25} {team2_name:<25}")
    print("-" * 80)
    
    # Function to safely print metrics
    def print_metric(label, key1, key2=None, format_str="{:.4f}", default="N/A"):
        val1 = "N/A"
        val2 = "N/A"
        
        # Get value from nested keys if provided
        if key2:
            if key1 in team1_stats and key2 in team1_stats[key1]:
                try:
                    val1 = format_str.format(team1_stats[key1][key2])
                except (ValueError, TypeError):
                    val1 = str(team1_stats[key1][key2])
            
            if key1 in team2_stats and key2 in team2_stats[key1]:
                try:
                    val2 = format_str.format(team2_stats[key1][key2])
                except (ValueError, TypeError):
                    val2 = str(team2_stats[key1][key2])
        else:
            if key1 in team1_stats:
                try:
                    val1 = format_str.format(team1_stats[key1])
                except (ValueError, TypeError):
                    val1 = str(team1_stats[key1])
            
            if key1 in team2_stats:
                try:
                    val2 = format_str.format(team2_stats[key1])
                except (ValueError, TypeError):
                    val2 = str(team2_stats[key1])
        
        print(f"{label:<30} {val1:<25} {val2:<25}")
    
    # Basic stats
    print_metric("Overall Win Rate", "win_rate", format_str="{:.2%}")
    print_metric("Recent Form", "recent_form", format_str="{:.2%}")
    print_metric("Total Matches", "matches", format_str="{}")
    print_metric("Avg Score Per Map", "avg_score", format_str="{:.2f}")
    print_metric("Avg Opponent Score", "avg_opponent_score", format_str="{:.2f}")
    print_metric("Score Differential", "score_differential", format_str="{:.2f}")
    
    # Head-to-head section
    print("\nHEAD-TO-HEAD ANALYSIS:")
    
    # Check if team1 has head-to-head data with team2
    h2h_found = False
    h2h_stats = None
    
    if 'opponent_stats' in team1_stats:
        for opponent_name, stats in team1_stats['opponent_stats'].items():
            if opponent_name.lower() == team2_name.lower() or team2_name.lower() in opponent_name.lower():
                h2h_stats = stats
                h2h_found = True
                break
    
    if h2h_found and h2h_stats:
        print(f"  {team1_name} vs {team2_name} head-to-head:")
        print(f"  - Matches: {h2h_stats.get('matches', 0)}")
        print(f"  - {team1_name} Win Rate: {h2h_stats.get('win_rate', 0):.2%}")
        print(f"  - Avg Score: {h2h_stats.get('avg_score', 0):.2f} - {h2h_stats.get('avg_opponent_score', 0):.2f}")
        print(f"  - Score Differential: {h2h_stats.get('score_differential', 0):.2f}")
    else:
        print(f"  No direct head-to-head data found between {team1_name} and {team2_name}")
    
    # Print advanced player stats if available
    if 'avg_player_rating' in team1_stats and 'avg_player_rating' in team2_stats:
        print("\nPLAYER STATISTICS:")
        print("-" * 80)
        print_metric("Avg Player Rating", "avg_player_rating", format_str="{:.2f}")
        print_metric("Avg ACS", "avg_player_acs", format_str="{:.2f}")
        print_metric("Avg K/D Ratio", "avg_player_kd", format_str="{:.2f}")
        print_metric("Avg KAST", "avg_player_kast", format_str="{:.2%}")
        print_metric("Avg ADR", "avg_player_adr", format_str="{:.2f}")
        print_metric("Avg Headshot %", "avg_player_headshot", format_str="{:.2%}")
        print_metric("Star Player Rating", "star_player_rating", format_str="{:.2f}")
        print_metric("Team Consistency", "team_consistency", format_str="{:.2f}")
        print_metric("FK/FD Ratio", "fk_fd_ratio", format_str="{:.2f}")
    
    # Print economy stats if available
    if 'pistol_win_rate' in team1_stats and 'pistol_win_rate' in team2_stats:
        print("\nECONOMY STATISTICS:")
        print("-" * 80)
        print_metric("Pistol Round Win Rate", "pistol_win_rate", format_str="{:.2%}")
        print_metric("Eco Round Win Rate", "eco_win_rate", format_str="{:.2%}")
        print_metric("Full Buy Win Rate", "full_buy_win_rate", format_str="{:.2%}")
        print_metric("Economy Efficiency", "economy_efficiency", format_str="{:.2f}")
    
    # Print map performance if available
    if 'map_performance' in team1_stats and 'map_performance' in team2_stats:
        print("\nMAP PERFORMANCE:")
        print("-" * 80)
        
        # Find common maps
        team1_maps = set(team1_stats['map_performance'].keys())
        team2_maps = set(team2_stats['map_performance'].keys())
        common_maps = team1_maps.intersection(team2_maps)
        
        for map_name in common_maps:
            print(f"\n  {map_name}:")
            t1_map = team1_stats['map_performance'][map_name]
            t2_map = team2_stats['map_performance'][map_name]
            
            if 'win_rate' in t1_map and 'win_rate' in t2_map:
                print(f"  - {team1_name} Win Rate: {t1_map['win_rate']:.2%} ({t1_map.get('wins', 0)}/{t1_map.get('played', 0)})")
                print(f"  - {team2_name} Win Rate: {t2_map['win_rate']:.2%} ({t2_map.get('wins', 0)}/{t2_map.get('played', 0)})")
            
            if 'attack_win_rate' in t1_map and 'attack_win_rate' in t2_map:
                print(f"  - {team1_name} Attack Win Rate: {t1_map['attack_win_rate']:.2%}")
                print(f"  - {team2_name} Attack Win Rate: {t2_map['attack_win_rate']:.2%}")
                
            if 'defense_win_rate' in t1_map and 'defense_win_rate' in t2_map:
                print(f"  - {team1_name} Defense Win Rate: {t1_map['defense_win_rate']:.2%}")
                print(f"  - {team2_name} Defense Win Rate: {t2_map['defense_win_rate']:.2%}")
    
    # Print match prediction
    print("\nMATCH PREDICTION:")
    print("-" * 80)
    print(f"  {team1_name} Win Probability: {results['team1_win_prob']:.2%}")
    print(f"  {team2_name} Win Probability: {results['team2_win_prob']:.2%}")
    print(f"  Confidence Interval: ({results['confidence_interval'][0]:.2%} - {results['confidence_interval'][1]:.2%})")
    print(f"  Model Agreement: {results['model_agreement']:.2f} (Higher is better)")
    
    # Print other bet types
    print("\nBET TYPE PROBABILITIES:")
    print("-" * 80)
    print(f"  {team1_name} +1.5 Maps: {results['team1_plus_1_5_prob']:.2%}")
    print(f"  {team2_name} +1.5 Maps: {results['team2_plus_1_5_prob']:.2%}")
    print(f"  {team1_name} -1.5 Maps (2-0 win): {results['team1_minus_1_5_prob']:.2%}")
    print(f"  {team2_name} -1.5 Maps (2-0 win): {results['team2_minus_1_5_prob']:.2%}")
    print(f"  Over 2.5 Maps: {results['over_2_5_maps_prob']:.2%}")
    print(f"  Under 2.5 Maps: {results['under_2_5_maps_prob']:.2%}")
    
    # Update the betting recommendations section:
    if 'betting_analysis' in results and results['betting_analysis']:
        print("\nBETTING RECOMMENDATIONS:")
        print("-" * 80)
        
        recommended_bets = []
        rejected_bets = []
        
        for bet_type, analysis in results['betting_analysis'].items():
             # Extract team name instead of generic "team1" or "team2"
            if 'team1' in bet_type:
                team_name = team1_stats['team_name']
                bet_desc = f"{team_name} {bet_type.replace('team1_', '').replace('_', ' ').upper()}"
            elif 'team2' in bet_type:
                team_name = team2_stats['team_name']
                bet_desc = f"{team_name} {bet_type.replace('team2_', '').replace('_', ' ').upper()}"
            else:
                bet_desc = bet_type.replace('_', ' ').upper()
            
            if analysis['recommended']:
                edge = analysis.get('expected_value', 0)
                roi = analysis.get('roi', 0)
                recommended_bet = analysis.get('recommended_bet', 0)
                recommended_bets.append((bet_desc, edge, roi, recommended_bet))
                
                print(f"  RECOMMENDED BET: {bet_desc}")
                print(f"  - Our Probability: {analysis['our_probability']:.2%}")
                print(f"  - Implied Probability: {analysis['implied_probability']:.2%}")
                print(f"  - Edge: {edge:.2%}")
                print(f"  - ROI: {roi:.2%}")
                print(f"  - Expected Value: {analysis.get('ev_percentage', 0):.2f}%")
                print(f"  - Confidence: {analysis.get('confidence', 0):.2f}")
                print(f"  - Kelly Fraction: {analysis.get('kelly_fraction', 0):.4f}")
                print(f"  - Recommended Bet Amount: ${recommended_bet}")
                print("")
                
                # Explain the recommendation
                explain_bet_recommendation(bet_type, analysis, results, team1_stats['team_name'], team2_stats['team_name'])
            else:
                # Track rejected bets
                rejected_bets.append((bet_desc, analysis.get('reason', 'Not recommended')))
        
        if not recommended_bets:
            print("  No profitable betting opportunities identified for this match.")
        else:
            # Sort by ROI
            recommended_bets.sort(key=lambda x: x[2], reverse=True)
            print("\nBETS RANKED BY EXPECTED ROI:")
            for i, (bet, edge, roi, amount) in enumerate(recommended_bets):
                print(f"  {i+1}. {bet}: {edge:.2%} edge, {roi:.2%} ROI, bet ${amount}")
        
        # Show rejected bets
        if rejected_bets:
            print("\nREJECTED BETS:")
            for bet, reason in rejected_bets:
                print(f"  {bet}: {reason}")
            # Sort by edge
            recommended_bets.sort(key=lambda x: x[1], reverse=True)
            print("\nBETS RANKED BY EDGE:")
            for i, (bet, edge, roi, amount) in enumerate(recommended_bets):
                print(f"  {i+1}. {bet}: {edge:.2%} edge, bet ${amount}")

def calculate_drawdown_metrics(bankroll_history):
    """
    Calculate maximum drawdown and other drawdown metrics.
    
    Args:
        bankroll_history: List of dictionaries containing bankroll history
        
    Returns:
        dict: Drawdown metrics including maximum drawdown
    """
    if not bankroll_history:
        return {
            'max_drawdown_pct': 0,
            'max_drawdown_amount': 0,
            'drawdown_periods': 0,
            'avg_drawdown_pct': 0,
            'max_drawdown_duration': 0
        }
    
    # Extract bankroll values
    bankrolls = [entry['bankroll'] for entry in bankroll_history]
    
    # Initialize variables
    peak = bankrolls[0]
    max_drawdown = 0
    max_drawdown_amount = 0
    drawdown_periods = 0
    current_drawdown_start = None
    max_drawdown_duration = 0
    current_drawdown_duration = 0
    all_drawdowns = []
    
    # Calculate drawdown metrics
    for i, value in enumerate(bankrolls):
        if value > peak:
            # New peak
            peak = value
            # If we were in a drawdown, it's now over
            if current_drawdown_start is not None:
                current_drawdown_start = None
                current_drawdown_duration = 0
        else:
            # In a drawdown
            drawdown = (peak - value) / peak
            drawdown_amount = peak - value
            
            # If this is the start of a new drawdown
            if current_drawdown_start is None:
                current_drawdown_start = i
                drawdown_periods += 1
            
            current_drawdown_duration += 1
            
            # Update max drawdown if current drawdown is larger
            if drawdown > max_drawdown:
                max_drawdown = drawdown
                max_drawdown_amount = drawdown_amount
                
            # Update max drawdown duration
            if current_drawdown_duration > max_drawdown_duration:
                max_drawdown_duration = current_drawdown_duration
                
            # Record this drawdown
            if drawdown > 0:
                all_drawdowns.append(drawdown)
    
    # Calculate average drawdown
    avg_drawdown = sum(all_drawdowns) / len(all_drawdowns) if all_drawdowns else 0
    
    return {
        'max_drawdown_pct': max_drawdown * 100,  # Convert to percentage
        'max_drawdown_amount': max_drawdown_amount,
        'drawdown_periods': drawdown_periods,
        'avg_drawdown_pct': avg_drawdown * 100,  # Convert to percentage
        'max_drawdown_duration': max_drawdown_duration
    }

def explain_bet_recommendation(bet_type, analysis, results, team1_name, team2_name):
    """Provide explanation for why a bet is recommended based on the stats"""
    
    print("  EXPLANATION:")
    
    # Explain moneyline bets
    if bet_type == 'team1_ml':
        # Check which factors favor team1
        print(f"  The model favors {team1_name} to win the match with {analysis['our_probability']:.2%} probability.")
        
        # Explain head-to-head advantage if it exists
        if results.get('h2h_advantage_team1', 0) > 0:
            print(f"  {team1_name} has a strong historical head-to-head advantage against {team2_name}.")
        
        # Explain any other key advantages
        if results.get('team1_win_prob', 0) > 0.6:
            print(f"  {team1_name}'s overall form and statistics indicate a significant advantage.")
            
    elif bet_type == 'team2_ml':
        print(f"  The model favors {team2_name} to win the match with {analysis['our_probability']:.2%} probability.")
        
        # Explain head-to-head advantage if it exists
        if results.get('h2h_advantage_team1', 0) < 0:
            print(f"  {team2_name} has a strong historical head-to-head advantage against {team1_name}.")
        
        # Explain any other key advantages
        if results.get('team2_win_prob', 0) > 0.6:
            print(f"  {team2_name}'s overall form and statistics indicate a significant advantage.")
    
    # Explain +1.5 bets
    elif bet_type == 'team1_plus_1_5':
        print(f"  {team1_name} has a {analysis['our_probability']:.2%} probability of winning at least one map.")
        print(f"  This is significantly higher than the {analysis['implied_probability']:.2%} implied by the odds.")
        if results.get('team1_plus_1_5_prob', 0) > 0.8:
            print(f"  Even as an underdog in the match, {team1_name} is likely to take at least one map.")
            
    elif bet_type == 'team2_plus_1_5':
        print(f"  {team2_name} has a {analysis['our_probability']:.2%} probability of winning at least one map.")
        print(f"  This is significantly higher than the {analysis['implied_probability']:.2%} implied by the odds.")
        if results.get('team2_plus_1_5_prob', 0) > 0.8:
            print(f"  Even as an underdog in the match, {team2_name} is likely to take at least one map.")
    
    # Explain -1.5 bets
    elif bet_type == 'team1_minus_1_5':
        print(f"  {team1_name} has a {analysis['our_probability']:.2%} probability of winning 2-0.")
        print(f"  The model suggests {team1_name} is significantly stronger than {team2_name} and can win without dropping a map.")
        
    elif bet_type == 'team2_minus_1_5':
        print(f"  {team2_name} has a {analysis['our_probability']:.2%} probability of winning 2-0.")
        print(f"  The model suggests {team2_name} is significantly stronger than {team1_name} and can win without dropping a map.")
    
    # Explain over/under bets
    elif bet_type == 'over_2_5_maps':
        print(f"  The match has a {analysis['our_probability']:.2%} probability of going to 3 maps.")
        print(f"  The teams appear evenly matched and likely to split the first two maps.")
        
    elif bet_type == 'under_2_5_maps':
        print(f"  The match has a {analysis['our_probability']:.2%} probability of ending in 2 maps.")
        print(f"  One team appears significantly stronger and likely to win 2-0.")
    
    # Explain why the odds provide value
    print(f"  The bookmaker's odds of {1/analysis['implied_probability']:.2f} represent value compared to our model's assessment.")
    print(f"  Long-term expected value: ${100 * (analysis['our_probability'] - analysis['implied_probability']):.2f} per $100 wagered.")

def input_odds_data():
    """Function to manually input odds data from bookmaker"""
    print("\nPlease enter the odds from your bookmaker for this match:")
    print("(Enter decimal odds format, e.g. 2.50 for +150, 1.67 for -150)")
    print("Leave blank for any bet types you're not interested in.")
    
    odds_data = {}
    
    # Team names for clarity
    team1 = input("\nEnter Team 1 name: ")
    team2 = input("Enter Team 2 name: ")
    
    # Moneyline
    try:
        odds = float(input(f"\n{team1} moneyline odds: ") or 0)
        if odds > 0:
            odds_data['team1_ml_odds'] = odds
    except ValueError:
        print("Invalid input, skipping.")
    
    try:
        odds = float(input(f"{team2} moneyline odds: ") or 0)
        if odds > 0:
            odds_data['team2_ml_odds'] = odds
    except ValueError:
        print("Invalid input, skipping.")
    
    # +1.5 maps
    try:
        odds = float(input(f"\n{team1} +1.5 maps odds: ") or 0)
        if odds > 0:
            odds_data['team1_plus_1_5_odds'] = odds
    except ValueError:
        print("Invalid input, skipping.")
    
    try:
        odds = float(input(f"{team2} +1.5 maps odds: ") or 0)
        if odds > 0:
            odds_data['team2_plus_1_5_odds'] = odds
    except ValueError:
        print("Invalid input, skipping.")
    
    # -1.5 maps
    try:
        odds = float(input(f"\n{team1} -1.5 maps odds: ") or 0)
        if odds > 0:
            odds_data['team1_minus_1_5_odds'] = odds
    except ValueError:
        print("Invalid input, skipping.")
    
# Over/Under 2.5 maps
    try:
        odds = float(input(f"\nOver 2.5 maps odds: ") or 0)
        if odds > 0:
            odds_data['over_2_5_maps_odds'] = odds
    except ValueError:
        print("Invalid input, skipping.")
    
    try:
        odds = float(input(f"Under 2.5 maps odds: ") or 0)
        if odds > 0:
            odds_data['under_2_5_maps_odds'] = odds
    except ValueError:
        print("Invalid input, skipping.")
    
    return team1, team2, odds_data

def load_prediction_artifacts():
    """
    Load the diverse ensemble and artifacts needed for prediction.
    """
    print("Loading prediction models and artifacts...")
    
    # Try to load the diverse ensemble
    try:
        with open('diverse_ensemble.pkl', 'rb') as f:
            ensemble_models = pickle.load(f)
        print(f"Loaded diverse ensemble with {len(ensemble_models)} models")
    except Exception as e:
        print(f"Error loading diverse ensemble: {e}")
        # Fall back to individual models if available
        try:
            ensemble_models = []
            for i in range(1, 11):
                try:
                    model_path = f'valorant_model_fold_{i}.h5'
                    model = load_model(model_path)
                    ensemble_models.append(('nn', model, None))
                    print(f"Loaded fallback model {i}/10")
                except:
                    # Skip if model doesn't exist
                    continue
            if not ensemble_models:
                print("Failed to load any models")
                return None, None, None
        except Exception as e:
            print(f"Error loading fallback models: {e}")
            return None, None, None
    
    # Load feature metadata
    try:
        with open('feature_metadata.pkl', 'rb') as f:
            feature_metadata = pickle.load(f)
        selected_features = feature_metadata.get('selected_features', [])
        print(f"Loaded {len(selected_features)} selected features")
    except Exception as e:
        print(f"Error loading feature metadata: {e}")
        try:
            with open('selected_feature_names.pkl', 'rb') as f:
                selected_features = pickle.load(f)
            print(f"Loaded {len(selected_features)} features from backup file")
        except Exception as e:
            print(f"Error loading feature names: {e}")
            # Last resort: try stable_features.pkl
            try:
                with open('stable_features.pkl', 'rb') as f:
                    selected_features = pickle.load(f)
                print(f"Loaded {len(selected_features)} features from stable_features.pkl")
            except Exception as e:
                print(f"Error loading any feature lists: {e}")
                return ensemble_models, None, None
    
    # Return ensemble_models, None for scaler (handled in ensemble), and selected_features
    return ensemble_models, None, selected_features

def track_betting_performance(prediction, bet_placed, bet_amount, outcome, odds):
    """
    Track betting performance over time.
    
    Args:
        prediction (dict): The prediction made by the model
        bet_placed (str): Type of bet placed
        bet_amount (float): Amount bet
        outcome (bool): Whether the bet won
        odds (float): Decimal odds offered
        
    Returns:
        None
    """
    # Load existing performance data or create new
    try:
        with open('betting_performance.json', 'r') as f:
            performance = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        performance = {
            'total_bets': 0,
            'wins': 0,
            'losses': 0,
            'total_wagered': 0,
            'total_returns': 0,
            'roi': 0,
            'bets': []
        }
    
    # Create new bet record
    bet_record = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'teams': f"{prediction['team1_name']} vs {prediction['team2_name']}",
        'bet_type': bet_placed,
        'amount': bet_amount,
        'odds': odds,
        'predicted_prob': prediction['betting_analysis'][bet_placed]['our_probability'],
        'implied_prob': prediction['betting_analysis'][bet_placed]['implied_probability'],
        'edge': prediction['betting_analysis'][bet_placed]['expected_value'],
        'outcome': 'win' if outcome else 'loss',
        'return': bet_amount * odds if outcome else 0,
        'profit': bet_amount * (odds - 1) if outcome else -bet_amount
    }
    
    # Update performance metrics
    performance['total_bets'] += 1
    if outcome:
        performance['wins'] += 1
    else:
        performance['losses'] += 1
    
    performance['total_wagered'] += bet_amount
    performance['total_returns'] += bet_record['return']
    performance['roi'] = (performance['total_returns'] - performance['total_wagered']) / performance['total_wagered'] if performance['total_wagered'] > 0 else 0
    
    # Add to history
    performance['bets'].append(bet_record)
    
    # Save updated performance
    with open('betting_performance.json', 'w') as f:
        json.dump(performance, f, indent=2)
    
    # Print performance update
    print("\nBetting Performance Updated:")
    print(f"Record: {performance['wins']}-{performance['losses']} ({performance['wins']/performance['total_bets']:.2%})")
    print(f"Total Wagered: ${performance['total_wagered']:.2f}")
    print(f"Total Returns: ${performance['total_returns']:.2f}")
    print(f"Profit: ${performance['total_returns'] - performance['total_wagered']:.2f}")
    print(f"ROI: {performance['roi']:.2%}")

def view_betting_performance():
    """Display historical betting performance with visualizations."""
    try:
        with open('betting_performance.json', 'r') as f:
            performance = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        print("No betting history found.")
        return
    
    print("\n===== BETTING PERFORMANCE SUMMARY =====")
    print(f"Total Bets: {performance['total_bets']}")
    print(f"Record: {performance['wins']}-{performance['losses']} ({performance['wins']/performance['total_bets']:.2%})")
    print(f"Total Wagered: ${performance['total_wagered']:.2f}")
    print(f"Total Returns: ${performance['total_returns']:.2f}")
    print(f"Profit: ${performance['total_returns'] - performance['total_wagered']:.2f}")
    print(f"ROI: {performance['roi']:.2%}")
    
    # Analyze by bet type
    bet_types = {}
    for bet in performance['bets']:
        bet_type = bet['bet_type']
        if bet_type not in bet_types:
            bet_types[bet_type] = {
                'total': 0,
                'wins': 0,
                'amount': 0,
                'returns': 0,
                'avg_edge': []
            }
        
        bet_types[bet_type]['total'] += 1
        if bet['outcome'] == 'win':
            bet_types[bet_type]['wins'] += 1
        bet_types[bet_type]['amount'] += bet['amount']
        bet_types[bet_type]['returns'] += bet['return']
        bet_types[bet_type]['avg_edge'].append(bet['edge'])
    
    print("\n===== PERFORMANCE BY BET TYPE =====")
    for bet_type, stats in bet_types.items():
        win_rate = stats['wins'] / stats['total'] if stats['total'] > 0 else 0
        roi = (stats['returns'] - stats['amount']) / stats['amount'] if stats['amount'] > 0 else 0
        avg_edge = sum(stats['avg_edge']) / len(stats['avg_edge']) if stats['avg_edge'] else 0
        
        print(f"\n{bet_type}:")
        print(f"  Record: {stats['wins']}-{stats['total']-stats['wins']} ({win_rate:.2%})")
        print(f"  Wagered: ${stats['amount']:.2f}")
        print(f"  Returns: ${stats['returns']:.2f}")
        print(f"  Profit: ${stats['returns'] - stats['amount']:.2f}")
        print(f"  ROI: {roi:.2%}")
        print(f"  Avg Edge: {avg_edge:.2%}")
    
    # Create visualizations if matplotlib is available
    try:
        # Prepare data for plots
        dates = [bet['timestamp'] for bet in performance['bets']]
        profits = []
        running_profit = 0
        for bet in performance['bets']:
            running_profit += bet['profit']
            profits.append(running_profit)
        
        # Create plots
        plt.figure(figsize=(12, 10))
        
        # Profit over time
        plt.subplot(2, 2, 1)
        plt.plot(range(len(profits)), profits, 'b-')
        plt.title('Cumulative Profit Over Time')
        plt.xlabel('Bet Number')
        plt.ylabel('Profit ($)')
        plt.grid(True)
        
        # Win rate by bet type
        plt.subplot(2, 2, 2)
        bet_type_names = list(bet_types.keys())
        win_rates = [bet_types[bt]['wins'] / bet_types[bt]['total'] if bet_types[bt]['total'] > 0 else 0 for bt in bet_type_names]
        
        plt.bar(range(len(bet_type_names)), win_rates)
        plt.xticks(range(len(bet_type_names)), bet_type_names, rotation=45)
        plt.title('Win Rate by Bet Type')
        plt.ylabel('Win Rate')
        plt.axhline(y=0.5, color='r', linestyle='-')
        plt.grid(True)
        
        # ROI by bet type
        plt.subplot(2, 2, 3)
        rois = [(bet_types[bt]['returns'] - bet_types[bt]['amount']) / bet_types[bt]['amount'] if bet_types[bt]['amount'] > 0 else 0 for bt in bet_type_names]
        
        plt.bar(range(len(bet_type_names)), rois)
        plt.xticks(range(len(bet_type_names)), bet_type_names, rotation=45)
        plt.title('ROI by Bet Type')
        plt.ylabel('ROI')
        plt.axhline(y=0, color='r', linestyle='-')
        plt.grid(True)
        
        # Edge vs. Outcome
        plt.subplot(2, 2, 4)
        edges = [bet['edge'] for bet in performance['bets']]
        outcomes = [1 if bet['outcome'] == 'win' else 0 for bet in performance['bets']]
        
        plt.scatter(edges, outcomes, alpha=0.6)
        plt.title('Bet Outcome vs. Predicted Edge')
        plt.xlabel('Predicted Edge')
        plt.ylabel('Outcome (1=Win, 0=Loss)')
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('betting_performance.png')
        plt.close()
        
        print("\nPerformance visualizations saved to betting_performance.png")
    except Exception as e:
        print(f"Error creating visualizations: {e}")

#-------------------------------------------------------------------------
# MAIN FUNCTION AND CLI INTERFACE
#-------------------------------------------------------------------------

@debug_func
def train_with_consistent_features(X, y, n_splits=10, random_state=42):
    """
    Enhanced training with focus on features that actually predict profitability.
    """
    print(f"\nTraining with enhanced {n_splits}-fold cross-validation for profitability")
    
    # Convert to DataFrame for easier manipulation
    df = clean_feature_data(X)
    X_arr = df.values
    y_arr = np.array(y)
    
    # More aggressive class balancing for better edge detection
    class_balance = np.bincount(y_arr)
    print(f"Original class distribution: {class_balance}")
    
    if len(class_balance) >= 2 and min(class_balance) < len(y_arr) * 0.4:
        print("Applying enhanced class balancing for edge detection")
        
        minority_class = np.argmin(class_balance)
        majority_indices = np.where(y_arr != minority_class)[0]
        minority_indices = np.where(y_arr == minority_class)[0]
        
        # More aggressive balancing
        if len(majority_indices) > len(minority_indices) * 1.5:
            keep_majority = np.random.choice(majority_indices,
                                           min(len(minority_indices) * 2, len(majority_indices)),
                                           replace=False)
            balanced_indices = np.concatenate([keep_majority, minority_indices])
        else:
            balanced_indices = np.concatenate([majority_indices, minority_indices])
        
        X_arr = X_arr[balanced_indices]
        y_arr = y_arr[balanced_indices]
        print(f"Balanced class distribution: {np.bincount(y_arr)}")
    
    # Use TimeSeriesSplit to avoid lookahead bias (crucial for betting)
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    # Track feature importance across folds
    all_feature_importances = np.zeros(df.shape[1])
    feature_stability_scores = np.zeros(df.shape[1])
    
    # Enhanced feature selection focusing on profitability predictors
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_arr)):
        print(f"Processing fold {fold + 1}/{n_splits}")
        
        X_train, X_val = X_arr[train_idx], X_arr[val_idx]
        y_train, y_val = y_arr[train_idx], y_arr[val_idx]
        
        # Apply SMOTE more conservatively
        class_counts = np.bincount(y_train)
        if np.min(class_counts) / np.sum(class_counts) < 0.35:
            try:
                if np.min(class_counts) >= 3:
                    min_samples = np.min(class_counts)
                    k_neighbors = min(3, min_samples-1)
                    smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
                    X_train, y_train = smote.fit_resample(X_train, y_train)
            except Exception:
                pass
        
        # Enhanced feature selection for betting edge detection
        rf = RandomForestClassifier(
            n_estimators=300,  # More trees for better stability
            max_depth=6,       # Prevent overfitting
            min_samples_split=25,
            min_samples_leaf=10,
            max_features='sqrt',
            random_state=random_state,
            class_weight='balanced_subsample'  # Better for imbalanced data
        )
        rf.fit(X_train, y_train)
        
        importances = rf.feature_importances_
        all_feature_importances += importances
        
        # Enhanced stability scoring
        stability_threshold = np.percentile(importances, 75)  # Top 25% features
        stable_features = importances >= stability_threshold
        feature_stability_scores += stable_features.astype(int)
    
    # Calculate combined feature scores
    avg_importances = all_feature_importances / n_splits
    stability_scores = feature_stability_scores / n_splits
    
    # Weight stability more heavily for betting models
    combined_scores = avg_importances * (0.6 + 0.4 * stability_scores)
    
    # Select features more conservatively
    indices = np.argsort(combined_scores)[::-1]
    cumulative_importance = np.cumsum(combined_scores[indices])
    total_importance = combined_scores.sum()
    
    # Use top features that capture 80% of importance
    n_features = np.where(cumulative_importance >= total_importance * 0.80)[0][0] + 1
    n_features = min(max(n_features, 12), 25)  # Between 12-25 features
    
    top_indices = indices[:n_features]
    selected_features = [df.columns[i] for i in top_indices]
    feature_mask = np.zeros(df.shape[1], dtype=bool)
    feature_mask[top_indices] = True
    
    print(f"Selected {n_features} features with enhanced stability criteria")
    
    # Final training with selected features
    train_idx, val_idx = list(tscv.split(X_arr))[-1]  # Use last split for final model
    X_train, X_val = X_arr[train_idx], X_arr[val_idx]
    y_train, y_val = y_arr[train_idx], y_arr[val_idx]
    
    # Create diverse ensemble optimized for betting
    ensemble_models, scaler = create_diverse_ensemble(X_train, y_train, feature_mask, random_state)
    
    # Evaluate ensemble performance
    X_val_selected = X_val[:, feature_mask]
    predictions = []
    
    for model_type, model, model_scaler in ensemble_models:
        try:
            X_val_pred = X_val_selected.copy()
            if model_scaler is not None:
                X_val_pred = model_scaler.transform(X_val_selected)
            
            if model_type == 'nn':
                preds = model.predict(X_val_pred, verbose=0).flatten()
            else:
                preds = model.predict_proba(X_val_pred)[:, 1]
            predictions.append(preds)
        except Exception as e:
            print(f"Warning: {model_type} model failed: {e}")
            continue
    
    if predictions:
        ensemble_preds = np.mean(predictions, axis=0)
        ensemble_binary = (ensemble_preds > 0.5).astype(int)
        
        accuracy = accuracy_score(y_val, ensemble_binary)
        precision = precision_score(y_val, ensemble_binary, zero_division=0)
        recall = recall_score(y_val, ensemble_binary, zero_division=0)
        f1 = f1_score(y_val, ensemble_binary, zero_division=0)
        auc = roc_auc_score(y_val, ensemble_preds)
        
        print(f"Ensemble Performance: Accuracy={accuracy:.4f}, AUC={auc:.4f}")
    else:
        accuracy = precision = recall = f1 = auc = 0
        print("Warning: No models succeeded in ensemble")
    
    # Save enhanced models and metadata
    with open('diverse_ensemble.pkl', 'wb') as f:
        pickle.dump(ensemble_models, f)
    
    with open('feature_mask.pkl', 'wb') as f:
        pickle.dump(feature_mask, f)
    
    with open('selected_feature_names.pkl', 'wb') as f:
        pickle.dump(selected_features, f)
    
    # Enhanced metadata for betting
    feature_metadata = {
        'selected_features': selected_features,
        'feature_importances': dict(zip(selected_features, combined_scores[top_indices])),
        'stability_scores': dict(zip(selected_features, stability_scores[top_indices])),
        'selection_method': 'enhanced_profitability_focused',
        'n_folds': n_splits,
        'profit_optimized': True
    }
    
    with open('feature_metadata.pkl', 'wb') as f:
        pickle.dump(feature_metadata, f)
    
    ensemble_metrics = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc,
        'feature_count': n_features,
        'stability_threshold': 0.75,
        'profit_optimized': True
    }
    
    print(f"Enhanced training complete. Model optimized for betting profitability.")
    return ensemble_models, scaler, selected_features, ensemble_metrics

#-------------------------------------------------------------------------
# BACKTESTING SYSTEM
#-------------------------------------------------------------------------

def load_backtesting_models():
    """
    Enhanced function to load ensemble models and metadata for backtesting
    with improved error handling and logging.
    """
    print("\n========== LOADING RETRAINED MODELS ==========")
    
    # 1. Load the diverse ensemble with better error handling
    ensemble_models = None
    try:
        print("Attempting to load diverse ensemble from diverse_ensemble.pkl...")
        with open('diverse_ensemble.pkl', 'rb') as f:
            ensemble_models = pickle.load(f)
            print(f"SUCCESS: Loaded diverse ensemble with {len(ensemble_models)} models")
            
            # Log model types for verification
            model_types = {}
            for model_type, _, _ in ensemble_models:
                if model_type not in model_types:
                    model_types[model_type] = 0
                model_types[model_type] += 1
            
            print("Model composition:")
            for model_type, count in model_types.items():
                print(f"  - {model_type.upper()}: {count} models")
    except Exception as e:
        print(f"ERROR: Failed to load diverse ensemble: {e}")
        print("Attempting to load individual fold models as fallback...")
        
        # Try loading individual fold models
        ensemble_models = []
        for i in range(1, 11):
            try:
                model = load_model(f'valorant_model_fold_{i}.h5')
                ensemble_models.append(('nn', model, None))
                print(f"Loaded fold model {i}")
            except Exception as model_e:
                print(f"Could not load model fold_{i}: {model_e}")
        
        if ensemble_models:
            print(f"SUCCESS: Loaded {len(ensemble_models)} individual fold models as fallback")
        else:
            print("ERROR: Failed to load any models. Backtesting cannot proceed.")
            return None, None
    
    # 2. Load feature metadata with enhanced error handling and priority order
    selected_features = None
    feature_metadata = None
    
    # Try loading from feature_metadata.pkl first (preferred source)
    try:
        print("Attempting to load feature metadata from feature_metadata.pkl...")
        with open('feature_metadata.pkl', 'rb') as f:
            feature_metadata = pickle.load(f)
            selected_features = feature_metadata.get('selected_features')
            feature_importances = feature_metadata.get('feature_importances', {})
            
            print(f"SUCCESS: Loaded {len(selected_features)} features from feature_metadata.pkl")
            print(f"Top 5 features by importance:")
            sorted_features = sorted(feature_importances.items(), 
                                  key=lambda x: x[1], reverse=True)[:5]
            for feature, importance in sorted_features:
                print(f"  - {feature}: {importance:.4f}")
    except Exception as e:
        print(f"ERROR: Failed to load feature_metadata.pkl: {e}")
    
    # If that failed, try loading from selected_feature_names.pkl
    if not selected_features:
        try:
            print("Attempting to load from selected_feature_names.pkl...")
            with open('selected_feature_names.pkl', 'rb') as f:
                selected_features = pickle.load(f)
                print(f"SUCCESS: Loaded {len(selected_features)} features from selected_feature_names.pkl")
        except Exception as e:
            print(f"ERROR: Failed to load selected_feature_names.pkl: {e}")
    
    # If that failed too, try stable_features.pkl
    if not selected_features:
        try:
            print("Attempting to load from stable_features.pkl...")
            with open('stable_features.pkl', 'rb') as f:
                selected_features = pickle.load(f)
                print(f"SUCCESS: Loaded {len(selected_features)} features from stable_features.pkl")
        except Exception as e:
            print(f"ERROR: Failed to load stable_features.pkl: {e}")
    
    # Last resort: Load feature mask and extract feature names
    if not selected_features:
        try:
            print("Attempting to reconstruct features from feature_mask.pkl...")
            with open('feature_mask.pkl', 'rb') as f:
                feature_mask = pickle.load(f)
                # This assumes you have access to the original feature names
                # In a real implementation, you'd need to have them stored somewhere
                print("WARNING: Using feature mask without feature names is not ideal")
                # Create placeholder features
                selected_features = [f'feature_{i}' for i in range(sum(feature_mask))]
                print(f"Created {len(selected_features)} placeholder features from mask")
        except Exception as e:
            print(f"ERROR: Failed to load feature_mask.pkl: {e}")
    
    # If we still don't have features, create fallback feature list
    if not selected_features:
        print("WARNING: Failed to load any feature list. Creating fallback features.")
        # Create a list of likely important features based on domain knowledge
        selected_features = [
            'win_rate_diff', 'better_win_rate_team1', 'recent_form_diff',
            'better_recent_form_team1', 'score_diff_differential',
            'better_score_diff_team1', 'avg_score_diff', 'h2h_win_rate',
            'h2h_matches', 'h2h_score_diff', 'h2h_advantage_team1',
            'h2h_significant', 'total_matches', 'match_count_diff',
            'avg_win_rate', 'avg_recent_form', 'wins_diff', 'losses_diff',
            'win_loss_ratio_diff', 'player_rating_diff', 'better_player_rating_team1',
            'avg_player_rating', 'pistol_win_rate_diff', 'better_pistol_team1',
            'eco_win_rate_diff', 'semi_eco_win_rate_diff', 'full_buy_win_rate_diff',
            'economy_efficiency_diff', 'rating_x_win_rate', 'pistol_x_eco',
            'pistol_x_full_buy', 'first_blood_x_win_rate', 'h2h_x_win_rate'
        ]
        print(f"Created fallback feature list with {len(selected_features)} features")
    
    # If needed, pad the feature list to match expected model input shape
    if ensemble_models and hasattr(ensemble_models[0][1], 'layers'):
        try:
            # Get expected input dimensionality from first model
            model_type, model, _ = ensemble_models[0]
            if model_type == 'nn':
                expected_dim = model.layers[0].input_shape[1]
                
                if len(selected_features) < expected_dim:
                    print(f"WARNING: Selected features ({len(selected_features)}) less than expected model input dimension ({expected_dim})")
                    padding_count = expected_dim - len(selected_features)
                    padding_features = [f"padding_feature_{i}" for i in range(padding_count)]
                    selected_features.extend(padding_features)
                    print(f"Added {padding_count} padding features to match model input dimension")
        except Exception as e:
            print(f"ERROR checking model dimensions: {e}")
    
    # Load scalers if available
    try:
        print("Attempting to load feature scaler...")
        with open('ensemble_scaler.pkl', 'rb') as f:
            scaler = pickle.load(f)
            print("SUCCESS: Loaded ensemble_scaler.pkl")
            
            # Add scaler to ensemble models that need it
            enhanced_ensemble = []
            for model_type, model, model_scaler in ensemble_models:
                if model_type in ['lr', 'svm'] and model_scaler is None:
                    enhanced_ensemble.append((model_type, model, scaler))
                    print(f"Added scaler to {model_type} model")
                else:
                    enhanced_ensemble.append((model_type, model, model_scaler))
            
            ensemble_models = enhanced_ensemble
    except Exception as e:
        print(f"WARNING: Failed to load ensemble_scaler.pkl: {e}")
    
    print(f"Model loading complete. Ensemble has {len(ensemble_models)} models and {len(selected_features)} features")
    return ensemble_models, selected_features

def simulate_odds(team1_win_prob, vig=0.045, market_efficiency=0.92):
    """
    Improved odds simulation that better reflects real sportsbook behavior.
    This is crucial for realistic backtesting results.
    """
    # Input validation to prevent errors
    if not isinstance(team1_win_prob, (int, float)) or np.isnan(team1_win_prob):
        team1_win_prob = 0.5
    
    team1_win_prob = np.clip(team1_win_prob, 0.05, 0.95)
    
    # Add market noise - real markets aren't perfectly efficient
    market_noise = np.random.uniform(-0.02, 0.02)
    adjusted_prob = np.clip(team1_win_prob + market_noise, 0.15, 0.85)
    
    # Sportsbooks are more efficient for popular markets
    sharp_adjustment = np.random.uniform(0.88, 0.95)  # Market sharpness
    
    # Apply reverse juice - books favor the public favorite slightly
    public_bias = 0.01 if adjusted_prob > 0.55 else -0.01
    
    implied_team1 = (adjusted_prob * sharp_adjustment) + public_bias
    implied_team2 = ((1 - adjusted_prob) * sharp_adjustment) - public_bias
    
    # Ensure positive probabilities
    implied_team1 = max(0.05, implied_team1)
    implied_team2 = max(0.05, implied_team2)
    
    # Normalize with vig
    total_implied = implied_team1 + implied_team2 + vig
    final_team1_implied = implied_team1 / total_implied
    final_team2_implied = implied_team2 / total_implied
    
    # Convert to odds with realistic bounds and error handling
    try:
        team1_odds = max(1.15, min(8.0, 1 / max(0.125, final_team1_implied)))
        team2_odds = max(1.15, min(8.0, 1 / max(0.125, final_team2_implied)))
    except (ZeroDivisionError, OverflowError):
        team1_odds = 2.0
        team2_odds = 2.0
    
    # Derivative markets (more juice, less efficient)
    derivative_vig = vig * 1.4
    
    # Calculate derivative probabilities with better error handling
    try:
        # Plus lines (team + 1.5 maps) - higher implied probability
        team1_plus_implied = min(0.88, 1 - (1 - final_team1_implied) ** 1.4)
        team2_plus_implied = min(0.88, 1 - final_team2_implied ** 1.4)
        
        # Minus lines (team - 1.5 maps) - lower implied probability  
        team1_minus_implied = max(0.08, final_team1_implied ** 1.8)
        team2_minus_implied = max(0.08, final_team2_implied ** 1.8)
        
        # Totals market (over/under 2.5 maps)
        base_over_prob = 2 * final_team1_implied * final_team2_implied * 1.2
        over_implied = np.clip(base_over_prob + 0.15, 0.35, 0.65)
        under_implied = np.clip(1 - over_implied, 0.35, 0.65)
        
    except (OverflowError, ValueError):
        # Safe defaults if calculations fail
        team1_plus_implied = 0.75
        team2_plus_implied = 0.75
        team1_minus_implied = 0.25
        team2_minus_implied = 0.25
        over_implied = 0.5
        under_implied = 0.5
    
    # Apply derivative vig with better normalization
    derivative_probs = [
        team1_plus_implied, team2_plus_implied,
        team1_minus_implied, team2_minus_implied,
        over_implied, under_implied
    ]
    
    # Normalize derivative probabilities
    avg_derivative_prob = np.mean(derivative_probs)
    normalization_factor = 1 + (derivative_vig / len(derivative_probs))
    
    def safe_odds_conversion(prob, min_odds=1.12, max_odds=12.0):
        """Safely convert probability to odds with comprehensive error handling"""
        try:
            prob = max(0.083, min(0.92, prob))
            odds = 1 / prob
            return max(min_odds, min(max_odds, odds))
        except (ZeroDivisionError, OverflowError, ValueError):
            return 2.0  # Safe default
    
    # Build odds dictionary with comprehensive error handling
    try:
        odds_dict = {
            'team1_ml_odds': round(max(1.15, team1_odds), 2),
            'team2_ml_odds': round(max(1.15, team2_odds), 2),
            'team1_plus_1_5_odds': round(safe_odds_conversion(team1_plus_implied / normalization_factor), 2),
            'team2_plus_1_5_odds': round(safe_odds_conversion(team2_plus_implied / normalization_factor), 2),
            'team1_minus_1_5_odds': round(safe_odds_conversion(team1_minus_implied / normalization_factor), 2),
            'team2_minus_1_5_odds': round(safe_odds_conversion(team2_minus_implied / normalization_factor), 2),
            'over_2_5_maps_odds': round(safe_odds_conversion(over_implied / normalization_factor), 2),
            'under_2_5_maps_odds': round(safe_odds_conversion(under_implied / normalization_factor), 2)
        }
        
        # Final validation to ensure all odds are reasonable
        for key, odds_value in odds_dict.items():
            if not isinstance(odds_value, (int, float)) or odds_value < 1.12 or odds_value > 12.0 or np.isnan(odds_value):
                if 'plus' in key:
                    odds_dict[key] = 1.30
                elif 'minus' in key:
                    odds_dict[key] = 3.50
                elif 'over' in key or 'under' in key:
                    odds_dict[key] = 2.00
                else:
                    odds_dict[key] = 2.00
        
        return odds_dict
        
    except Exception as e:
        print(f"Error in odds simulation, using safe defaults: {e}")
        return {
            'team1_ml_odds': 2.00,
            'team2_ml_odds': 2.00,
            'team1_plus_1_5_odds': 1.30,
            'team2_plus_1_5_odds': 1.30,
            'team1_minus_1_5_odds': 3.50,
            'team2_minus_1_5_odds': 3.50,
            'over_2_5_maps_odds': 2.00,
            'under_2_5_maps_odds': 2.00
        }

def calculate_derivative_probabilities(team1_ml_prob, correlation_factor=0.2):
    """
    Calculate derivative market probabilities with proper correlation modeling.
    
    Args:
        team1_ml_prob: Moneyline probability for team1
        correlation_factor: How correlated are the maps (0.1-0.4 typical)
    
    Returns:
        dict: Dictionary of probabilities for all bet types
    """
    team2_ml_prob = 1 - team1_ml_prob
    
    # Single map win probabilities (slightly regressed toward 50%)
    team1_map_prob = 0.5 + (team1_ml_prob - 0.5) * 0.85
    team2_map_prob = 1 - team1_map_prob
    
    # Plus 1.5 maps (win at least 1 map in BO3)
    # P(win at least 1) = 1 - P(lose both)
    team1_plus_prob = 1 - (team2_map_prob ** (2.0 - correlation_factor))
    team2_plus_prob = 1 - (team1_map_prob ** (2.0 - correlation_factor))
    
    # Minus 1.5 maps (win 2-0)
    # P(win both) = P(win map 1) * P(win map 2 | won map 1)
    team1_minus_prob = team1_map_prob ** (2.0 + correlation_factor * 0.5)
    team2_minus_prob = team2_map_prob ** (2.0 + correlation_factor * 0.5)
    
    # Over/Under 2.5 maps (match goes to map 3)
    # This happens when teams split first 2 maps
    over_prob = 2 * team1_map_prob * team2_map_prob * (1 + correlation_factor)
    over_prob = np.clip(over_prob, 0.3, 0.7)  # Reasonable bounds
    under_prob = 1 - over_prob
    
    return {
        'team1_ml': np.clip(team1_ml_prob, 0.15, 0.85),
        'team2_ml': np.clip(team2_ml_prob, 0.15, 0.85),
        'team1_plus_1_5': np.clip(team1_plus_prob, 0.2, 0.9),
        'team2_plus_1_5': np.clip(team2_plus_prob, 0.2, 0.9),
        'team1_minus_1_5': np.clip(team1_minus_prob, 0.05, 0.7),
        'team2_minus_1_5': np.clip(team2_minus_prob, 0.05, 0.7),  
        'over_2_5_maps': np.clip(over_prob, 0.3, 0.7),
        'under_2_5_maps': np.clip(under_prob, 0.3, 0.7)
    }

def calculate_single_map_prob_for_backtesting(match_win_prob):
    """Calculate single map win probability from match win probability."""
    # Simplified calculation - can be adjusted based on your model
    single_map_prob = max(0.3, min(0.7, np.sqrt(match_win_prob)))
    return single_map_prob

def kelly_criterion(win_prob, decimal_odds, fractional=0.15):
    """Calculate Kelly bet size with more conservative fractional adjustment."""
    # Kelly formula: f* = (bp - q) / b
    b = decimal_odds - 1  # Decimal odds to b format
    p = win_prob
    q = 1 - p
    
    kelly = (b * p - q) / b
    
    # Apply fractional Kelly (more conservative)
    kelly = kelly * fractional
    
    # Apply additional cap for safety
    kelly = min(kelly, 0.05)  # Never bet more than 5% regardless of Kelly formula
    
    # Ensure non-negative
    return max(0, kelly)
def analyze_specific_match(results, match_id):
    """
    Analyze a specific match from backtest results in detail.
    
    Args:
        results (dict): Backtest results
        match_id (str): ID of the match to analyze
        
    Returns:
        dict: Detailed analysis of the match
    """
    # Find match in predictions
    match_prediction = None
    for pred in results['predictions']:
        if pred.get('match_id') == match_id:
            match_prediction = pred
            break
    
    if not match_prediction:
        print(f"Match {match_id} not found in results")
        return None
    
    # Find bets placed on this match
    match_bets = None
    for bet_record in results['bets']:
        if bet_record.get('match_id') == match_id:
            match_bets = bet_record
            break
    
    # Create detailed analysis
    analysis = {
        'match_id': match_id,
        'teams': f"{match_prediction['team1']} vs {match_prediction['team2']}",
        'prediction': {
            'predicted_winner': match_prediction['predicted_winner'],
            'actual_winner': match_prediction['actual_winner'],
            'team1_prob': match_prediction['team1_prob'],
            'team2_prob': match_prediction['team2_prob'],
            'confidence': match_prediction['confidence'],
            'correct': match_prediction['correct'],
            'score': match_prediction['score'],
            'date': match_prediction.get('date', 'Unknown')
        },
        'bets': []
    }
    
    # Add betting details if available
    if match_bets:
        analysis['bets'] = match_bets['bets']
        
        # Calculate aggregate betting performance
        total_wagered = sum(bet['amount'] for bet in match_bets['bets'])
        total_returns = sum(bet['returns'] for bet in match_bets['bets'])
        winning_bets = sum(1 for bet in match_bets['bets'] if bet['won'])
        
        analysis['betting_summary'] = {
            'total_bets': len(match_bets['bets']),
            'winning_bets': winning_bets,
            'total_wagered': total_wagered,
            'total_returns': total_returns,
            'profit': total_returns - total_wagered,
            'roi': (total_returns - total_wagered) / total_wagered if total_wagered > 0 else 0
        }
    
    # Print analysis
    print(f"\n===== MATCH ANALYSIS: {analysis['teams']} =====")
    print(f"Date: {analysis['prediction']['date']}")
    print(f"Score: {analysis['prediction']['score']}")
    print(f"Prediction: {analysis['prediction']['predicted_winner']} to win")
    print(f"Probability: {analysis['prediction']['team1_prob']:.2%} vs {analysis['prediction']['team2_prob']:.2%}")
    print(f"Confidence: {analysis['prediction']['confidence']:.2f}")
    print(f"Result: {'CORRECT' if analysis['prediction']['correct'] else 'INCORRECT'}")
    
    if 'betting_summary' in analysis:
        print("\nBetting Summary:")
        print(f"Total Bets: {analysis['betting_summary']['total_bets']}")
        print(f"Winning Bets: {analysis['betting_summary']['winning_bets']}")
        print(f"Wagered: ${analysis['betting_summary']['total_wagered']:.2f}")
        print(f"Returns: ${analysis['betting_summary']['total_returns']:.2f}")
        print(f"Profit: ${analysis['betting_summary']['profit']:.2f}")
        print(f"ROI: {analysis['betting_summary']['roi']:.2%}")
        
        print("\nIndividual Bets:")
        for i, bet in enumerate(analysis['bets']):
            print(f"  {i+1}. {bet['bet_type'].replace('_', ' ').upper()}")
            print(f"     Amount: ${bet['amount']:.2f}")
            print(f"     Odds: {bet['odds']:.2f}")
            print(f"     Edge: {bet['edge']:.2%}")
            print(f"     Result: {'WON' if bet['won'] else 'LOST'}")
            print(f"     Returns: ${bet['returns']:.2f}")
            print(f"     Profit: ${bet['profit']:.2f}")
    
    return analysis

def identify_key_insights(results):
    """
    Identify key insights and recommendations from backtest results.
    
    Args:
        results (dict): Backtest results
        
    Returns:
        dict: Key insights and recommendations
    """
    insights = {
        'overall_performance': {},
        'bet_types': [],
        'teams': [],
        'confidence_insights': {},
        'edge_insights': {},
        'recommendations': []
    }
    
    # Extract overall performance metrics
    insights['overall_performance'] = {
        'accuracy': results['performance']['accuracy'],
        'roi': results['performance']['roi'],
        'profit': results['performance']['profit'],
        'win_rate': results['performance']['win_rate'],
        'total_bets': results['performance'].get('total_bets', 0),
        'total_predictions': len(results['predictions'])
    }
    
    # Analyze bet types
    if 'metrics' in results and 'bet_types' in results['metrics']:
        bet_types = []
        for bet_type, stats in results['metrics']['bet_types'].items():
            if stats['total'] >= 5:  # Only include bet types with sufficient sample size
                win_rate = stats['won'] / stats['total'] if stats['total'] > 0 else 0
                roi = (stats['returns'] - stats['wagered']) / stats['wagered'] if stats['wagered'] > 0 else 0
                
                bet_types.append({
                    'type': bet_type,
                    'total': stats['total'],
                    'win_rate': win_rate,
                    'roi': roi,
                    'profit': stats['returns'] - stats['wagered'],
                    'profitable': roi > 0,
                    'recommendation': 'strong_bet' if roi > 0.1 else ('potential_bet' if roi > 0 else 'avoid')
                })
        
        # Sort by ROI
        bet_types.sort(key=lambda x: x['roi'], reverse=True)
        insights['bet_types'] = bet_types
    
    # Analyze team performance
    if 'team_performance' in results:
        teams = []
        for team_name, stats in results['team_performance'].items():
            if stats.get('bets', 0) >= 5:  # Only include teams with sufficient betting history
                prediction_accuracy = stats.get('correct', 0) / stats.get('predictions', 1) if stats.get('predictions', 0) > 0 else 0
                win_rate = stats.get('wins', 0) / stats.get('bets', 1) if stats.get('bets', 0) > 0 else 0
                roi = (stats.get('returns', 0) - stats.get('wagered', 0)) / stats.get('wagered', 1) if stats.get('wagered', 0) > 0 else 0
                
                teams.append({
                    'name': team_name,
                    'predictions': stats.get('predictions', 0),
                    'prediction_accuracy': prediction_accuracy,
                    'bets': stats.get('bets', 0),
                    'win_rate': win_rate,
                    'roi': roi,
                    'profit': stats.get('returns', 0) - stats.get('wagered', 0),
                    'profitable': roi > 0,
                    'recommendation': 'focus' if roi > 0.1 else ('potential' if roi > 0 else 'avoid')
                })
        
        # Sort by ROI
        teams.sort(key=lambda x: x['roi'], reverse=True)
        insights['teams'] = teams
    
    # Analyze confidence levels
    if 'metrics' in results and 'confidence_bins' in results['metrics']:
        confidence_data = results['metrics']['confidence_bins']
        
        # Calculate accuracy by confidence level
        confidence_by_accuracy = {}
        for conf_key, stats in confidence_data.items():
            if stats['total'] >= 5:  # Only include confidence levels with sufficient sample size
                confidence_by_accuracy[conf_key] = stats['correct'] / stats['total']
        
        # Find most reliable confidence threshold
        reliable_thresholds = []
        for conf_key, accuracy in confidence_by_accuracy.items():
            conf_level = float(conf_key.replace('%', '')) / 100
            if accuracy > 0.55:  # Only consider confidence levels with decent accuracy
                reliable_thresholds.append((conf_key, conf_level, accuracy))
        
        if reliable_thresholds:
            # Sort by balance of confidence and accuracy
            reliable_thresholds.sort(key=lambda x: x[1] * x[2], reverse=True)
            recommended_threshold = reliable_thresholds[0][0]
            recommended_accuracy = reliable_thresholds[0][2]
        else:
            recommended_threshold = "None"
            recommended_accuracy = 0
        
        insights['confidence_insights'] = {
            'accuracy_by_confidence': confidence_by_accuracy,
            'recommended_threshold': recommended_threshold,
            'expected_accuracy': recommended_accuracy
        }
    
    # Analyze edge levels
    if 'metrics' in results and 'roi_by_edge' in results['metrics']:
        edge_data = results['metrics']['roi_by_edge']
        
        # Calculate ROI by edge level
        roi_by_edge = {}
        for edge_key, stats in edge_data.items():
            if stats['wagered'] >= 100:  # Only include edge levels with sufficient volume
                roi_by_edge[edge_key] = (stats['returns'] - stats['wagered']) / stats['wagered']
        
        # Find most profitable edge threshold
        profitable_thresholds = []
        for edge_key, roi in roi_by_edge.items():
            if roi > 0:  # Only consider profitable edge levels
                edge_parts = edge_key.replace('%', '').split('-')
                min_edge = float(edge_parts[0]) / 100
                profitable_thresholds.append((edge_key, min_edge, roi))
        
        if profitable_thresholds:
            # Sort by ROI
            profitable_thresholds.sort(key=lambda x: x[2], reverse=True)
            recommended_threshold = profitable_thresholds[0][0]
            expected_roi = profitable_thresholds[0][2]
        else:
            recommended_threshold = "None"
            expected_roi = 0
        
        insights['edge_insights'] = {
            'roi_by_edge': roi_by_edge,
            'recommended_threshold': recommended_threshold,
            'expected_roi': expected_roi
        }
    
    # Generate recommendations
    recommendations = []
    
    # Recommendation 1: Overall strategy viability
    if insights['overall_performance']['roi'] > 0.1:
        recommendations.append("The overall betting strategy is highly profitable with an ROI of {:.1%}. Continue using the current approach.".format(
            insights['overall_performance']['roi']))
    elif insights['overall_performance']['roi'] > 0:
        recommendations.append("The overall betting strategy is marginally profitable with an ROI of {:.1%}. Consider focusing on the most profitable bet types.".format(
            insights['overall_performance']['roi']))
    else:
        recommendations.append("The overall betting strategy is not profitable with an ROI of {:.1%}. Major adjustments are needed.".format(
            insights['overall_performance']['roi']))
    
    # Recommendation 2: Best bet types
    profitable_bet_types = [bt for bt in insights['bet_types'] if bt['roi'] > 0.05 and bt['total'] >= 10]
    if profitable_bet_types:
        top_bet = profitable_bet_types[0]
        recommendations.append("Focus on '{}' bets, which showed {:.1%} ROI across {} bets.".format(
            top_bet['type'].replace('_', ' ').upper(), top_bet['roi'], top_bet['total']))
    
    # Recommendation 3: Teams to focus on or avoid
    profitable_teams = [team for team in insights['teams'] if team['roi'] > 0.1 and team['bets'] >= 5]
    unprofitable_teams = [team for team in insights['teams'] if team['roi'] < -0.1 and team['bets'] >= 5]
    
    if profitable_teams:
        top_team = profitable_teams[0]
        recommendations.append("Target matches involving {}, which showed {:.1%} ROI across {} bets.".format(
            top_team['name'], top_team['roi'], top_team['bets']))
    
    if unprofitable_teams:
        worst_team = unprofitable_teams[-1]
        recommendations.append("Avoid betting on matches involving {}, which showed {:.1%} ROI across {} bets.".format(
            worst_team['name'], worst_team['roi'], worst_team['bets']))
    
    # Recommendation 4: Confidence threshold
    if 'confidence_insights' in insights and insights['confidence_insights']['recommended_threshold'] != "None":
        recommendations.append("Only place bets when model confidence is at least {}. This threshold yielded {:.1%} prediction accuracy.".format(
            insights['confidence_insights']['recommended_threshold'],
            insights['confidence_insights']['expected_accuracy']))
    
    # Recommendation 5: Edge threshold
    if 'edge_insights' in insights and insights['edge_insights']['recommended_threshold'] != "None":
        recommendations.append("Only place bets with a predicted edge of at least {}. This threshold yielded {:.1%} ROI.".format(
            insights['edge_insights']['recommended_threshold'],
            insights['edge_insights']['expected_roi']))
    
    # Recommendation 6: Bankroll management
    if insights['overall_performance']['roi'] > 0:
        win_rate = insights['overall_performance']['win_rate']
        recommendations.append("Use Kelly criterion with a fraction of {:.1%} to optimize bankroll growth based on {:.1%} win rate.".format(
            min(0.05, win_rate / 4),  # Conservative Kelly fraction
            win_rate))
    
    insights['recommendations'] = recommendations
    
    # Print insights
    print("\n===== KEY INSIGHTS =====")
    
    print("\nOverall Performance:")
    print(f"Prediction Accuracy: {insights['overall_performance']['accuracy']:.2%}")
    print(f"Betting Win Rate: {insights['overall_performance']['win_rate']:.2%}")
    print(f"ROI: {insights['overall_performance']['roi']:.2%}")
    print(f"Total Profit: ${insights['overall_performance']['profit']:.2f}")
    
    print("\nTop Performing Bet Types:")
    for i, bet_type in enumerate(insights['bet_types'][:3]):
        if bet_type['profitable']:
            print(f"{i+1}. {bet_type['type'].replace('_', ' ').upper()}: {bet_type['roi']:.2%} ROI ({bet_type['total']} bets)")
    
    print("\nTop Performing Teams:")
    for i, team in enumerate(insights['teams'][:3]):
        if team['profitable']:
            print(f"{i+1}. {team['name']}: {team['roi']:.2%} ROI ({team['bets']} bets)")
    
    print("\nRecommendations:")
    for i, rec in enumerate(insights['recommendations']):
        print(f"{i+1}. {rec}")
    
    return insights

def get_backtest_params():
    """
    Interactive function to get parameters for backtesting.
    
    Returns:
        dict: Parameters for backtesting
    """
    print("\n===== BACKTEST CONFIGURATION =====")
    
    # Default parameters
    params = {
        'team_limit': 50,
        'bankroll': 1000.0,
        'bet_pct': 0.05,
        'min_edge': 0.08,
        'confidence_threshold': 0.4,
        'start_date': None,
        'end_date': None
    }
    
    # Interactive parameter entry
    print("\nEnter parameters (press Enter to use defaults):")
    
    try:
        # Team limit
        team_input = input(f"Number of teams to analyze [{params['team_limit']}]: ")
        if team_input.strip():
            params['team_limit'] = int(team_input)
        
        # Bankroll
        bankroll_input = input(f"Starting bankroll [${params['bankroll']}]: ")
        if bankroll_input.strip():
            params['bankroll'] = float(bankroll_input)
        
        # Bet percentage
        bet_pct_input = input(f"Maximum bet size as percentage of bankroll [{params['bet_pct']*100}%]: ")
        if bet_pct_input.strip():
            params['bet_pct'] = float(bet_pct_input) / 100
        
        # Minimum edge
        min_edge_input = input(f"Minimum required edge [{params['min_edge']*100}%]: ")
        if min_edge_input.strip():
            params['min_edge'] = float(min_edge_input) / 100
        
        # Confidence threshold
        conf_input = input(f"Minimum model confidence [{params['confidence_threshold']*100}%]: ")
        if conf_input.strip():
            params['confidence_threshold'] = float(conf_input) / 100
        
        # Date range
        date_range = input("Use specific date range? (y/n): ").lower().startswith('y')
        if date_range:
            start_date = input("Start date (YYYY-MM-DD or blank for no limit): ")
            end_date = input("End date (YYYY-MM-DD or blank for no limit): ")
            
            if start_date.strip():
                params['start_date'] = start_date.strip()
            if end_date.strip():
                params['end_date'] = end_date.strip()
    
    except ValueError as e:
        print(f"Invalid input: {e}")
        print("Using default values for all parameters.")
    
    # Print final configuration
    print("\nBacktest Configuration:")
    print(f"Teams to analyze: {params['team_limit']}")
    print(f"Starting bankroll: ${params['bankroll']}")
    print(f"Maximum bet size: {params['bet_pct']*100}% of bankroll")
    print(f"Minimum edge: {params['min_edge']*100}%")
    print(f"Minimum confidence: {params['confidence_threshold']*100}%")
    
    if params['start_date'] or params['end_date']:
        date_range = f"{params['start_date'] or 'Beginning'} to {params['end_date'] or 'Present'}"
        print(f"Date range: {date_range}")
    else:
        print("Date range: All available data")
    
    return params

def select_recommended_bets(betting_analysis, team1_name, team2_name):
    """
    Select recommended bets using the same criteria as backtesting.
    """
    # Get all recommended bets
    recommended_bets = {k: v for k, v in betting_analysis.items() if v['recommended']}
    
    if not recommended_bets:
        return {}
    
    # Use same prioritization as backtesting
    high_roi_bets = {k: v for k, v in recommended_bets.items() if v.get('high_roi_bet', False)}
    other_bets = {k: v for k, v in recommended_bets.items() if not v.get('high_roi_bet', False)}
    
    # Sort bets by edge
    sorted_high_roi_bets = sorted(high_roi_bets.items(), key=lambda x: x[1]['edge'], reverse=True)
    sorted_other_bets = sorted(other_bets.items(), key=lambda x: x[1]['edge'], reverse=True)
    
    # Combine lists with high ROI bets first
    sorted_bets = sorted_high_roi_bets + sorted_other_bets
    
    # Max bets per match - same as backtesting
    max_bets = 3
    
    # Select bets with diversification
    selected_bets = {}
    bet_teams = set()
    bet_categories = set()
    
    for bet_type, analysis in sorted_bets:
        # Stop if we've reached max bets
        if len(selected_bets) >= max_bets:
            break
        
        # Determine team and category
        if 'team1' in bet_type:
            team = team1_name
        elif 'team2' in bet_type:
            team = team2_name
        else:
            team = None
            
        category = '_'.join(bet_type.split('_')[1:])  # e.g., 'ml', 'plus_1_5'
        
        # Use same diversification logic as backtesting
        if team is None or category not in bet_categories or analysis.get('high_roi_bet', False):
            selected_bets[bet_type] = analysis
            bet_teams.add(team) if team else None
            bet_categories.add(category)
    
    return selected_bets


def analyze_betting_edge_for_backtesting(team1_win_prob, team2_win_prob, odds_data, confidence_score, bankroll=1000.0):
    """
    Significantly improved betting edge analysis with much stricter criteria.
    This is the core function that determines profitability.
    """
    print(f"\n=== ENHANCED BETTING ANALYSIS ===")
    print(f"Team1 Win Prob: {team1_win_prob:.4f}, Team2 Win Prob: {team2_win_prob:.4f}")
    print(f"Confidence Score: {confidence_score:.4f}")
    print(f"Bankroll: ${bankroll:.2f}")
    
    betting_analysis = {}
    
    # Much stricter thresholds for profitability
    MIN_EDGE_BASE = 0.045  # Increased from 0.002 to 0.045 (4.5%)
    MIN_CONFIDENCE = 0.60   # Increased from 0.01 to 0.65 (65%)
    MIN_PROBABILITY = 0.35  # Don't bet on extreme underdogs
    MAX_PROBABILITY = 0.75  # Don't bet on extreme favorites
    
    # Confidence-based edge scaling
    if confidence_score < MIN_CONFIDENCE:
        print(f"Confidence {confidence_score:.3f} below minimum {MIN_CONFIDENCE:.3f} - no bets")
        return {bet_type: create_no_bet_analysis(bet_type, odds_data.get(f'{bet_type}_odds', 2.0), 
                                                "Insufficient confidence") 
                for bet_type in ['team1_ml', 'team2_ml', 'team1_plus_1_5', 'team2_plus_1_5', 
                                'team1_minus_1_5', 'team2_minus_1_5', 'over_2_5_maps', 'under_2_5_maps']}
    
    # Dynamic edge threshold based on confidence
    edge_threshold = MIN_EDGE_BASE * (2.0 - confidence_score)  # Higher confidence = lower threshold
    edge_threshold = max(0.035, min(0.065, edge_threshold))  # Bound between 3.5% and 6.5%
    
    print(f"Dynamic edge threshold: {edge_threshold:.3f}")
    
    # Calculate derivative market probabilities with improved modeling
    single_map_prob = calculate_improved_single_map_prob(team1_win_prob, confidence_score)
    
    # More realistic probability calculations
    correlation_factor = 0.25 + (confidence_score * 0.15)  # Higher confidence = more correlation
    
    team1_plus_prob = calculate_plus_line_prob(single_map_prob, correlation_factor)
    team2_plus_prob = calculate_plus_line_prob(1 - single_map_prob, correlation_factor)
    
    team1_minus_prob = calculate_minus_line_prob(single_map_prob, correlation_factor)
    team2_minus_prob = calculate_minus_line_prob(1 - single_map_prob, correlation_factor)
    
    over_prob, under_prob = calculate_totals_prob(single_map_prob, correlation_factor)
    
    print(f"Calculated probabilities - Single map: {single_map_prob:.3f}")
    print(f"Plus lines: T1={team1_plus_prob:.3f}, T2={team2_plus_prob:.3f}")
    print(f"Minus lines: T1={team1_minus_prob:.3f}, T2={team2_minus_prob:.3f}")
    print(f"Totals: Over={over_prob:.3f}, Under={under_prob:.3f}")
    
    bet_types = [
        ('team1_ml', team1_win_prob, odds_data.get('team1_ml_odds', 0)),
        ('team2_ml', team2_win_prob, odds_data.get('team2_ml_odds', 0)),
        ('team1_plus_1_5', team1_plus_prob, odds_data.get('team1_plus_1_5_odds', 0)),
        ('team2_plus_1_5', team2_plus_prob, odds_data.get('team2_plus_1_5_odds', 0)),
        ('team1_minus_1_5', team1_minus_prob, odds_data.get('team1_minus_1_5_odds', 0)),
        ('team2_minus_1_5', team2_minus_prob, odds_data.get('team2_minus_1_5_odds', 0)),
        ('over_2_5_maps', over_prob, odds_data.get('over_2_5_maps_odds', 0)),
        ('under_2_5_maps', under_prob, odds_data.get('under_2_5_maps_odds', 0))
    ]
    
    # Ultra-conservative Kelly sizing
    MAX_KELLY_FRACTION = 0.015  # Never bet more than 1.5% of bankroll
    MAX_SINGLE_BET = min(bankroll * 0.02, 25.0)  # Cap at $25 or 2% of bankroll
    
    profitable_bets = 0
    
    for bet_type, prob, odds in bet_types:
        print(f"\n--- Analyzing {bet_type} ---")
        
        if odds <= 1.0:
            print(f"Invalid odds: {odds}")
            betting_analysis[bet_type] = create_no_bet_analysis(bet_type, odds, "Invalid odds")
            continue
            
        # Probability bounds check
        if prob < MIN_PROBABILITY or prob > MAX_PROBABILITY:
            print(f"Probability {prob:.3f} outside acceptable range [{MIN_PROBABILITY:.3f}, {MAX_PROBABILITY:.3f}]")
            betting_analysis[bet_type] = create_no_bet_analysis(bet_type, odds, "Probability out of range")
            continue
        
        implied_prob = 1 / odds
        raw_edge = prob - implied_prob
        
        # Apply confidence adjustment to our probability estimate
        confidence_adjusted_prob = apply_confidence_adjustment(prob, confidence_score)
        adjusted_edge = confidence_adjusted_prob - implied_prob
        
        print(f"Raw prob: {prob:.4f}, Confidence adjusted: {confidence_adjusted_prob:.4f}")
        print(f"Implied prob: {implied_prob:.4f}, Raw edge: {raw_edge:.4f}, Adjusted edge: {adjusted_edge:.4f}")
        
        # Strict edge requirement
        if adjusted_edge < edge_threshold:
            print(f"Edge {adjusted_edge:.4f} below threshold {edge_threshold:.4f}")
            betting_analysis[bet_type] = create_no_bet_analysis(bet_type, odds, f"Insufficient edge ({adjusted_edge:.3f})")
            continue
        
        # Ultra-conservative Kelly calculation
        b = odds - 1
        p = confidence_adjusted_prob
        q = 1 - p
        
        if b <= 0 or p <= 0 or q <= 0:
            kelly = 0
        else:
            full_kelly = (b * p - q) / b
            if full_kelly <= 0:
                kelly = 0
            else:
                # Use tiny fraction of Kelly with additional safety
                kelly = full_kelly * 0.08  # 8% of full Kelly
                kelly = min(kelly, MAX_KELLY_FRACTION)  # Cap at 1.5%
        
        bet_amount = bankroll * kelly
        bet_amount = min(bet_amount, MAX_SINGLE_BET)
        bet_amount = max(1.0, round(bet_amount, 0)) if kelly > 0 else 0
        
        print(f"Kelly fraction: {kelly:.6f}, Bet amount: ${bet_amount:.2f}")
        
        # Final safety checks
        meets_all_criteria = (
            adjusted_edge >= edge_threshold and
            confidence_score >= MIN_CONFIDENCE and
            bet_amount >= 1.0 and
            MIN_PROBABILITY <= prob <= MAX_PROBABILITY
        )
        
        if meets_all_criteria:
            profitable_bets += 1
            
        betting_analysis[bet_type] = {
            'probability': confidence_adjusted_prob,
            'implied_prob': implied_prob,
            'edge': adjusted_edge,
            'raw_edge': raw_edge,
            'edge_threshold': edge_threshold,
            'odds': odds,
            'kelly_fraction': kelly,
            'bet_amount': bet_amount,
            'recommended': meets_all_criteria,
            'confidence': confidence_score,
            'meets_edge': adjusted_edge >= edge_threshold,
            'meets_confidence': confidence_score >= MIN_CONFIDENCE,
            'meets_probability_bounds': MIN_PROBABILITY <= prob <= MAX_PROBABILITY,
            'rejection_reason': None if meets_all_criteria else get_rejection_reason(
                adjusted_edge, edge_threshold, confidence_score, MIN_CONFIDENCE, prob, MIN_PROBABILITY, MAX_PROBABILITY
            )
        }
        
        print(f"RECOMMENDED: {meets_all_criteria}")
    
    print(f"\n=== SUMMARY: {profitable_bets} bets recommended out of {len(bet_types)} analyzed ===")
    print(f"Total recommended risk: ${sum(analysis.get('bet_amount', 0) for analysis in betting_analysis.values() if analysis.get('recommended', False)):.2f}")
    
    return betting_analysis

def calculate_improved_single_map_prob(match_win_prob, confidence_score):
    """Calculate single map probability with improved modeling"""
    # Base conversion with confidence adjustment
    base_single = 0.5 + (match_win_prob - 0.5) * 0.75
    
    # Confidence-based adjustment - higher confidence means less regression to mean
    confidence_factor = 0.8 + (confidence_score * 0.2)
    adjusted_single = 0.5 + (base_single - 0.5) * confidence_factor
    
    return np.clip(adjusted_single, 0.25, 0.75)


def calculate_plus_line_prob(single_map_prob, correlation):
    """Calculate +1.5 maps probability"""
    # Probability of winning at least 1 map in best-of-3
    prob_lose_both = (1 - single_map_prob) ** (2.2 - correlation * 0.4)
    plus_prob = 1 - prob_lose_both
    return np.clip(plus_prob, 0.15, 0.95)

def calculate_minus_line_prob(single_map_prob, correlation):
    """Calculate -1.5 maps probability (winning 2-0)"""
    # Probability of winning both maps
    minus_prob = single_map_prob ** (2.4 + correlation * 0.3)
    return np.clip(minus_prob, 0.05, 0.75)

def calculate_totals_prob(single_map_prob, correlation):
    """Calculate over/under 2.5 maps probability"""
    # Probability match goes to 3 maps
    over_prob = 2 * single_map_prob * (1 - single_map_prob) * (1.1 + correlation * 0.2)
    over_prob = np.clip(over_prob, 0.25, 0.75)
    under_prob = 1 - over_prob
    return over_prob, under_prob

def apply_confidence_adjustment(prob, confidence_score):
    """Apply confidence-based adjustment to probability estimates"""
    if confidence_score < 0.5:
        # Low confidence - regress toward 50%
        return 0.5 + (prob - 0.5) * 0.6
    elif confidence_score < 0.7:
        # Medium confidence - slight regression
        return 0.5 + (prob - 0.5) * 0.85
    else:
        # High confidence - minimal regression
        return 0.5 + (prob - 0.5) * 0.95

def create_no_bet_analysis(bet_type, odds, reason):
    """Create analysis object for rejected bets"""
    return {
        'probability': 0.5,
        'implied_prob': 1 / max(1.01, odds),
        'edge': 0,
        'raw_edge': 0,
        'edge_threshold': 0.045,
        'odds': odds,
        'kelly_fraction': 0,
        'bet_amount': 0,
        'recommended': False,
        'confidence': 0,
        'meets_edge': False,
        'meets_confidence': False,
        'meets_probability_bounds': False,
        'rejection_reason': reason
    }

def get_rejection_reason(edge, edge_threshold, confidence, min_confidence, prob, min_prob, max_prob):
    """Get detailed reason for bet rejection"""
    reasons = []
    if edge < edge_threshold:
        reasons.append(f"Edge {edge:.3f} < {edge_threshold:.3f}")
    if confidence < min_confidence:
        reasons.append(f"Confidence {confidence:.3f} < {min_confidence:.3f}")
    if prob < min_prob or prob > max_prob:
        reasons.append(f"Probability {prob:.3f} outside [{min_prob:.3f}, {max_prob:.3f}]")
    return "; ".join(reasons)



# 3. Updated betting analysis with adjusted thresholds

def train_team_specific_models(team_data_collection):
    """
    Create specialized models for teams with sufficient data.
    
    Args:
        team_data_collection: Dictionary of team data
        
    Returns:
        dict: Dictionary of team-specific models
    """
    print("Training team-specific models...")
    
    team_models = {}
    team_predictors = {}
    
    # Find teams with sufficient data
    for team_name, team_data in team_data_collection.items():
        if len(team_data.get('matches', [])) >= 30:
            # Build team-specific dataset
            try:
                X, y = build_team_dataset(team_data, team_data_collection)
                
                if len(X) >= 25:  # Ensure enough samples for training
                    team_predictors[team_name] = (X, y)
                    print(f"Created dataset for {team_name} with {len(X)} samples")
            except Exception as e:
                print(f"Error building dataset for {team_name}: {e}")
    
    # Train models for teams with sufficient data
    for team_name, (X, y) in team_predictors.items():
        try:
            print(f"Training model for {team_name}...")
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # Train model
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Use random forest for team-specific models (more stable with less data)
            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=5,
                random_state=42
            )
            model.fit(X_train_scaled, y_train)
            
            # Evaluate
            y_pred = model.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            
            if accuracy > 0.55:  # Only keep models that perform well
                team_models[team_name] = {
                    'model': model,
                    'scaler': scaler,
                    'accuracy': accuracy,
                    'samples': len(X)
                }
                print(f"Added model for {team_name} with {accuracy:.4f} accuracy")
            else:
                print(f"Skipping model for {team_name} due to low accuracy ({accuracy:.4f})")
        except Exception as e:
            print(f"Error training model for {team_name}: {e}")
    
    print(f"Created {len(team_models)} team-specific models")
    return team_models

def build_team_dataset(team_data, team_data_collection):
    """
    Build a dataset for team-specific modeling.
    
    Args:
        team_data: Data for the specific team
        team_data_collection: Dictionary of all team data
        
    Returns:
        tuple: (features, labels)
    """
    X = []
    y = []
    
    team_name = team_data.get('team_name')
    team_matches = team_data.get('matches', [])
    
    for match in team_matches:
        opponent_name = match.get('opponent_name')
        
        # Skip if opponent not in data collection
        if not opponent_name or opponent_name not in team_data_collection:
            continue
            
        # Get opponent data
        opponent_data = team_data_collection[opponent_name]
        
        # Prepare match features
        features = prepare_data_for_model(team_data.get('stats', {}), opponent_data.get('stats', {}))
        
        if features:
            X.append(features)
            y.append(1 if match.get('team_won') else 0)
    
    return np.array(X) if X else np.array([]), np.array(y) if y else np.array([])

def bayesian_predict(ensemble_models, X, num_samples=500):
    """
    Use Bayesian approach for better uncertainty estimation.
    
    Args:
        ensemble_models: List of models for prediction
        X: Features to predict
        num_samples: Number of samples for Bayesian estimation
        
    Returns:
        tuple: (mean_prediction, std_dev, confidence_interval)
    """
    print("Performing Bayesian prediction...")
    
    # Ensure X is properly shaped
    if len(X.shape) == 1:
        X = X.reshape(1, -1)
    
    all_predictions = []
    
    # First pass: get initial predictions from all models
    model_predictions = {}
    
    for model_type, model, model_scaler in ensemble_models:
        try:
            # Apply scaling if needed
            X_pred = X.copy()
            if model_scaler is not None:
                X_pred = model_scaler.transform(X_pred)
            
            # Make prediction based on model type
            if model_type == 'nn':
                # For neural networks, sample with dropout enabled to approximate Bayesian posterior
                K.set_learning_phase(1)  # Enable dropout at inference time
                
                # Take multiple samples
                nn_samples = []
                for _ in range(20):  # Take 20 samples per NN
                    pred = model.predict(X_pred, verbose=0)[0][0]
                    nn_samples.append(pred)
                
                model_predictions[model_type] = nn_samples
                all_predictions.extend(nn_samples)
                
                # Reset learning phase
                K.set_learning_phase(0)
            else:
                # For other models, use their predictions directly
                if hasattr(model, 'predict_proba'):
                    pred = model.predict_proba(X_pred)[0][1]
                    model_predictions[model_type] = [pred]
                    all_predictions.append(pred)
        except Exception as e:
            print(f"Error with {model_type} model: {e}")
    
    # Second pass: generate posterior samples using bootstrap resampling
    posterior_samples = []
    for _ in range(num_samples):
        # Randomly select a model type with weighted probability
        weights = {
            'nn': 0.6,      # Neural networks get highest weight
            'gb': 0.15,     # Gradient boosting
            'rf': 0.15,     # Random forest
            'lr': 0.05,     # Logistic regression
            'svm': 0.05     # SVM
        }
        
        available_models = list(model_predictions.keys())
        if not available_models:
            break
            
        # Adjust weights based on available models
        total_weight = sum(weights[m] for m in available_models)
        adjusted_weights = [weights[m] / total_weight for m in available_models]
        
        # Sample a model
        selected_model = np.random.choice(available_models, p=adjusted_weights)
        
        # Sample a prediction from that model's predictions
        model_preds = model_predictions[selected_model]
        if model_preds:
            sampled_pred = np.random.choice(model_preds)
            posterior_samples.append(sampled_pred)
    
    # Calculate statistics
    if posterior_samples:
        mean_pred = np.mean(posterior_samples)
        std_pred = np.std(posterior_samples)
        
        # Calculate 95% confidence interval
        lower_bound = np.percentile(posterior_samples, 2.5)
        upper_bound = np.percentile(posterior_samples, 97.5)
        
        # Calculate support ratio (between 0 and 1)
        above_half = sum(1 for p in posterior_samples if p > 0.5) / len(posterior_samples)
        support_ratio = max(above_half, 1 - above_half)  # Always 0.5 to 1.0
        
        # Calculate confidence based on uncertainty and consensus
        confidence = support_ratio * (1 - min(1, std_pred * 3))
        
        return mean_pred, std_pred, (lower_bound, upper_bound), confidence
    else:
        return 0.5, 0.25, (0.25, 0.75), 0.0

def implement_cross_validation_safeguards(X, y):
    """
    Implement nested cross-validation for more realistic accuracy estimation
    and to prevent overfitting.
    
    Args:
        X: Features array
        y: Labels array
        
    Returns:
        tuple: (estimated_accuracy, selected_features, best_params)
    """
    print("Implementing nested cross-validation safeguards...")
    
    # Outer cross-validation for final performance estimation
    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    
    outer_scores = []
    selected_features_counts = {}
    
    # For each train-test split in the outer CV
    for i, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):
        print(f"Outer fold {i+1}/5...")
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        # Further split training data for feature selection
        X_train_fs, X_val_fs, y_train_fs, y_val_fs = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        # Perform feature selection on a subset of training data
        selector = SelectFromModel(
            RandomForestClassifier(n_estimators=100, random_state=42),
            threshold='median'
        )
        selector.fit(X_train_fs, y_train_fs)
        
        # Get selected feature indices
        selected_indices = selector.get_support()
        
        # Count selected features for stability analysis
        for i, selected in enumerate(selected_indices):
            if selected:
                if i not in selected_features_counts:
                    selected_features_counts[i] = 0
                selected_features_counts[i] += 1
        
        # Use selected features
        X_train_selected = X_train[:, selected_indices]
        X_test_selected = X_test[:, selected_indices]
        
        # Now do parameter tuning with inner CV
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7, None],
            'min_samples_split': [2, 5, 10]
        }
        
        clf = GridSearchCV(
            RandomForestClassifier(random_state=42),
            param_grid,
            cv=inner_cv,
            scoring='accuracy',
            n_jobs=-1
        )
        clf.fit(X_train_selected, y_train)
        
        # Get best params
        best_params = clf.best_params_
        
        # Train final model with best params
        best_clf = RandomForestClassifier(random_state=42, **best_params)
        best_clf.fit(X_train_selected, y_train)
        
        # Test on held-out data
        y_pred = best_clf.predict(X_test_selected)
        acc = accuracy_score(y_test, y_pred)
        
        print(f"  Fold accuracy: {acc:.4f} with {sum(selected_indices)} features")
        outer_scores.append(acc)
    
    # Calculate final performance estimate
    mean_accuracy = np.mean(outer_scores)
    std_accuracy = np.std(outer_scores)
    
    # Use more conservative estimate (lower bound of confidence interval)
    estimated_accuracy = mean_accuracy - 1.96 * std_accuracy / np.sqrt(len(outer_scores))
    
    # Select stable features (selected in at least 4 out of 5 folds)
    stable_features = [idx for idx, count in selected_features_counts.items() if count >= 4]
    
    print(f"Nested CV results: {mean_accuracy:.4f}  {std_accuracy:.4f}")
    print(f"Conservative accuracy estimate: {estimated_accuracy:.4f}")
    print(f"Selected {len(stable_features)} stable features")
    
    return estimated_accuracy, stable_features, best_params

def detect_line_value(odds, predicted_prob):
    """
    Detect if line has more value now vs expected movement.
    
    Args:
        odds: Current decimal odds
        predicted_prob: Our predicted probability
        
    Returns:
        tuple: (recommendation, expected_value)
    """
    implied_prob = 1 / odds
    edge = predicted_prob - implied_prob
    
    # Calculate expected value as a percentage
    ev_percentage = edge * 100
    
    # Determine recommendation based on edge size
    if edge > 0.1:  # Very strong edge
        return "STRONG BET - exceptional value", ev_percentage
    elif edge > 0.07:
        return "BET NOW - strong value that may disappear", ev_percentage
    elif edge > 0.04:
        return "BET - substantial value", ev_percentage
    elif edge > 0.02:
        return "CONSIDER - modest value", ev_percentage
    elif edge > 0:
        return "MONITOR - slight value, watch for line movement", ev_percentage
    else:
        return "PASS - no value detected", ev_percentage

def track_odds_movement(match_id, team1, team2, current_odds, predicted_prob):
    """
    Track odds movement to identify betting patterns and optimal timing.
    
    Args:
        match_id: Unique identifier for the match
        team1: First team name
        team2: Second team name
        current_odds: Current odds from bookmaker
        predicted_prob: Our predicted probability
        
    Returns:
        dict: Analysis of odds movement
    """
    # Create directory if not exists
    os.makedirs("odds_tracking", exist_ok=True)
    
    odds_history_file = f"odds_tracking/{match_id}_odds.json"
    
    # Load existing data or create new
    try:
        with open(odds_history_file, 'r') as f:
            odds_history = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        odds_history = {
            'match_id': match_id,
            'team1': team1,
            'team2': team2,
            'predicted_prob': predicted_prob,
            'timestamps': []
        }
    
    # Add current timestamp and odds
    odds_history['timestamps'].append({
        'time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'odds': current_odds
    })
    
    # Save updated history
    with open(odds_history_file, 'w') as f:
        json.dump(odds_history, f, indent=2)
    
    # Analyze odds movement if we have multiple measurements
    if len(odds_history['timestamps']) > 1:
        analysis = analyze_odds_movement(odds_history)
        return analysis
    
    return {"message": "First odds recording, no movement to analyze yet"}

def analyze_odds_movement(odds_history):
    """
    Analyze recorded odds movement to detect patterns and optimal betting timing.
    
    Args:
        odds_history: Dictionary containing odds history
        
    Returns:
        dict: Analysis results
    """
    timestamps = odds_history['timestamps']
    
    # Sort by time
    timestamps.sort(key=lambda x: x['time'])
    
    # Calculate movement for each market
    markets = set()
    for ts in timestamps:
        for market in ts['odds'].keys():
            markets.add(market)
    
    analysis = {
        'match_id': odds_history['match_id'],
        'team1': odds_history['team1'],
        'team2': odds_history['team2'],
        'markets': {},
        'recommendation': {}
    }
    
    # Analyze each market separately
    for market in markets:
        market_data = []
        
        for ts in timestamps:
            if market in ts['odds']:
                market_data.append({
                    'time': ts['time'],
                    'odds': ts['odds'][market]
                })
        
        if len(market_data) > 1:
            # Calculate movement
            first_odds = market_data[0]['odds']
            last_odds = market_data[-1]['odds']
            odds_change = last_odds - first_odds
            
            # Calculate movement direction and rate
            direction = "increasing" if odds_change > 0 else "decreasing"
            hours_elapsed = (datetime.strptime(market_data[-1]['time'], '%Y-%m-%d %H:%M:%S') - 
                         datetime.strptime(market_data[0]['time'], '%Y-%m-%d %H:%M:%S')).total_seconds() / 3600
            
            rate = odds_change / hours_elapsed if hours_elapsed > 0 else 0
            
            # Calculate implied probability change
            first_implied = 1 / first_odds
            last_implied = 1 / last_odds
            prob_change = last_implied - first_implied
            
            # Determine if line is moving in our favor
            if market.startswith('team1') and odds_history['predicted_prob'] > 0.5:
                # For team1 bets when we predict team1 to win
                favorable = direction == "increasing"
            elif market.startswith('team2') and odds_history['predicted_prob'] < 0.5:
                # For team2 bets when we predict team2 to win
                favorable = direction == "increasing"
            else:
                # For other situations
                favorable = False
            
            # Store analysis
            analysis['markets'][market] = {
                'first_odds': first_odds,
                'last_odds': last_odds,
                'change': odds_change,
                'direction': direction,
                'rate': rate,
                'probability_change': prob_change,
                'favorable': favorable
            }
            
            # Make recommendation
            if favorable and abs(rate) > 0.05:  # Significant favorable movement
                analysis['recommendation'][market] = "BET NOW - line moving in favorable direction"
            elif not favorable and abs(rate) > 0.05:  # Significant unfavorable movement
                analysis['recommendation'][market] = "WAIT - line moving against us"
            else:  # Minimal movement
                analysis['recommendation'][market] = "NEUTRAL - stable line"
    
    return analysis

def adjust_for_streak(bet_history, base_kelly, bet_type, team_name):
    """
    Adjust Kelly fraction based on current winning/losing streak.
    
    Args:
        bet_history: List of past bets
        base_kelly: Base Kelly fraction
        bet_type: Type of bet (e.g., team1_ml)
        team_name: Name of the team the bet is on
        
    Returns:
        float: Adjusted Kelly fraction
    """
    # Find recent bets of the same type involving this team
    recent_bets = []
    
    if not bet_history:
        return base_kelly
    
    for bet in reversed(bet_history):
        # Check if bet involves the same team
        if team_name in [bet.get('team1', ''), bet.get('team2', '')]:
            # Find matching bet type in bet details
            for bet_detail in bet.get('bets', []):
                if bet_detail.get('bet_type') == bet_type:
                    recent_bets.append(bet_detail.get('won', False))
                    if len(recent_bets) >= 5:  # Only look at 5 most recent
                        break
            
            if len(recent_bets) >= 5:
                break
    
    # Calculate current streak
    current_streak = 0
    for won in recent_bets:
        if won:
            if current_streak >= 0:
                current_streak += 1
            else:
                # Break losing streak
                current_streak = 1
        else:
            if current_streak <= 0:
                current_streak -= 1
            else:
                # Break winning streak
                current_streak = -1
    
    # Adjust Kelly fraction based on streak
    if current_streak >= 3:  # On a winning streak
        # Increase bet size to capitalize on good form
        adjustment = 1.2  # Increase by 20%
        print(f"On {current_streak}-bet winning streak: increasing bet size by 20%")
    elif current_streak <= -3:  # On a losing streak
        # Decrease bet size to preserve bankroll
        adjustment = 0.8  # Decrease by 20%
        print(f"On {abs(current_streak)}-bet losing streak: reducing bet size by 20%")
    elif current_streak >= 1:  # Mild winning momentum
        adjustment = 1.1  # Small increase
        print(f"Mild winning momentum: increasing bet size by 10%")
    elif current_streak <= -1:  # Mild losing momentum
        adjustment = 0.9  # Small decrease
        print(f"Mild losing momentum: reducing bet size by 10%")
    else:
        adjustment = 1.0  # No adjustment
    
    return base_kelly * adjustment

def calculate_dynamic_kelly(win_prob, decimal_odds, confidence, past_precision=None):
    """
    Calculate Kelly Criterion with dynamic adjustments based on historical precision.
    
    Args:
        win_prob: Predicted probability of winning
        decimal_odds: Decimal odds offered by bookmaker
        confidence: Confidence score in this prediction
        past_precision: Historical prediction precision (optional)
        
    Returns:
        float: Kelly stake as fraction of bankroll
    """
    # Standard Kelly calculation
    b = decimal_odds - 1  # Convert to b notation
    p = win_prob
    q = 1 - p
    
    # Avoid division by zero
    if b <= 0:
        return 0
    
    kelly = (b * p - q) / b
    
    # Apply confidence adjustment
    confidence_factor = 0.5 + (confidence * 0.5)  # Scale from 0.5 to 1.0
    
    # Apply historical precision adjustment if available
    if past_precision is not None:
        precision_factor = min(1.2, max(0.8, past_precision))  # Bound between 0.8 and 1.2
    else:
        precision_factor = 1.0
    
    # Apply fractional Kelly (conservative approach)
    fractional = 0.3  # 30% of full Kelly
    
    # Combine all adjustments
    adjusted_kelly = kelly * fractional * confidence_factor * precision_factor
    
    # Apply absolute cap for safety
    return min(adjusted_kelly, 0.07)  # Never bet more than 7% of bankroll

def check_profit_targets(starting_bankroll, current_bankroll, target_percentage=1000000):
    """
    Check if profit targets have been reached.
    
    Args:
        starting_bankroll: Starting bankroll
        current_bankroll: Current bankroll
        target_percentage: Target profit percentage
        
    Returns:
        tuple: (reached, message)
    """
    profit = current_bankroll - starting_bankroll
    profit_percentage = (profit / starting_bankroll) * 100
    
    if profit_percentage >= target_percentage:
        return True, f"Success! Profit target of {target_percentage}% reached. Current profit: ${profit:.2f} ({profit_percentage:.1f}%)"
    
    # If we're close to the target, give an update
    if profit_percentage >= target_percentage * 0.8:
        return False, f"Approaching profit target. Current profit: ${profit:.2f} ({profit_percentage:.1f}% of {target_percentage}% target)"
    
    return False, ""

def evaluate_bet_outcome(bet_type, actual_winner, team1_score, team2_score):
    """Determine if a bet won based on actual results."""
    # Moneyline bets
    if bet_type == 'team1_ml':
        return team1_score > team2_score
    elif bet_type == 'team2_ml':
        return team2_score > team1_score
    
    # Map spread bets
    elif bet_type == 'team1_plus_1_5':
        return team1_score + 1.5 > team2_score
    elif bet_type == 'team2_plus_1_5':
        return team2_score + 1.5 > team1_score
    elif bet_type == 'team1_minus_1_5':
        return team1_score - 1.5 > team2_score
    elif bet_type == 'team2_minus_1_5':
        return team2_score - 1.5 > team1_score
    
    # Map total bets
    elif bet_type == 'over_2_5_maps':
        return team1_score + team2_score > 2.5
    elif bet_type == 'under_2_5_maps':
        return team1_score + team2_score < 2.5
    
    return False

def extract_match_score(match_data):
    """Extract the actual map score from match data."""
    if 'map_score' in match_data:
        # Parse map score in format like "2:0"
        try:
            score_parts = match_data['map_score'].split(':')
            team1_score = int(score_parts[0])
            team2_score = int(score_parts[1])
            return team1_score, team2_score
        except (ValueError, IndexError):
            pass
    
    # Fallback to using team_score and opponent_score
    team1_score = match_data.get('team_score', 0)
    team2_score = match_data.get('opponent_score', 0)
    return team1_score, team2_score

def calculate_map_advantage(team1_stats, team2_stats, map_name):
    """Calculate map-specific advantage based on team strengths."""
    if 'map_statistics' not in team1_stats or 'map_statistics' not in team2_stats:
        return 0
        
    if map_name not in team1_stats['map_statistics'] or map_name not in team2_stats['map_statistics']:
        return 0
        
    team1_map = team1_stats['map_statistics'][map_name]
    team2_map = team2_stats['map_statistics'][map_name]
    
    # Extract win rates with fallbacks
    team1_win_rate = team1_map.get('win_percentage', 0.5)
    team2_win_rate = team2_map.get('win_percentage', 0.5)
    
    # Factor in matches played for statistical significance
    team1_matches = team1_map.get('matches_played', 1)
    team2_matches = team2_map.get('matches_played', 1)
    
    # Weight advantage by matches played (more matches = more reliable data)
    t1_weight = min(1.0, team1_matches / 10)  # Cap at 10 matches
    t2_weight = min(1.0, team2_matches / 10)
    
    # Calculate weighted advantage
    raw_advantage = team1_win_rate - team2_win_rate
    weighted_advantage = raw_advantage * (t1_weight + t2_weight) / 2
    
    # Add side preference advantage
    if 'side_preference' in team1_map and 'side_preference' in team2_map:
        # If teams prefer opposite sides, no advantage
        if team1_map['side_preference'] != team2_map['side_preference']:
            pass
        # If both prefer the same side, the team with stronger preference has advantage
        else:
            t1_strength = team1_map.get('side_preference_strength', 0)
            t2_strength = team2_map.get('side_preference_strength', 0)
            if t1_strength > t2_strength:
                weighted_advantage += 0.02  # Small bonus
            elif t2_strength > t1_strength:
                weighted_advantage -= 0.02
    
    return weighted_advantage

def calculate_stylistic_matchup_advantage(team1_stats, team2_stats):
    """Calculate advantage based on playing style compatibility."""
    advantage = 0
    
    # 1. Economy efficiency vs. poor pistol rounds
    if ('economy_efficiency' in team1_stats and 'pistol_win_rate' in team2_stats and
        team1_stats.get('economy_efficiency', 0.5) > 0.6 and team2_stats.get('pistol_win_rate', 0.5) < 0.4):
        advantage += 0.05  # Team1 has advantage against teams with poor pistol rounds
    
    # 2. Strong pistol rounds vs poor economy efficiency
    if ('pistol_win_rate' in team1_stats and 'economy_efficiency' in team2_stats and
        team1_stats.get('pistol_win_rate', 0.5) > 0.6 and team2_stats.get('economy_efficiency', 0.5) < 0.4):
        advantage += 0.05
    
    # 3. Early fragging capability vs slow starters
    if 'avg_player_acs' in team1_stats and 'avg_player_adr' in team2_stats:
        t1_early_impact = team1_stats.get('avg_player_acs', 0) / 100
        t2_slow_start = team2_stats.get('avg_player_adr', 0) / 100
        if t1_early_impact > 2.5 and t2_slow_start < 1.5:
            advantage += 0.03
    
    # 4. Team consistency advantage
    if 'team_consistency' in team1_stats and 'team_consistency' in team2_stats:
        if team1_stats['team_consistency'] > team2_stats['team_consistency'] + 0.2:
            advantage += 0.04  # More consistent team has advantage
    
    # 5. Check for strong FK/FD advantage
    if 'fk_fd_ratio' in team1_stats and 'fk_fd_ratio' in team2_stats:
        t1_ratio = team1_stats['fk_fd_ratio']
        t2_ratio = team2_stats['fk_fd_ratio']
        if t1_ratio > t2_ratio * 1.5:  # Team1 gets 50% more first kills
            advantage += 0.05
    
    return advantage

def calculate_win_rate_stability(team_matches, window_size=5):
    """Calculate the stability of a team's win rate over time."""
    if not isinstance(team_matches, list) or len(team_matches) < window_size * 2:
        return 0.5  # Default stability for teams with little data
    
    # Sort matches by date
    sorted_matches = sorted(team_matches, key=lambda x: x.get('date', ''))
    
    # Calculate win rates in consecutive windows
    win_rates = []
    for i in range(0, len(sorted_matches), window_size):
        if i + window_size <= len(sorted_matches):
            window = sorted_matches[i:i+window_size]
            wins = sum(1 for m in window if m.get('team_won', False))
            win_rates.append(wins / window_size)
    
    # Calculate stability as 1 - standard deviation (higher = more stable)
    if len(win_rates) >= 2:
        stability = 1 - min(0.5, np.std(win_rates))
        return stability
    
    return 0.5  # Default value    

def get_teams_for_backtesting(limit=100, use_cache=True, cache_path="cache/valorant_data_cache.pkl"):
    """
    Get a list of teams for backtesting, optionally using cached data.
    
    Args:
        limit (int): Maximum number of teams to return
        use_cache (bool): Whether to use cached data if available
        cache_path (str): Path to the cache file
        
    Returns:
        list: Teams for backtesting
    """
    # Try to use cached data if available
    if use_cache:
        print(f"Checking for cached team data for backtesting...")
        if os.path.exists(cache_path):
            team_data = load_cache(cache_path)
            if team_data:
                # Convert cached team data to the format expected by backtesting
                teams_list = []
                for team_name, team_info in team_data.items():
                    team_dict = {
                        'id': team_info.get('team_id', ''),
                        'name': team_name,
                        'ranking': team_info.get('ranking')
                    }
                    teams_list.append(team_dict)
                
                # Sort by ranking if available
                teams_list = sorted(teams_list, 
                                   key=lambda x: x['ranking'] if x['ranking'] else float('inf'))
                
                # Limit to requested number
                teams_list = teams_list[:limit]
                print(f"Using {len(teams_list)} teams from cache for backtesting")
                return teams_list
    
    # Fallback to API if cache not available or not using cache
    print(f"Fetching teams from API: {API_URL}/teams?limit={limit}")
    try:
        teams_response = requests.get(f"{API_URL}/teams?limit={limit}")
        if teams_response.status_code != 200:
            print(f"Error fetching teams: {teams_response.status_code}")
            return []
        teams_data = teams_response.json()
        if 'data' not in teams_data:
            print("No 'data' field found in the response")
            return []
        teams_list = []
        for team in teams_data['data']:
            if 'ranking' in team and team['ranking'] and team['ranking'] <= 100:
                teams_list.append(team)
        if not teams_list:
            print(f"No teams with rankings found. Using the first {min(100, limit)} teams instead.")
            if len(teams_data['data']) > 0:
                teams_list = teams_data['data'][:min(100, limit)]
        print(f"Selected {len(teams_list)} teams for backtesting from API")
        return teams_list
    except Exception as e:
        print(f"Error in get_teams_for_backtesting: {e}")
        return []

# 1. Fix Neural Network Calibration in predict_with_ensemble function
def predict_with_ensemble(ensemble_models, X):
    """
    Enhanced ensemble prediction with better calibration for betting applications.
    This function is critical for accurate probability estimates.
    """
    if not ensemble_models:
        raise ValueError("No models provided for prediction")
    
    if len(X.shape) == 1:
        X = X.reshape(1, -1)
    
    # Collect predictions by model type for better weighting
    predictions_by_type = {'nn': [], 'gb': [], 'rf': [], 'lr': [], 'svm': []}
    model_confidences = {}
    
    print(f"Making predictions with {len(ensemble_models)} models")
    
    for i, (model_type, model, model_scaler) in enumerate(ensemble_models):
        try:
            X_pred = X.copy()
            
            # Apply scaling if needed
            if model_scaler is not None:
                try:
                    X_pred = model_scaler.transform(X_pred)
                except Exception as e:
                    print(f"Scaling failed for {model_type}: {e}")
                    continue
            
            # Get prediction based on model type
            if model_type == 'nn':
                # For neural networks, get multiple samples to estimate uncertainty
                pred_samples = []
                for _ in range(5):  # Multiple forward passes
                    pred = model.predict(X_pred, verbose=0)[0][0]
                    pred_samples.append(pred)
                
                pred = np.mean(pred_samples)
                pred_std = np.std(pred_samples)
                
                # Neural network confidence based on prediction uncertainty
                confidence = 1.0 - min(0.5, pred_std * 4)
                
            else:
                # For tree-based and linear models
                if hasattr(model, 'predict_proba'):
                    pred_proba = model.predict_proba(X_pred)[0]
                    pred = pred_proba[1]  # Probability of class 1
                    
                    # Confidence based on prediction certainty
                    confidence = max(pred_proba) * 2 - 1  # Convert to 0-1 scale
                    confidence = max(0, min(1, confidence))
                    
                else:
                    pred = float(model.predict(X_pred)[0])
                    confidence = 0.7  # Default confidence for models without probabilities
            
            # Bound predictions to reasonable range
            pred = np.clip(pred, 0.15, 0.85)
            
            # Check for invalid predictions
            if np.isnan(pred) or not np.isfinite(pred):
                print(f"Invalid prediction from {model_type}: {pred}")
                continue
            
            predictions_by_type[model_type].append(pred)
            model_confidences[f"{model_type}_{i}"] = confidence
            
            print(f"{model_type.upper()} prediction: {pred:.4f} (confidence: {confidence:.3f})")
            
        except Exception as e:
            print(f"Error with {model_type} model: {e}")
            continue
    
    # Calculate ensemble prediction with improved weighting
    ensemble_predictions = []
    
    # Enhanced model type weights based on betting performance research
    type_weights = {
        'nn': 0.35,      # Neural networks - good for complex patterns
        'gb': 0.30,      # Gradient boosting - excellent for tabular data
        'rf': 0.20,      # Random forest - stable and interpretable
        'lr': 0.10,      # Logistic regression - good baseline
        'svm': 0.05      # SVM - complementary perspective
    }
    
    weighted_predictions = []
    total_weight = 0
    
    for model_type, preds in predictions_by_type.items():
        if preds:
            type_pred = np.mean(preds)
            type_std = np.std(preds) if len(preds) > 1 else 0.05
            
            # Confidence-adjusted weight (more consistent predictions get higher weight)
            consistency_factor = 1.0 - min(0.4, type_std * 3)
            adjusted_weight = type_weights[model_type] * consistency_factor
            
            weighted_predictions.append((type_pred, adjusted_weight))
            total_weight += adjusted_weight
            
            print(f"{model_type.upper()} weighted: {type_pred:.4f} (weight: {adjusted_weight:.3f})")
    
    if not weighted_predictions:
        print("ERROR: No valid predictions from any model")
        return 0.5, ['0.5000'], 0.1
    
    # Calculate weighted ensemble prediction
    final_pred = sum(pred * weight for pred, weight in weighted_predictions) / total_weight
    
    # Calculate model agreement for confidence
    all_preds = [pred for pred, _ in weighted_predictions]
    pred_std = np.std(all_preds)
    model_agreement = 1.0 - min(0.6, pred_std * 3)  # Higher agreement = higher confidence
    
    # Enhanced confidence calculation
    confidence_factors = list(model_confidences.values())
    base_confidence = np.mean(confidence_factors) if confidence_factors else 0.5
    
    # Combine model agreement with base confidence
    final_confidence = (base_confidence * 0.6) + (model_agreement * 0.4)
    
    # Apply betting-specific calibration
    # Regress extreme predictions toward center (more conservative for betting)
    calibrated_pred = apply_betting_calibration(final_pred, final_confidence)
    
    # Ensure final bounds
    calibrated_pred = np.clip(calibrated_pred, 0.2, 0.8)
    final_confidence = np.clip(final_confidence, 0.1, 0.95)
    
    # Create raw predictions string for debugging
    raw_predictions_str = [f'{pred:.4f}' for pred, _ in weighted_predictions[:5]]
    
    print(f"Final ensemble: {calibrated_pred:.4f} (confidence: {final_confidence:.3f})")
    print(f"Model agreement: {model_agreement:.3f}, Calibration applied: {abs(final_pred - calibrated_pred):.4f}")
    
    return calibrated_pred, raw_predictions_str, final_confidence


def calculate_bet_type_probabilities(win_probability, confidence_score):
    """
    Calculate probabilities for different bet types using the EXACT same
    approach as backtesting - MODIFIED TO MATCH BACKTESTING
    """
    # Use EXACT same logic as analyze_betting_edge_for_backtesting
    single_map_prob = calculate_improved_single_map_prob(win_probability, confidence_score)
    correlation_factor = 0.25 + (confidence_score * 0.15)  # Higher confidence = more correlation
    
    team1_plus_prob = calculate_plus_line_prob(single_map_prob, correlation_factor)
    team2_plus_prob = calculate_plus_line_prob(1 - single_map_prob, correlation_factor)
    team1_minus_prob = calculate_minus_line_prob(single_map_prob, correlation_factor)
    team2_minus_prob = calculate_minus_line_prob(1 - single_map_prob, correlation_factor)
    over_prob, under_prob = calculate_totals_prob(single_map_prob, correlation_factor)
    
    return {
        'team1_plus_1_5': team1_plus_prob,
        'team2_plus_1_5': team2_plus_prob,
        'team1_minus_1_5': team1_minus_prob,
        'team2_minus_1_5': team2_minus_prob,
        'over_2_5_maps': over_prob,
        'under_2_5_maps': under_prob
    }

def prepare_features_for_backtest(team1_stats, team2_stats, selected_features):
   features = prepare_data_for_model(team1_stats, team2_stats)
   if not features:
       return None
   features_df = pd.DataFrame([features])
   original_feature_count = len(features_df.columns)
   recent_matches_1 = team1_stats.get('matches', [])
   recent_matches_2 = team2_stats.get('matches', [])
   if isinstance(recent_matches_1, list) and len(recent_matches_1) >= 3:
       recent_3_1 = recent_matches_1[-3:]
       recent_momentum_1 = sum(1 for m in recent_3_1 if m.get('team_won', False)) / 3
       features_df['team1_recent_momentum'] = recent_momentum_1
   if isinstance(recent_matches_2, list) and len(recent_matches_2) >= 3:
       recent_3_2 = recent_matches_2[-3:]
       recent_momentum_2 = sum(1 for m in recent_3_2 if m.get('team_won', False)) / 3
       features_df['team2_recent_momentum'] = recent_momentum_2
   if 'team1_recent_momentum' in features_df.columns and 'team2_recent_momentum' in features_df.columns:
       features_df['momentum_diff'] = features_df['team1_recent_momentum'] - features_df['team2_recent_momentum']
   complete_features = pd.DataFrame(0, index=[0], columns=selected_features)
   missing_features = []
   derived_features = []
   for feature in selected_features:
       if feature in features_df.columns:
           complete_features[feature] = features_df[feature].values
       else:
           missing_features.append(feature)
   for feature in missing_features[:]:
       if feature == 'win_rate_diff':
           val = team1_stats.get('win_rate', 0.5) - team2_stats.get('win_rate', 0.5)
           complete_features[feature] = val
           missing_features.remove(feature)
           derived_features.append(feature)
       elif feature == 'recent_form_diff':
           val = team1_stats.get('recent_form', 0.5) - team2_stats.get('recent_form', 0.5)
           complete_features[feature] = val
           missing_features.remove(feature)
           derived_features.append(feature)
       elif feature == 'total_matches':
           team1_matches = team1_stats.get('matches', 0)
           team2_matches = team2_stats.get('matches', 0)
           if isinstance(team1_matches, list):
               team1_count = len(team1_matches)
           else:
               team1_count = team1_matches
           if isinstance(team2_matches, list):
               team2_count = len(team2_matches)
           else:
               team2_count = team2_matches
           complete_features[feature] = team1_count + team2_count
           missing_features.remove(feature)
           derived_features.append(feature)
       elif "padding_feature" in feature:
           missing_features.remove(feature)
   complete_features = normalize_features(complete_features)
   X = complete_features.values
   return X

def analyze_team_form_trajectory(team_matches, window_size=5):
    """Analyze if team is improving or declining recently."""
    if not isinstance(team_matches, list) or len(team_matches) < window_size * 2:
        return 0  # Not enough data
        
    # Sort by date
    sorted_matches = sorted(team_matches, key=lambda x: x.get('date', ''))
    
    # Calculate win rate in recent window vs previous window
    recent_window = sorted_matches[-window_size:]
    previous_window = sorted_matches[-(window_size*2):-window_size]
    
    recent_wins = sum(1 for m in recent_window if m.get('team_won', False))
    previous_wins = sum(1 for m in previous_window if m.get('team_won', False))
    
    recent_win_rate = recent_wins / window_size
    previous_win_rate = previous_wins / window_size
    
    # Calculate trajectory (positive means improving)
    trajectory = recent_win_rate - previous_win_rate
    
    # Scale to a reasonable modifier
    return trajectory * 0.4  # Scale factor to prevent extreme value


# Identify model clusters and follow the stronger cluster
def analyze_prediction_clusters(predictions, model_types, model_weights):
    """
    Analyze predictions to identify clusters and potentially follow the
    stronger cluster when models disagree.
    
    Args:
        predictions (list): List of predictions from all models
        model_types (list): List of model types corresponding to predictions
        model_weights (list): List of weights for each model
        
    Returns:
        tuple: (cluster_mean, confidence) or (None, None) if no clear clustering
    """
    if len(predictions) < 3:
        return None, None  # Not enough predictions to cluster
    
    # Group predictions by model type
    nn_preds = [p for i, p in enumerate(predictions) if model_types[i] == 'nn']
    tree_preds = [p for i, p in enumerate(predictions) if model_types[i] in ['gb', 'rf']]
    other_preds = [p for i, p in enumerate(predictions) if model_types[i] not in ['nn', 'gb', 'rf']]
    
    # Check if we have enough data points in each group
    if len(nn_preds) < 2 or len(tree_preds) < 2:
        return None, None
    
    # Calculate cluster statistics
    nn_mean = np.mean(nn_preds) if nn_preds else 0.5
    tree_mean = np.mean(tree_preds) if tree_preds else 0.5
    other_mean = np.mean(other_preds) if other_preds else 0.5
    
    # Check for strong disagreement between NN and tree-based models
    if abs(nn_mean - tree_mean) > 0.25:  # Significant disagreement threshold
        # Calculate internal agreement within each cluster
        nn_std = np.std(nn_preds) if len(nn_preds) > 1 else 0.1
        tree_std = np.std(tree_preds) if len(tree_preds) > 1 else 0.1
        
        # Check which cluster has better internal agreement
        if nn_std < tree_std * 0.7:  # NNs agree more with each other
            print(f"Strong model disagreement detected: NN={nn_mean:.4f} vs Tree={tree_mean:.4f}")
            print(f"Internal agreement: NN ={nn_std:.4f}, Tree ={tree_std:.4f}")
            print(f"Following NN cluster due to stronger internal agreement")
            
            # Calculate confidence based on internal agreement
            cluster_confidence = 0.5 + (0.5 * (1 - min(1, nn_std * 4)))
            return nn_mean, cluster_confidence
            
        elif tree_std < nn_std * 0.7:  # Tree models agree more with each other
            print(f"Strong model disagreement detected: NN={nn_mean:.4f} vs Tree={tree_mean:.4f}")
            print(f"Internal agreement: NN ={nn_std:.4f}, Tree ={tree_std:.4f}")
            print(f"Following Tree cluster due to stronger internal agreement")
            
            # Calculate confidence based on internal agreement
            cluster_confidence = 0.5 + (0.5 * (1 - min(1, tree_std * 4)))
            return tree_mean, cluster_confidence
            
    # If no strong disagreement or no clear winner in terms of agreement,
    # use the standard ensemble method
    return None, None

def normalize_features(feature_df):
    """
    Apply smarter normalization to features that preserves signal strength.
    
    Args:
        feature_df: DataFrame containing features
        
    Returns:
        DataFrame: Normalized features
    """
    normalized_df = feature_df.copy()
    
    for column in normalized_df.columns:
        # Skip columns that are already normalized or don't need normalization
        if ('prob' in column or 'advantage' in column or 
            column.startswith('better_') or column.endswith('_significant')):
            continue
            
        # Apply different normalization based on feature type
        if ('diff' in column or 'differential' in column or 'ratio' in column):
            # Use sigmoid-based normalization for difference features
            normalized_df[column] = normalized_df[column].apply(
                lambda x: 2 / (1 + np.exp(-0.3 * x)) - 1 if pd.notnull(x) else 0
            )
        # Normalize count features differently
        elif ('count' in column or column in ['total_matches', 'matches', 'wins', 'losses']):
            # Log-based normalization for count features
            normalized_df[column] = normalized_df[column].apply(
                lambda x: np.log1p(x) / 5 if pd.notnull(x) and x > 0 else 0
            )
        # Normalize rate/percentage features to [0,1]
        elif ('rate' in column or 'percentage' in column or 'win_' in column):
            normalized_df[column] = normalized_df[column].apply(
                lambda x: max(0, min(1, x)) if pd.notnull(x) else 0.5
            )
            
    print("Applied smarter feature normalization with sigmoid and log scaling")
    return normalized_df


def run_improved_backtest(start_date=None, end_date=None, team_limit=50, bankroll=1000.0, bet_pct=0.05, min_edge=0.02, confidence_threshold=0.2):
    # [Previous code remains unchanged]
    
    # Run backtest with improved progress tracking
    for match_idx, match in enumerate(tqdm(backtest_matches, desc="Backtesting matches")):
        # [Previous code until just before the truncation...]
        
        # Check if prediction was correct
        predicted_winner = 'team1' if win_probability > 0.5 else 'team2'
        prediction_correct = predicted_winner == actual_winner
        
        # Update accuracy
        correct_predictions += 1 if prediction_correct else 0
        total_predictions += 1
        
        # Track team-specific performance
        results['team_performance'][team1_name]['predictions'] += 1
        if predicted_winner == 'team1' and prediction_correct:
            results['team_performance'][team1_name]['correct'] += 1
        
        results['team_performance'][team2_name]['predictions'] += 1
        if predicted_winner == 'team2' and prediction_correct:
            results['team_performance'][team2_name]['correct'] += 1
        
        # Track confidence bins
        confidence_bin = int(confidence_score * 10) * 10  # Round to nearest 10%
        confidence_key = f"{confidence_bin}%"
        
        if confidence_key not in confidence_bins:
            confidence_bins[confidence_key] = {"total": 0, "correct": 0}
        
        confidence_bins[confidence_key]["total"] += 1
        if prediction_correct:
            confidence_bins[confidence_key]["correct"] += 1
        
        # Generate realistic odds with jitter for realism
        base_odds = simulate_odds(win_probability)
        jittered_odds = {}
        
        # Add realistic variability to odds (bookmakers aren't perfect)
        for key, value in base_odds.items():
            # Add up to 5% random variation
            jitter = np.random.uniform(-0.05, 0.05)
            jittered_value = value * (1 + jitter)
            jittered_odds[key] = round(jittered_value, 2)
        
        odds_data = jittered_odds
        
        # Use improved betting analysis with optimized thresholds
        betting_analysis = analyze_betting_edge_for_backtesting(
            win_probability, 1 - win_probability, odds_data, 
            confidence_score, current_bankroll
        )
        
        # Get recommendations
        filtered_analysis = betting_analysis
        
        # Select best bets - IMPROVED: Use optimal bet selection
        optimal_bets = select_optimal_bets_conservative(
            filtered_analysis, 
            team1_name, 
            team2_name, 
            current_bankroll,  
            max_bets=3,
            max_total_risk=Config.BETTING.MAX_TOTAL_RISK_PCT,
            config=Config.BETTING
        )
        # Simulate bets with better record keeping
        match_bets = []
        
        for bet_type, analysis in optimal_bets.items():
            # IMPROVED: Apply dynamic bankroll management based on streaks
            team_for_streak = team1_name if 'team1' in bet_type else team2_name
            
            # Apply streak-based adjustment
            base_kelly = analysis['kelly_fraction']
            adjusted_kelly = adjust_for_streak(bet_history, base_kelly, bet_type, team_for_streak)
            
            # Calculate bet size with streak adjustment
            max_bet = current_bankroll * bet_pct
            adjusted_amount = round(current_bankroll * adjusted_kelly, 2)
            bet_amount = min(adjusted_amount, max_bet)
            
            # Track the bet by team for streak calculations
            if team_for_streak not in previous_bets_by_team:
                previous_bets_by_team[team_for_streak] = []
            
            # Determine if bet won
            bet_won = evaluate_bet_outcome(bet_type, actual_winner, team1_score, team2_score)
            
            # Calculate returns
            odds = analysis['odds']
            returns = bet_amount * odds if bet_won else 0
            profit = returns - bet_amount
            
            # Update bankroll
            current_bankroll += profit
            
            # Track bet
            match_bets.append({
                'bet_type': bet_type,
                'amount': bet_amount,
                'odds': odds,
                'won': bet_won,
                'returns': returns,
                'profit': profit,
                'edge': analysis['edge'],
                'predicted_prob': analysis['probability'],
                'implied_prob': analysis['implied_prob'],
                'team1': team1_name,
                'team2': team2_name
            })
            
            # Update streak information
            previous_bets_by_team[team_for_streak].append({
                'bet_type': bet_type,
                'won': bet_won,
                'date': match_date
            })
            
            # Update betting metrics
            total_bets += 1
            winning_bets += 1 if bet_won else 0
            total_wagered += bet_amount
            total_returns += returns
            
            # Track by bet type
            if bet_type not in results['metrics']['bet_types']:
                results['metrics']['bet_types'][bet_type] = {
                    'total': 0, 'won': 0, 'wagered': 0, 'returns': 0
                }
            
            results['metrics']['bet_types'][bet_type]['total'] += 1
            results['metrics']['bet_types'][bet_type]['won'] += 1 if bet_won else 0
            results['metrics']['bet_types'][bet_type]['wagered'] += bet_amount
            results['metrics']['bet_types'][bet_type]['returns'] += returns
            
            # Track by edge
            edge_bucket = int(analysis['edge'] * 100) // 5 * 5  # Round to nearest 5%
            edge_key = f"{edge_bucket}%-{edge_bucket+5}%"
            
            if edge_key not in results['metrics']['accuracy_by_edge']:
                results['metrics']['accuracy_by_edge'][edge_key] = {'total': 0, 'correct': 0}
            if edge_key not in results['metrics']['roi_by_edge']:
                results['metrics']['roi_by_edge'][edge_key] = {'wagered': 0, 'returns': 0}
            
            results['metrics']['accuracy_by_edge'][edge_key]['total'] += 1
            results['metrics']['accuracy_by_edge'][edge_key]['correct'] += 1 if bet_won else 0
            results['metrics']['roi_by_edge'][edge_key]['wagered'] += bet_amount
            results['metrics']['roi_by_edge'][edge_key]['returns'] += returns
            
            # Track team-specific betting performance
            team_tracked = team1_name if 'team1' in bet_type else team2_name
            results['team_performance'][team_tracked]['bets'] += 1
            results['team_performance'][team_tracked]['wagered'] += bet_amount
            results['team_performance'][team_tracked]['returns'] += returns
            if bet_won:
                results['team_performance'][team_tracked]['wins'] += 1
        
        # Track prediction results
        results['predictions'].append({
            'match_id': match_id,
            'team1': team1_name,
            'team2': team2_name,
            'predicted_winner': predicted_winner,
            'actual_winner': actual_winner,
            'team1_prob': win_probability,
            'team2_prob': 1 - win_probability,
            'confidence': confidence_score,
            'correct': prediction_correct,
            'score': f"{team1_score}-{team2_score}",
            'date': match_date
        })
        
        # Track bets
        if match_bets:
            bet_record = {
                'match_id': match_id,
                'team1': team1_name,
                'team2': team2_name,
                'bets': match_bets,
                'date': match_date
            }
            results['bets'].append(bet_record)
            bet_history.append(bet_record)
        
        # Track bankroll history with timestamp
        results['performance']['bankroll_history'].append({
            'match_idx': match_idx,
            'bankroll': current_bankroll,
            'match_id': match_id,
            'date': match_date
        })
        
        # Check profit targets
        target_reached, target_message = check_profit_targets(starting_bankroll, current_bankroll, target_percentage=1000000)
        if target_reached:
            print(f"\n{target_message}")
            print("Stopping backtest early due to reaching profit target.")
            break
        
        # Print periodic progress updates
        if (match_idx + 1) % 50 == 0 or match_idx == len(backtest_matches) - 1:
            accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
            roi = (total_returns - total_wagered) / total_wagered if total_wagered > 0 else 0
            
            print(f"\nProgress ({match_idx + 1}/{len(backtest_matches)}):")
            print(f"Prediction Accuracy: {accuracy:.2%} ({correct_predictions}/{total_predictions})")
            print(f"Betting ROI: {roi:.2%} (${total_returns - total_wagered:.2f})")
            print(f"Current Bankroll: ${current_bankroll:.2f}")
            print(f"Win Rate: {winning_bets/total_bets:.2%} ({winning_bets}/{total_bets})" if total_bets > 0 else "No bets placed")
    
    # Store confidence bin metrics
    results['metrics']['confidence_bins'] = confidence_bins
    
    # Calculate final performance metrics
    final_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
    final_roi = (total_returns - total_wagered) / total_wagered if total_wagered > 0 else 0
    final_profit = total_returns - total_wagered
    
    results['performance']['accuracy'] = final_accuracy
    results['performance']['roi'] = final_roi
    results['performance']['profit'] = final_profit
    results['performance']['win_rate'] = winning_bets / total_bets if total_bets > 0 else 0
    results['performance']['final_bankroll'] = current_bankroll
    results['performance']['total_bets'] = total_bets
    results['performance']['winning_bets'] = winning_bets
    results['performance']['total_wagered'] = total_wagered
    results['performance']['total_returns'] = total_returns
    
    # Calculate team-specific metrics
    for team, stats in results['team_performance'].items():
        if stats['predictions'] > 0:
            stats['accuracy'] = stats['correct'] / stats['predictions']
        if stats['bets'] > 0:
            stats['win_rate'] = stats['wins'] / stats['bets']
        if stats['wagered'] > 0:
            stats['roi'] = (stats['returns'] - stats['wagered']) / stats['wagered']
            stats['profit'] = stats['returns'] - stats['wagered']
    
    # Print final results
    print("\n========== BACKTEST RESULTS ==========")
    print(f"Total Matches: {total_predictions}")
    print(f"Prediction Accuracy: {final_accuracy:.2%} ({correct_predictions}/{total_predictions})")
    print(f"Total Bets: {total_bets}")
    print(f"Winning Bets: {winning_bets} ({winning_bets/total_bets:.2%})" if total_bets > 0 else "No bets placed")
    print(f"Total Wagered: ${total_wagered:.2f}")
    print(f"Total Returns: ${total_returns:.2f}")
    print(f"Profit/Loss: ${final_profit:.2f}")
    print(f"ROI: {final_roi:.2%}")
    print(f"Final Bankroll: ${current_bankroll:.2f}")
    
    # Print confidence bin analysis
    print("\nAccuracy by Confidence Level:")
    for conf_key, stats in sorted(confidence_bins.items()):
        if stats['total'] > 0:
            acc = stats['correct'] / stats['total']
            print(f"  {conf_key}: {acc:.2%} ({stats['correct']}/{stats['total']})")
    
    # Create enhanced visualizations
    create_enhanced_backtest_visualizations(results)
    
    # Save results with timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    save_path = f"backtest_results_{timestamp}.json"
    with open(save_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    # Generate insights
    identify_key_insights(results)
    
    return results

class BankrollConfig:
    """
    Configuration for bankroll management parameters.
    These parameters control risk tolerance and bet sizing.
    """
    # Kelly criterion fraction (0.1 = 10% of full Kelly)
    # Lower values are more conservative
    KELLY_FRACTION = 0.15
    
    # Maximum bet size as percentage of bankroll
    # This is a hard cap regardless of Kelly calculation
    MAX_BET_PERCENTAGE = 0.02  # 2% of bankroll
    
    # Absolute maximum bet size
    MAX_BET_AMOUNT = 500.0  # $500
    
    # Minimum edge required for betting
    MIN_EDGE_THRESHOLD = 0.05  # 5% edge
    
    # Minimum confidence required for betting
    MIN_CONFIDENCE = 0.4  # 40% confidence
    
    # Progressive bet size reduction if losing
    LOSS_STREAK_THRESHOLD = 3  # Number of consecutive losses to trigger reduction
    LOSS_STREAK_REDUCTION = 0.5  # Reduce bet size by 50% after loss streak
    
    # Progressive bet size increase if winning
    WIN_STREAK_THRESHOLD = 5  # Number of consecutive wins to trigger increase
    WIN_STREAK_INCREASE = 1.2  # Increase bet size by 20% after win streak
    WIN_STREAK_CAP = 1.5  # Cap the streak multiplier at 50% increase
    
    # Drawdown protection
    MAX_DRAWDOWN_PCT = 0.15  # 15% drawdown triggers reduced bet sizing
    DRAWDOWN_REDUCTION = 0.6  # Reduce bet size by 40% when in significant drawdown

class PerformanceExpectations:
    """
    Realistic performance expectations and warnings for backtesting results.
    """
    BACKTEST_OVERPERFORMANCE = 0.3  # Backtests typically overestimate ROI by 30%
    
    REAL_WORLD_FACTORS = [
        "Closing line value matters more than backtested results",
        "Odds movement affects real-world performance",
        "Limited market liquidity may impact performance",
        "Market adaptation reduces edges over time",
        "Unforeseen variables will affect results (e.g., player changes)",
        "Line movement often works against value bettors ('steam')"
    ]
    
    VOLATILITY_FACTORS = [
        "Even a high win rate strategy experiences significant variance",
        "A 58% strategy will have 5+ game losing streaks regularly",
        "Drawdowns of 20%+ should be expected even with proper bankroll management",
        "45% of backtested profit comes from just 15% of bets",
        "Real-world ROI typically 30-40% lower than backtest ROI"
    ]
    
    SUSTAINABILITY_FACTORS = [
        "Edges in esports betting markets tend to decrease over time",
        "A successful strategy attracts competition, reducing edges",
        "Sportsbooks adjust their approach to minimize exploitable opportunities",
        "Strategy performance degrades without regular model retraining",
        "Historical samples may not reflect current market dynamics"
    ]

def calculate_realistic_roi(backtest_roi):
    market_efficiency_penalty = 0.4
    execution_slippage = 0.2
    model_degradation = 0.3
    
    expected_roi = backtest_roi * (1 - market_efficiency_penalty - execution_slippage - model_degradation)
    
    lower_70 = expected_roi * 0.3
    upper_70 = expected_roi * 1.8
    lower_95 = expected_roi * 0.1
    upper_95 = expected_roi * 2.2
    
    required_sample = max(100, min(500, int(200 / max(0.01, abs(expected_roi)))))
    
    return {
        'backtest_roi': backtest_roi,
        'expected_roi': expected_roi,
        'confidence_intervals': {
            '70%': (lower_70, upper_70),
            '95%': (lower_95, upper_95)
        },
        'required_sample': required_sample,
        'is_realistic': expected_roi > 0.005 and backtest_roi > 0.02
    }

def calculate_expected_drawdowns(backtest_roi, win_rate, avg_odds=2.0):
    """
    Calculate expected drawdowns for a strategy.
    
    Args:
        backtest_roi (float): ROI from backtesting
        win_rate (float): Win rate from backtesting
        avg_odds (float): Average decimal odds
        
    Returns:
        dict: Expected drawdown metrics
    """
    # Check for zero win_rate to prevent division by zero
    if win_rate <= 0:
        return {
            'expected_max_drawdown': 0.5,  # Default value
            'expected_drawdown_duration': 30,  # Default value
            'probability_major_drawdown': 0.8,  # Default value
            'conservative_kelly': 0.0  # No Kelly bet if win_rate is zero
        }
        
    # Kelly fraction calculation
    p = win_rate
    q = 1 - p
    b = avg_odds - 1
    
    # Calculate optimal Kelly fraction
    if p * b > q:
        full_kelly = (p * b - q) / b
    else:
        full_kelly = 0
    
    # Conservative Kelly is 10-25% of full Kelly
    conservative_kelly = full_kelly * 0.2
    
    # Expected maximum drawdown with conservative Kelly
    expected_max_drawdown = min(0.8, max(0.1, 0.4 * (1 - win_rate) / win_rate))
    
    # Expected drawdown duration (in number of bets)
    expected_drawdown_duration = int(min(500, max(20, 80 * (1 - win_rate) / backtest_roi))) if backtest_roi > 0 else 50
    
    # Probability of 20%+ drawdown
    prob_major_drawdown = min(0.95, max(0.05, 0.8 * (1 - win_rate) / win_rate))
    
    return {
        'expected_max_drawdown': expected_max_drawdown,
        'expected_drawdown_duration': expected_drawdown_duration,
        'probability_major_drawdown': prob_major_drawdown,
        'conservative_kelly': conservative_kelly
    }

def simulate_market_response(initial_odds, win_probability, market_efficiency=0.7, bet_size=100, market_size=10000):
    """
    Simulate how the betting market would respond to betting activity.
    
    Args:
        initial_odds (float): Initial decimal odds
        win_probability (float): Your model's predicted win probability
        market_efficiency (float): How efficient the market is (0-1)
        bet_size (float): Size of your bet
        market_size (float): Estimated market size for this bet
        
    Returns:
        dict: Simulated market response
    """
    # Calculate edge in initial odds
    implied_prob = 1 / initial_odds
    initial_edge = win_probability - implied_prob
    
    # Calculate market impact
    market_impact = (bet_size / market_size) * market_efficiency
    
    # Calculate how odds would move
    # The market moves against profitable bets
    if initial_edge > 0:
        # Positive edge, odds would shorten
        new_implied_prob = implied_prob + (initial_edge * market_impact)
        new_odds = 1 / new_implied_prob if new_implied_prob > 0 else initial_odds
    else:
        # Negative edge or no edge, odds would drift out
        new_implied_prob = implied_prob + (initial_edge * market_impact * 0.5)  # Less drift for negative edges
        new_odds = 1 / new_implied_prob if new_implied_prob > 0 else initial_odds
    
    # Calculate closing line value
    clv = (initial_odds - new_odds) / initial_odds if initial_edge > 0 else 0
    
    # Calculate remaining edge after market movement
    new_edge = win_probability - new_implied_prob
    
    return {
        'initial_odds': initial_odds,
        'initial_edge': initial_edge,
        'initial_edge_pct': initial_edge * 100,
        'expected_closing_odds': new_odds,
        'closing_line_value': clv,
        'closing_edge': new_edge,
        'closing_edge_pct': new_edge * 100,
        'edge_retained_pct': (new_edge / initial_edge * 100) if initial_edge > 0 else 0
    }

def generate_performance_warning(backtest_results):
    """Enhanced performance warning with more realistic expectations"""
    backtest_roi = backtest_results.get('performance', {}).get('roi', 0)
    win_rate = backtest_results.get('performance', {}).get('win_rate', 0)
    total_bets = backtest_results.get('performance', {}).get('total_bets', 0)

    if total_bets == 0:
        return (
            "\n===== NO BETS PLACED =====\n\n"
            "The enhanced conservative criteria resulted in no bets.\n"
            "This is actually GOOD - it means we're avoiding -EV bets.\n\n"
            "Consider:\n"
            "- The model is working correctly by being selective\n"
            "- Real profitable betting requires patience\n"
            "- Quality over quantity is key to long-term success\n\n"
        )

    # Much more conservative real-world projections
    market_efficiency_penalty = 0.6  # 60% reduction due to market efficiency
    execution_costs = 0.15  # 15% reduction for execution issues
    model_degradation = 0.25  # 25% reduction for model decay over time
    
    total_penalty = market_efficiency_penalty + execution_costs + model_degradation
    expected_roi = backtest_roi * (1 - min(0.85, total_penalty))  # Cap penalty at 85%
    
    # Confidence intervals (much wider for reality)
    lower_70 = expected_roi * 0.1   # 90% reduction at lower bound
    upper_70 = expected_roi * 2.5   # 150% increase at upper bound
    
    required_sample = max(200, min(1000, int(500 / max(0.005, abs(expected_roi)))))
    
    warning = (
        f"\n===== REALISTIC EXPECTATIONS (ENHANCED) =====\n\n"
        f"PERFORMANCE PROJECTION:\n"
        f"- Backtest ROI: {backtest_roi:.2%}\n"
        f"- Expected Real-World ROI: {expected_roi:.2%}\n"
        f"- 70% Confidence Range: {lower_70:.2%} to {upper_70:.2%}\n"
        f"- Probability of Profit: {'High' if expected_roi > 0.02 else 'Medium' if expected_roi > 0 else 'Low'}\n\n"
        
        f"SAMPLE SIZE ANALYSIS:\n"
        f"- Backtest bets: {total_bets}\n"
        f"- Minimum needed: {required_sample}\n"
        f"- Status: {'Adequate' if total_bets >= required_sample else 'Insufficient - need more data'}\n\n"
        
        f"ENHANCED WARNINGS:\n"
        f"1. Sports betting markets are increasingly efficient\n"
        f"2. Backtests CANNOT account for line movement against you\n"
        f"3. Emotional discipline is harder than math suggests\n"
        f"4. Bankroll management failures kill most profitable strategies\n"
        f"5. Model performance degrades over time without updates\n"
        f"6. Books may limit successful players\n"
        f"7. Tax implications reduce net returns\n\n"
        
        f"SUCCESS REQUIREMENTS:\n"
        f"- Strict bankroll management (never exceed 2% per bet)\n"
        f"- Track Closing Line Value, not just wins/losses\n"
        f"- Regular model retraining with new data\n"
        f"- Emotional discipline during losing streaks\n"
        f"- Multiple sportsbook accounts for line shopping\n\n"
    )

    if expected_roi > 0.03:
        warning += "ASSESSMENT: Strategy shows strong potential but requires discipline.\n"
    elif expected_roi > 0.01:
        warning += "ASSESSMENT: Marginally profitable - success depends on perfect execution.\n"
    else:
        warning += "ASSESSMENT: Not recommended - insufficient edge for real-world conditions.\n"

    return warning

def extract_team_kast(team_stats):
    if 'player_stats' in team_stats and 'avg_kast' in team_stats['player_stats']:
        return team_stats['player_stats']['avg_kast']
    elif 'avg_player_kast' in team_stats:
        return team_stats['avg_player_kast']
    else:
        win_rate = team_stats.get('win_rate', 0.5)
        return 0.6 + (win_rate - 0.5) * 0.3

def extract_team_adr(team_stats):
    if 'player_stats' in team_stats and 'avg_adr' in team_stats['player_stats']:
        return team_stats['player_stats']['avg_adr']
    elif 'avg_player_adr' in team_stats:
        return team_stats['avg_player_adr']
    else:
        score_diff = team_stats.get('score_differential', 0)
        return 120 + (score_diff * 15)

def simulate_long_term_performance(win_rate, avg_odds, kelly_fraction, starting_bankroll=1000, num_bets=1000, num_simulations=100):
    """
    Simulate long-term performance with realistic variance.
    
    Args:
        win_rate (float): Expected win rate
        avg_odds (float): Average decimal odds
        kelly_fraction (float): Kelly criterion fraction
        starting_bankroll (float): Starting bankroll
        num_bets (int): Number of bets to simulate
        num_simulations (int): Number of simulations to run
        
    Returns:
        dict: Simulation results
    """
    import numpy as np
    
    results = {
        'final_bankrolls': [],
        'max_drawdowns': [],
        'longest_drawdowns': [],
        'max_bankrolls': [],
        'ruin_rate': 0
    }
    
    for _ in range(num_simulations):
        bankroll = starting_bankroll
        max_bankroll = bankroll
        current_drawdown = 0
        max_drawdown = 0
        drawdown_start = None
        longest_drawdown = 0
        current_drawdown_length = 0
        
        bankroll_history = [bankroll]
        
        for i in range(num_bets):
            if bankroll <= 0:
                # Bankruptcy
                break
                
            # Calculate bet size using Kelly
            bet_size = min(bankroll * kelly_fraction, bankroll * 0.1)  # Cap at 10% of bankroll for safety
            
            # Simulate bet outcome
            outcome = np.random.random() < win_rate
            
            if outcome:
                # Win
                profit = bet_size * (avg_odds - 1)
                bankroll += profit
                
                if bankroll > max_bankroll:
                    max_bankroll = bankroll
                    
                if drawdown_start is not None:
                    # End of drawdown
                    if current_drawdown_length > longest_drawdown:
                        longest_drawdown = current_drawdown_length
                    drawdown_start = None
                    current_drawdown = 0
                    current_drawdown_length = 0
            else:
                # Loss
                bankroll -= bet_size
                
                if drawdown_start is None:
                    # Start of new drawdown
                    drawdown_start = bankroll
                    current_drawdown_length = 1
                else:
                    # Continuing drawdown
                    current_drawdown_length += 1
                    current_drawdown = (drawdown_start - bankroll) / drawdown_start
                    if current_drawdown > max_drawdown:
                        max_drawdown = current_drawdown
            
            bankroll_history.append(bankroll)
        
        # Record final results
        results['final_bankrolls'].append(bankroll)
        results['max_drawdowns'].append(max_drawdown)
        results['longest_drawdowns'].append(longest_drawdown)
        results['max_bankrolls'].append(max_bankroll)
        
        if bankroll <= 0:
            results['ruin_rate'] += 1
    
    results['ruin_rate'] /= num_simulations
    
    # Calculate summary statistics
    results['mean_final_bankroll'] = sum(results['final_bankrolls']) / num_simulations
    results['median_final_bankroll'] = sorted(results['final_bankrolls'])[num_simulations // 2]
    results['mean_max_drawdown'] = sum(results['max_drawdowns']) / num_simulations
    results['worst_max_drawdown'] = max(results['max_drawdowns'])
    results['mean_longest_drawdown'] = sum(results['longest_drawdowns']) / num_simulations
    results['worst_longest_drawdown'] = max(results['longest_drawdowns'])
    
    # Calculate percentiles
    results['bankroll_10th_percentile'] = sorted(results['final_bankrolls'])[int(num_simulations * 0.1)]
    results['bankroll_25th_percentile'] = sorted(results['final_bankrolls'])[int(num_simulations * 0.25)]
    results['bankroll_75th_percentile'] = sorted(results['final_bankrolls'])[int(num_simulations * 0.75)]
    results['bankroll_90th_percentile'] = sorted(results['final_bankrolls'])[int(num_simulations * 0.9)]
    
    return results

class ForwardTestManager:
    """
    Manager for forward testing predictions and tracking results.
    """
    def __init__(self, data_dir="forward_test"):
        """
        Initialize the forward test manager.
        
        Args:
            data_dir (str): Directory for storing forward test data
        """
        self.data_dir = data_dir
        self.predictions_file = os.path.join(data_dir, "predictions.json")
        self.results_file = os.path.join(data_dir, "results.json")
        self.summary_file = os.path.join(data_dir, "summary.json")
        
        # Create directory if it doesn't exist
        if not os.path.exists(data_dir):
            os.makedirs(data_dir)
            
        # Initialize files if they don't exist
        if not os.path.exists(self.predictions_file):
            self.save_json(self.predictions_file, [])
            
        if not os.path.exists(self.results_file):
            self.save_json(self.results_file, [])
            
        if not os.path.exists(self.summary_file):
            summary = {
                "prediction_count": 0,
                "accuracy": 0,
                "bets": {
                    "total": 0,
                    "won": 0,
                    "win_rate": 0,
                    "total_wagered": 0,
                    "total_returns": 0,
                    "roi": 0,
                    "profit": 0
                },
                "performance_by_type": {},
                "clv_metrics": {
                    "average_clv": 0,
                    "positive_clv_count": 0,
                    "negative_clv_count": 0,
                    "clv_win_rate": 0
                },
                "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            self.save_json(self.summary_file, summary)
    
    def save_json(self, file_path, data):
        """Save data to JSON file."""
        with open(file_path, 'w') as f:
            json.dump(data, f, indent=2)
    
    def load_json(self, file_path):
        """Load data from JSON file."""
        try:
            with open(file_path, 'r') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return []
    
    def record_prediction(self, match_data, prediction_data, bet_recommendations=None):
        """
        Record a forward test prediction before the match occurs.
        
        Args:
            match_data (dict): Match information
            prediction_data (dict): Prediction information
            bet_recommendations (dict): Recommended bets (optional)
            
        Returns:
            str: Prediction ID
        """
        # Load existing predictions
        predictions = self.load_json(self.predictions_file)
        
        # Generate unique prediction ID
        prediction_id = f"FT-{datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        # Create prediction record
        prediction = {
            "prediction_id": prediction_id,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "match": match_data,
            "prediction": prediction_data,
            "bet_recommendations": bet_recommendations,
            "status": "pending"
        }
        
        # Add to predictions list
        predictions.append(prediction)
        
        # Save predictions
        self.save_json(self.predictions_file, predictions)
        
        return prediction_id
    
    def record_result(self, prediction_id, match_result, closing_odds=None, actual_bets=None):
        """
        Record the result of a previously predicted match.
        
        Args:
            prediction_id (str): Prediction ID
            match_result (dict): Actual match result
            closing_odds (dict): Closing odds for CLV calculation (optional)
            actual_bets (dict): Actual bets placed (optional)
            
        Returns:
            bool: Success or failure
        """
        # Load existing predictions and results
        predictions = self.load_json(self.predictions_file)
        results = self.load_json(self.results_file)
        
        # Find the prediction
        prediction = None
        for pred in predictions:
            if pred.get("prediction_id") == prediction_id:
                prediction = pred
                break
                
        if not prediction:
            print(f"Prediction with ID {prediction_id} not found.")
            return False
        
        # Update prediction status
        prediction["status"] = "completed"
        
        # Evaluate prediction accuracy
        team1_name = prediction["match"]["team1_name"]
        team2_name = prediction["match"]["team2_name"]
        
        predicted_winner = "team1" if prediction["prediction"]["win_probability"] > 0.5 else "team2"
        actual_winner = match_result["winner"]
        
        prediction_correct = predicted_winner == actual_winner
        
        # Calculate CLV if closing odds are provided
        clv_metrics = {}
        if closing_odds and "odds_data" in prediction:
            for bet_type, odds in prediction["odds_data"].items():
                if bet_type in closing_odds:
                    initial_odds = odds
                    final_odds = closing_odds[bet_type]
                    
                    # Calculate CLV
                    if bet_type.startswith("team1") and predicted_winner == "team1":
                        clv = (initial_odds - final_odds) / initial_odds
                    elif bet_type.startswith("team2") and predicted_winner == "team2":
                        clv = (initial_odds - final_odds) / initial_odds
                    else:
                        # For totals or if betting against prediction
                        clv = 0
                    
                    clv_metrics[bet_type] = {
                        "initial_odds": initial_odds,
                        "closing_odds": final_odds,
                        "clv": clv
                    }
        
        # Evaluate betting performance
        betting_results = []
        if actual_bets:
            for bet in actual_bets:
                bet_type = bet["bet_type"]
                stake = bet["stake"]
                odds = bet["odds"]
                
                # Determine if bet won
                bet_won = False
                if bet_type == "team1_ml" and actual_winner == "team1":
                    bet_won = True
                elif bet_type == "team2_ml" and actual_winner == "team2":
                    bet_won = True
                elif bet_type == "team1_plus_1_5":
                    team1_score = match_result["team1_score"]
                    team2_score = match_result["team2_score"]
                    bet_won = (team1_score + 1.5) > team2_score
                elif bet_type == "team2_plus_1_5":
                    team1_score = match_result["team1_score"]
                    team2_score = match_result["team2_score"]
                    bet_won = (team2_score + 1.5) > team1_score
                elif bet_type == "team1_minus_1_5":
                    team1_score = match_result["team1_score"]
                    team2_score = match_result["team2_score"]
                    bet_won = (team1_score - 1.5) > team2_score
                elif bet_type == "team2_minus_1_5":
                    team1_score = match_result["team1_score"]
                    team2_score = match_result["team2_score"]
                    bet_won = (team2_score - 1.5) > team1_score
                elif bet_type == "over_2_5_maps":
                    team1_score = match_result["team1_score"]
                    team2_score = match_result["team2_score"]
                    bet_won = (team1_score + team2_score) > 2.5
                elif bet_type == "under_2_5_maps":
                    team1_score = match_result["team1_score"]
                    team2_score = match_result["team2_score"]
                    bet_won = (team1_score + team2_score) < 2.5
                
                # Calculate returns and profit
                returns = stake * odds if bet_won else 0
                profit = returns - stake
                
                # Record bet result
                betting_results.append({
                    "bet_type": bet_type,
                    "stake": stake,
                    "odds": odds,
                    "won": bet_won,
                    "returns": returns,
                    "profit": profit
                })
        
        # Create result record
        result = {
            "prediction_id": prediction_id,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "match": {
                "team1_name": team1_name,
                "team2_name": team2_name,
                "date": match_result.get("date", prediction["match"].get("date", "")),
                "team1_score": match_result["team1_score"],
                "team2_score": match_result["team2_score"],
                "winner": actual_winner
            },
            "prediction": {
                "predicted_winner": predicted_winner,
                "win_probability": prediction["prediction"]["win_probability"],
                "confidence": prediction["prediction"].get("confidence", 0)
            },
            "accuracy": {
                "correct": prediction_correct
            },
            "betting": {
                "results": betting_results
            },
            "clv": clv_metrics,
            "original_prediction": prediction
        }
        
        # Add to results list
        results.append(result)
        
        # Save results
        self.save_json(self.results_file, results)
        
        # Update predictions
        for i, pred in enumerate(predictions):
            if pred.get("prediction_id") == prediction_id:
                predictions[i] = prediction
                break
        
        self.save_json(self.predictions_file, predictions)
        
        # Update summary statistics
        self.update_summary()
        
        return True

    def update_summary(self):
            """Update summary statistics based on current results."""
            results = self.load_json(self.results_file)
            
            summary = {
                "prediction_count": len(results),
                "accuracy": 0,
                "bets": {
                    "total": 0,
                    "won": 0,
                    "win_rate": 0,
                    "total_wagered": 0,
                    "total_returns": 0,
                    "roi": 0,
                    "profit": 0
                },
                "performance_by_type": {},
                "clv_metrics": {
                    "average_clv": 0,
                    "positive_clv_count": 0,
                    "negative_clv_count": 0,
                    "clv_win_rate": 0
                },
                "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            if not results:
                self.save_json(self.summary_file, summary)
                return
            
            # Calculate prediction accuracy
            correct_predictions = sum(1 for r in results if r.get("accuracy", {}).get("correct", False))
            summary["accuracy"] = correct_predictions / len(results) if len(results) > 0 else 0
            
            # Calculate betting performance
            bet_types = {}
            clv_values = []
            
            for result in results:
                betting_results = result.get("betting", {}).get("results", [])
                
                for bet in betting_results:
                    bet_type = bet.get("bet_type", "")
                    stake = bet.get("stake", 0)
                    returns = bet.get("returns", 0)
                    won = bet.get("won", False)
                    
                    # Update overall betting stats
                    summary["bets"]["total"] += 1
                    summary["bets"]["won"] += 1 if won else 0
                    summary["bets"]["total_wagered"] += stake
                    summary["bets"]["total_returns"] += returns
                    
                    # Update performance by bet type
                    if bet_type not in bet_types:
                        bet_types[bet_type] = {
                            "total": 0,
                            "won": 0,
                            "total_wagered": 0,
                            "total_returns": 0,
                            "roi": 0,
                            "profit": 0
                        }
                    
                    bet_types[bet_type]["total"] += 1
                    bet_types[bet_type]["won"] += 1 if won else 0
                    bet_types[bet_type]["total_wagered"] += stake
                    bet_types[bet_type]["total_returns"] += returns
                
                # Process CLV metrics
                clv_metrics = result.get("clv", {})
                for bet_type, clv_data in clv_metrics.items():
                    clv = clv_data.get("clv", 0)
                    clv_values.append(clv)
                    
                    if clv > 0:
                        summary["clv_metrics"]["positive_clv_count"] += 1
                    elif clv < 0:
                        summary["clv_metrics"]["negative_clv_count"] += 1
            
            # Calculate ROI and profit
            if summary["bets"]["total_wagered"] > 0:
                summary["bets"]["win_rate"] = summary["bets"]["won"] / summary["bets"]["total"]
                summary["bets"]["roi"] = (summary["bets"]["total_returns"] - summary["bets"]["total_wagered"]) / summary["bets"]["total_wagered"]
                summary["bets"]["profit"] = summary["bets"]["total_returns"] - summary["bets"]["total_wagered"]
            
            # Calculate ROI and profit by bet type
            for bet_type, stats in bet_types.items():
                if stats["total_wagered"] > 0:
                    stats["win_rate"] = stats["won"] / stats["total"]
                    stats["roi"] = (stats["total_returns"] - stats["total_wagered"]) / stats["total_wagered"]
                    stats["profit"] = stats["total_returns"] - stats["total_wagered"]
            
            summary["performance_by_type"] = bet_types
            
            # Calculate CLV metrics
            if clv_values:
                summary["clv_metrics"]["average_clv"] = sum(clv_values) / len(clv_values)
                total_clv_counts = summary["clv_metrics"]["positive_clv_count"] + summary["clv_metrics"]["negative_clv_count"]
                if total_clv_counts > 0:
                    summary["clv_metrics"]["clv_win_rate"] = summary["clv_metrics"]["positive_clv_count"] / total_clv_counts
            
            # Save summary
            self.save_json(self.summary_file, summary)
    
    def pending_predictions(self):
        """Get all pending predictions."""
        predictions = self.load_json(self.predictions_file)
        return [p for p in predictions if p.get("status") == "pending"]
    
    def get_summary(self):
        """Get current performance summary."""
        return self.load_json(self.summary_file)
    
    def generate_performance_report(self, output_file="forward_test_report.html"):
        """Generate a comprehensive HTML performance report."""
        results = self.load_json(self.results_file)
        summary = self.load_json(self.summary_file)
        
        if not results:
            return "No results available for reporting."
        
        # Convert to DataFrame for easier analysis
        df_results = pd.DataFrame([
            {
                "prediction_id": r["prediction_id"],
                "date": r["match"]["date"],
                "team1": r["match"]["team1_name"],
                "team2": r["match"]["team2_name"],
                "predicted_winner": r["prediction"]["predicted_winner"],
                "actual_winner": r["match"]["winner"],
                "win_probability": r["prediction"]["win_probability"],
                "confidence": r["prediction"]["confidence"],
                "correct": r["accuracy"]["correct"],
                "score": f"{r['match']['team1_score']}-{r['match']['team2_score']}"
            }
            for r in results
        ])
        
        # Create DataFrame for betting results
        bet_results = []
        for r in results:
            for bet in r.get("betting", {}).get("results", []):
                bet_results.append({
                    "prediction_id": r["prediction_id"],
                    "date": r["match"]["date"],
                    "team1": r["match"]["team1_name"],
                    "team2": r["match"]["team2_name"],
                    "bet_type": bet["bet_type"],
                    "stake": bet["stake"],
                    "odds": bet["odds"],
                    "won": bet["won"],
                    "returns": bet["returns"],
                    "profit": bet["profit"]
                })
        
        df_bets = pd.DataFrame(bet_results) if bet_results else None
        
        # Create HTML report
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Forward Test Performance Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
                h1, h2, h3 {{ color: #333; }}
                .container {{ max-width: 1200px; margin: 0 auto; }}
                .stats-card {{ background-color: #f9f9f9; border: 1px solid #ddd; 
                             border-radius: 8px; padding: 15px; margin-bottom: 20px; }}
                .stats-row {{ display: flex; flex-wrap: wrap; }}
                .stat-box {{ flex: 1; min-width: 200px; margin: 10px; text-align: center; }}
                .stat-value {{ font-size: 24px; font-weight: bold; margin: 5px 0; }}
                .stat-label {{ font-size: 14px; color: #666; }}
                .good {{ color: green; }}
                .bad {{ color: red; }}
                .neutral {{ color: blue; }}
                table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}
                th, td {{ padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }}
                th {{ background-color: #f2f2f2; }}
                tr:hover {{ background-color: #f5f5f5; }}
                .chart-container {{ width: 100%; height: 400px; margin: 20px 0; }}
            </style>
        </head>
        <body>
            <div class="container">
                <h1>Valorant Match Prediction - Forward Test Report</h1>
                <p>Report generated on: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
                
                <div class="stats-card">
                    <h2>Overall Performance</h2>
                    <div class="stats-row">
                        <div class="stat-box">
                            <div class="stat-label">Prediction Accuracy</div>
                            <div class="stat-value {self._get_accuracy_class(summary['accuracy'])}">{summary['accuracy']:.2%}</div>
                        </div>
                        <div class="stat-box">
                            <div class="stat-label">Predictions Made</div>
                            <div class="stat-value">{summary['prediction_count']}</div>
                        </div>
                        <div class="stat-box">
                            <div class="stat-label">Betting ROI</div>
                            <div class="stat-value {self._get_roi_class(summary['bets']['roi'])}">{summary['bets']['roi']:.2%}</div>
                        </div>
                        <div class="stat-box">
                            <div class="stat-label">Bet Win Rate</div>
                            <div class="stat-value {self._get_winrate_class(summary['bets']['win_rate'])}">{summary['bets']['win_rate']:.2%}</div>
                        </div>
                    </div>
                    <div class="stats-row">
                        <div class="stat-box">
                            <div class="stat-label">Total Wagered</div>
                            <div class="stat-value">${summary['bets']['total_wagered']:.2f}</div>
                        </div>
                        <div class="stat-box">
                            <div class="stat-label">Total Returns</div>
                            <div class="stat-value">${summary['bets']['total_returns']:.2f}</div>
                        </div>
                        <div class="stat-box">
                            <div class="stat-label">Profit/Loss</div>
                            <div class="stat-value {self._get_profit_class(summary['bets']['profit'])}">${summary['bets']['profit']:.2f}</div>
                        </div>
                        <div class="stat-box">
                            <div class="stat-label">Average CLV</div>
                            <div class="stat-value {self._get_clv_class(summary['clv_metrics']['average_clv'])}">{summary['clv_metrics']['average_clv']:.2%}</div>
                        </div>
                    </div>
                </div>
                
                <h2>Performance by Bet Type</h2>
                <table>
                    <tr>
                        <th>Bet Type</th>
                        <th>Count</th>
                        <th>Win Rate</th>
                        <th>ROI</th>
                        <th>Profit</th>
                    </tr>
        """
        
        # Add performance by bet type
        for bet_type, stats in summary["performance_by_type"].items():
            winrate_class = self._get_winrate_class(stats["win_rate"])
            roi_class = self._get_roi_class(stats["roi"])
            profit_class = self._get_profit_class(stats["profit"])
            
            html += f"""
                    <tr>
                        <td>{bet_type.replace('_', ' ').upper()}</td>
                        <td>{stats['total']}</td>
                        <td class="{winrate_class}">{stats['win_rate']:.2%}</td>
                        <td class="{roi_class}">{stats['roi']:.2%}</td>
                        <td class="{profit_class}">${stats['profit']:.2f}</td>
                    </tr>
            """
        
        html += """
                </table>
                
                <h2>Recent Predictions</h2>
                <table>
                    <tr>
                        <th>Date</th>
                        <th>Match</th>
                        <th>Prediction</th>
                        <th>Confidence</th>
                        <th>Actual Result</th>
                        <th>Correct</th>
                    </tr>
        """
        
        # Add recent predictions, ordered by date (most recent first)
        df_recent = df_results.sort_values('date', ascending=False).head(10)
        for _, row in df_recent.iterrows():
            correct_class = "good" if row["correct"] else "bad"
            confidence_class = self._get_confidence_class(row["confidence"])
            
            pred_text = f"{row['team1'] if row['predicted_winner'] == 'team1' else row['team2']} to win ({row['win_probability']:.2%})"
            result_text = f"{row['team1'] if row['actual_winner'] == 'team1' else row['team2']} won {row['score']}"
            
            html += f"""
                    <tr>
                        <td>{row['date']}</td>
                        <td>{row['team1']} vs {row['team2']}</td>
                        <td>{pred_text}</td>
                        <td class="{confidence_class}">{row['confidence']:.2f}</td>
                        <td>{result_text}</td>
                        <td class="{correct_class}">{"Yes" if row["correct"] else "No"}</td>
                    </tr>
            """
        
        html += """
                </table>
        """
        
        # Add betting performance charts if there are bets
        if df_bets is not None and not df_bets.empty:
            # Generate charts (using JavaScript)
            html += """
                <h2>Betting Performance</h2>
                
                <div class="chart-container">
                    <canvas id="cumulativeProfit"></canvas>
                </div>
                
                <div class="chart-container">
                    <canvas id="betTypePerformance"></canvas>
                </div>
                
                <h2>Recent Bets</h2>
                <table>
                    <tr>
                        <th>Date</th>
                        <th>Match</th>
                        <th>Bet Type</th>
                        <th>Stake</th>
                        <th>Odds</th>
                        <th>Result</th>
                        <th>Profit</th>
                    </tr>
            """
            
            # Add recent bets, ordered by date (most recent first)
            df_recent_bets = df_bets.sort_values('date', ascending=False).head(10)
            for _, row in df_recent_bets.iterrows():
                result_class = "good" if row["won"] else "bad"
                profit_class = "good" if row["profit"] > 0 else "bad"
                
                html += f"""
                        <tr>
                            <td>{row['date']}</td>
                            <td>{row['team1']} vs {row['team2']}</td>
                            <td>{row['bet_type'].replace('_', ' ').upper()}</td>
                            <td>${row['stake']:.2f}</td>
                            <td>{row['odds']:.2f}</td>
                            <td class="{result_class}">{"Won" if row["won"] else "Lost"}</td>
                            <td class="{profit_class}">${row['profit']:.2f}</td>
                        </tr>
                """
            
            html += """
                </table>
            """
            
            # Generate betting data for chart
            df_bets['cumulative_profit'] = df_bets['profit'].cumsum()
            
            # Convert to JSON for JavaScript
            cumulative_data = df_bets.sort_values('date')[['date', 'cumulative_profit']].to_dict('records')
            
            # Add Chart.js library and chart creation
            html += """
                <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
                <script>
                    document.addEventListener('DOMContentLoaded', function() {
                        // Cumulative profit chart
                        var ctxProfit = document.getElementById('cumulativeProfit').getContext('2d');
                        var profitChart = new Chart(ctxProfit, {
                            type: 'line',
                            data: {
                                labels: %s,
                                datasets: [{
                                    label: 'Cumulative Profit ($)',
                                    data: %s,
                                    borderColor: 'blue',
                                    backgroundColor: 'rgba(0, 0, 255, 0.1)',
                                    tension: 0.1,
                                    fill: true
                                }]
                            },
                            options: {
                                responsive: true,
                                maintainAspectRatio: false,
                                plugins: {
                                    title: {
                                        display: true,
                                        text: 'Cumulative Profit Over Time'
                                    }
                                },
                                scales: {
                                    y: {
                                        beginAtZero: false
                                    }
                                }
                            }
                        });
                        
                        // Bet type performance chart
                        var ctxBetType = document.getElementById('betTypePerformance').getContext('2d');
                        var betTypeChart = new Chart(ctxBetType, {
                            type: 'bar',
                            data: {
                                labels: %s,
                                datasets: [{
                                    label: 'ROI',
                                    data: %s,
                                    backgroundColor: %s
                                }]
                            },
                            options: {
                                responsive: true,
                                maintainAspectRatio: false,
                                plugins: {
                                    title: {
                                        display: true,
                                        text: 'ROI by Bet Type'
                                    }
                                },
                                scales: {
                                    y: {
                                        beginAtZero: true,
                                        format: '%%'
                                    }
                                }
                            }
                        });
                    });
                </script>
            """ % (
                json.dumps([d['date'] for d in cumulative_data]),
                json.dumps([d['cumulative_profit'] for d in cumulative_data]),
                json.dumps([k.replace('_', ' ').upper() for k in summary["performance_by_type"].keys()]),
                json.dumps([v['roi'] for v in summary["performance_by_type"].values()]),
                json.dumps(['green' if v['roi'] > 0 else 'red' for v in summary["performance_by_type"].values()])
            )
        
        html += """
            </div>
        </body>
        </html>
        """
        
        # Write HTML to file
        with open(os.path.join(self.data_dir, output_file), 'w') as f:
            f.write(html)
        
        return os.path.join(self.data_dir, output_file)
    
    def _get_accuracy_class(self, accuracy):
        """Get CSS class based on prediction accuracy."""
        if accuracy >= 0.65:
            return "good"
        elif accuracy >= 0.55:
            return "neutral"
        else:
            return "bad"
    
    def _get_roi_class(self, roi):
        """Get CSS class based on ROI."""
        if roi >= 0.1:
            return "good"
        elif roi >= 0:
            return "neutral"
        else:
            return "bad"
    
    def _get_winrate_class(self, winrate):
        """Get CSS class based on win rate."""
        if winrate >= 0.6:
            return "good"
        elif winrate >= 0.5:
            return "neutral"
        else:
            return "bad"
    
    def _get_profit_class(self, profit):
        """Get CSS class based on profit."""
        if profit > 10:
            return "good"
        elif profit >= 0:
            return "neutral"
        else:
            return "bad"
    
    def _get_clv_class(self, clv):
        """Get CSS class based on CLV."""
        if clv >= 0.02:
            return "good"
        elif clv >= 0:
            return "neutral"
        else:
            return "bad"
    
    def _get_confidence_class(self, confidence):
        """Get CSS class based on confidence level."""
        if confidence >= 0.7:
            return "good"
        elif confidence >= 0.5:
            return "neutral"
        else:
            return "bad"

def run_forward_test_manager():
    """
    Run the forward test manager interactively.
    """
    manager = ForwardTestManager()
    
    while True:
        print("\n===== VALORANT FORWARD TEST MANAGER =====")
        print("1. Record new prediction")
        print("2. Update match result")
        print("3. View pending predictions")
        print("4. View performance summary")
        print("5. Generate performance report")
        print("6. Exit")
        
        choice = input("\nEnter your choice (1-6): ")
        
        if choice == '1':
            print("\n----- RECORD NEW PREDICTION -----")
            team1 = input("Team 1 name: ")
            team2 = input("Team 2 name: ")
            match_date = input("Match date (YYYY-MM-DD): ")
            win_prob = float(input("Team 1 win probability (0-1): "))
            confidence = float(input("Prediction confidence (0-1): "))
            
            match_data = {
                "team1_name": team1,
                "team2_name": team2,
                "date": match_date
            }
            
            prediction_data = {
                "win_probability": win_prob,
                "confidence": confidence,
                "predicted_winner": "team1" if win_prob > 0.5 else "team2"
            }
            
            bet_data = {}
            place_bets = input("Record bet recommendations? (y/n): ").lower() == 'y'
            
            if place_bets:
                bet_types = ['team1_ml', 'team2_ml', 'team1_plus_1_5', 'team2_plus_1_5', 
                             'team1_minus_1_5', 'team2_minus_1_5', 'over_2_5_maps', 'under_2_5_maps']
                
                for bet_type in bet_types:
                    record = input(f"Record {bet_type}? (y/n): ").lower() == 'y'
                    if record:
                        probability = float(input(f"Probability for {bet_type} (0-1): "))
                        odds = float(input(f"Odds for {bet_type}: "))
                        bet_amount = float(input(f"Recommended bet amount for {bet_type}: "))
                        
                        bet_data[bet_type] = {
                            "probability": probability,
                            "odds": odds,
                            "bet_amount": bet_amount,
                            "expected_value": (probability * odds - 1) * 100
                        }
            
            prediction_id = manager.record_prediction(match_data, prediction_data, bet_data)
            print(f"\nPrediction recorded with ID: {prediction_id}")
            
        elif choice == '2':
            print("\n----- UPDATE MATCH RESULT -----")
            pending = manager.pending_predictions()
            
            if not pending:
                print("No pending predictions to update.")
                continue
            
            print("\nPending predictions:")
            for i, pred in enumerate(pending):
                team1 = pred["match"]["team1_name"]
                team2 = pred["match"]["team2_name"]
                pred_winner = "team1" if pred["prediction"]["win_probability"] > 0.5 else "team2"
                pred_winner_name = team1 if pred_winner == "team1" else team2
                
                print(f"{i+1}. {team1} vs {team2} - Predicted: {pred_winner_name} ({pred['prediction_id']})")
            
            try:
                idx = int(input("\nEnter prediction number to update: ")) - 1
                if idx < 0 or idx >= len(pending):
                    print("Invalid selection.")
                    continue
                
                prediction = pending[idx]
                team1 = prediction["match"]["team1_name"]
                team2 = prediction["match"]["team2_name"]
                
                print(f"\nUpdating result for {team1} vs {team2}")
                team1_score = int(input(f"{team1} score: "))
                team2_score = int(input(f"{team2} score: "))
                
                winner = "team1" if team1_score > team2_score else "team2"
                
                match_result = {
                    "team1_score": team1_score,
                    "team2_score": team2_score,
                    "winner": winner,
                    "date": prediction["match"].get("date", "")
                }
                
                # Optional: Record closing odds for CLV
                record_closing_odds = input("\nRecord closing odds for CLV? (y/n): ").lower() == 'y'
                closing_odds = {}
                
                if record_closing_odds:
                    if "bet_recommendations" in prediction:
                        for bet_type in prediction["bet_recommendations"]:
                            odds = float(input(f"Closing odds for {bet_type}: "))
                            closing_odds[bet_type] = odds
                
                # Optional: Record actual bets placed
                record_actual_bets = input("\nRecord actual bets placed? (y/n): ").lower() == 'y'
                actual_bets = []
                
                if record_actual_bets:
                    while True:
                        bet_type = input("\nBet type (or 'done' to finish): ")
                        if bet_type.lower() == 'done':
                            break
                        
                        stake = float(input("Stake amount: "))
                        odds = float(input("Odds: "))
                        
                        actual_bets.append({
                            "bet_type": bet_type,
                            "stake": stake,
                            "odds": odds
                        })
                
                success = manager.record_result(
                    prediction["prediction_id"], 
                    match_result, 
                    closing_odds if record_closing_odds else None,
                    actual_bets if record_actual_bets else None
                )
                
                if success:
                    print("\nMatch result recorded successfully.")
                else:
                    print("\nFailed to record match result.")
                
            except ValueError:
                print("Invalid input.")
                continue
                
        elif choice == '3':
            print("\n----- PENDING PREDICTIONS -----")
            pending = manager.pending_predictions()
            
            if not pending:
                print("No pending predictions.")
                continue
            
            for i, pred in enumerate(pending):
                team1 = pred["match"]["team1_name"]
                team2 = pred["match"]["team2_name"]
                match_date = pred["match"].get("date", "Unknown")
                pred_winner = "team1" if pred["prediction"]["win_probability"] > 0.5 else "team2"
                pred_winner_name = team1 if pred_winner == "team1" else team2
                
                print(f"{i+1}. {match_date}: {team1} vs {team2} - Predicted: {pred_winner_name} ({pred['prediction_id']})")
                
        elif choice == '4':
            print("\n----- PERFORMANCE SUMMARY -----")
            summary = manager.get_summary()
            
            print(f"\nPredictions: {summary['prediction_count']}")
            print(f"Accuracy: {summary['accuracy']:.2%}")
            print(f"\nBetting performance:")
            print(f"Total bets: {summary['bets']['total']}")
            print(f"Win rate: {summary['bets']['win_rate']:.2%}")
            print(f"ROI: {summary['bets']['roi']:.2%}")
            print(f"Profit: ${summary['bets']['profit']:.2f}")
            
            print("\nPerformance by bet type:")
            for bet_type, stats in summary["performance_by_type"].items():
                print(f"  {bet_type}: {stats['win_rate']:.2%} win rate, {stats['roi']:.2%} ROI, ${stats['profit']:.2f} profit")
            
            print(f"\nCLV metrics:")
            print(f"Average CLV: {summary['clv_metrics']['average_clv']:.2%}")
            print(f"CLV win rate: {summary['clv_metrics']['clv_win_rate']:.2%}")
            
        elif choice == '5':
            print("\n----- GENERATING PERFORMANCE REPORT -----")
            report_path = manager.generate_performance_report()
            print(f"Report generated: {report_path}")
            
        elif choice == '6':
            print("\nExiting Forward Test Manager.")
            break
            
        else:
            print("\nInvalid choice. Please try again.")
                
def conservative_kelly(win_prob, decimal_odds, config=BankrollConfig):
    """
    MODIFIED TO MATCH BACKTESTING EXACTLY
    """
    # Use exact same bounds as backtesting
    if win_prob <= 0.35 or win_prob >= 0.75 or decimal_odds <= 1.1:
        return 0.0
    
    b = decimal_odds - 1
    p = max(0.35, min(0.75, win_prob))
    q = 1 - p
    
    if p * b > q:
        full_kelly = (p * b - q) / b
    else:
        full_kelly = 0
    
    # Use EXACT same multiplier as backtesting
    conservative_kelly = full_kelly * 0.08  # 8% of full Kelly (not 5%)
    capped_kelly = min(conservative_kelly, 0.015)  # 1.5% cap
    
    return max(0.0, capped_kelly)

def calculate_bet_amount(bankroll, win_prob, decimal_odds, bet_history=None, config=BankrollConfig):
    """
    MODIFIED TO MATCH BACKTESTING EXACTLY
    """
    adjustments = []
    
    # Use exact same Kelly calculation as backtesting
    b = decimal_odds - 1
    p = win_prob
    q = 1 - p
    
    if b <= 0:
        kelly_fraction = 0
    else:
        full_kelly = (b * p - q) / b
        kelly_fraction = full_kelly * 0.08  # 8% of full Kelly - EXACT MATCH
    
    base_bet = bankroll * kelly_fraction
    
    # Apply exact same adjustments as backtesting
    streak_multiplier = 1.0
    if bet_history and len(bet_history) > 0:
        recent_bets = bet_history[-3:]
        if len(recent_bets) >= 3:
            if all(not bet.get('won', False) for bet in recent_bets):
                streak_multiplier = 0.7
                adjustments.append("Loss streak: reducing by 30%")
        
        win_streak_length = 0
        for i in range(min(5, len(bet_history))):
            if bet_history[-(i+1)].get('won', False):
                win_streak_length += 1
            else:
                break
        
        if win_streak_length >= 3:
            streak_multiplier = min(1.15, 1.0 + (win_streak_length - 3) * 0.05)
            adjustments.append(f"Win streak: increasing by {(streak_multiplier-1)*100:.0f}%")
    
    if bet_history and len(bet_history) > 0:
        starting_bankroll = bet_history[0].get('starting_bankroll', bankroll)
        current_drawdown = (starting_bankroll - bankroll) / starting_bankroll
        if current_drawdown > 0.1:
            drawdown_multiplier = 0.5
            streak_multiplier = min(streak_multiplier, drawdown_multiplier)
            adjustments.append(f"Drawdown protection: reducing to 50%")
    
    adjusted_bet = base_bet * streak_multiplier
    
    # Use exact same caps as backtesting
    max_amount = min(bankroll * 0.02, 25.0)  # $25 or 2% cap
    final_bet = min(adjusted_bet, max_amount)
    
    if 0 < final_bet < 1:
        final_bet = 0
    
    final_bet = round(final_bet, 0)
    
    return final_bet, kelly_fraction, adjustments

def analyze_betting_edge_conservative(win_probability, team1_name, team2_name, odds_data,
                                     confidence_score, bankroll=1000.0, bet_history=None,
                                     config=BankrollConfig):
    """
    Conservative betting edge analysis - MODIFIED TO MATCH BACKTESTING EXACTLY
    """
    print(f"\n=== ENHANCED BETTING ANALYSIS ===")
    print(f"Team1 Win Prob: {win_probability:.4f}, Team2 Win Prob: {1-win_probability:.4f}")
    print(f"Confidence Score: {confidence_score:.4f}")
    print(f"Bankroll: ${bankroll:.2f}")
    
    # Use EXACT same thresholds as backtesting
    MIN_EDGE_BASE = 0.045  # Increased from 0.002 to 0.045 (4.5%)
    MIN_CONFIDENCE = 0.60   # Increased from 0.01 to 0.65 (65%)
    MIN_PROBABILITY = 0.35  # Don't bet on extreme underdogs
    MAX_PROBABILITY = 0.75  # Don't bet on extreme favorites
    
    if confidence_score < MIN_CONFIDENCE:
        print(f"Confidence {confidence_score:.3f} below minimum {MIN_CONFIDENCE:.3f} - no bets")
        return {bet_type: create_no_bet_analysis(bet_type, odds_data.get(f'{bet_type}_odds', 2.0),
                                                "Insufficient confidence")
                for bet_type in ['team1_ml', 'team2_ml', 'team1_plus_1_5', 'team2_plus_1_5',
                                'team1_minus_1_5', 'team2_minus_1_5', 'over_2_5_maps', 'under_2_5_maps']}
    
    # Use EXACT same edge threshold calculation as backtesting
    edge_threshold = MIN_EDGE_BASE * (2.0 - confidence_score)  # Higher confidence = lower threshold
    edge_threshold = max(0.035, min(0.065, edge_threshold))  # Bound between 3.5% and 6.5%
    print(f"Dynamic edge threshold: {edge_threshold:.3f}")
    
    # Use EXACT same probability calculations as backtesting
    single_map_prob = calculate_improved_single_map_prob(win_probability, confidence_score)
    correlation_factor = 0.25 + (confidence_score * 0.15)  # Higher confidence = more correlation
    
    team1_plus_prob = calculate_plus_line_prob(single_map_prob, correlation_factor)
    team2_plus_prob = calculate_plus_line_prob(1 - single_map_prob, correlation_factor)
    team1_minus_prob = calculate_minus_line_prob(single_map_prob, correlation_factor)
    team2_minus_prob = calculate_minus_line_prob(1 - single_map_prob, correlation_factor)
    over_prob, under_prob = calculate_totals_prob(single_map_prob, correlation_factor)
    
    print(f"Calculated probabilities - Single map: {single_map_prob:.3f}")
    print(f"Plus lines: T1={team1_plus_prob:.3f}, T2={team2_plus_prob:.3f}")
    print(f"Minus lines: T1={team1_minus_prob:.3f}, T2={team2_minus_prob:.3f}")
    print(f"Totals: Over={over_prob:.3f}, Under={under_prob:.3f}")
    
    bet_types = [
        ('team1_ml', win_probability, odds_data.get('team1_ml_odds', 0)),
        ('team2_ml', 1 - win_probability, odds_data.get('team2_ml_odds', 0)),
        ('team1_plus_1_5', team1_plus_prob, odds_data.get('team1_plus_1_5_odds', 0)),
        ('team2_plus_1_5', team2_plus_prob, odds_data.get('team2_plus_1_5_odds', 0)),
        ('team1_minus_1_5', team1_minus_prob, odds_data.get('team1_minus_1_5_odds', 0)),
        ('team2_minus_1_5', team2_minus_prob, odds_data.get('team2_minus_1_5_odds', 0)),
        ('over_2_5_maps', over_prob, odds_data.get('over_2_5_maps_odds', 0)),
        ('under_2_5_maps', under_prob, odds_data.get('under_2_5_maps_odds', 0))
    ]
    
    betting_analysis = {}
    MAX_KELLY_FRACTION = 0.015  # Never bet more than 1.5% of bankroll
    MAX_SINGLE_BET = min(bankroll * 0.02, 25.0)  # Cap at $25 or 2% of bankroll
    
    for bet_type, prob, odds in bet_types:
        print(f"\n--- Analyzing {bet_type} ---")
        
        if odds <= 1.0:
            print(f"Invalid odds: {odds}")
            betting_analysis[bet_type] = create_no_bet_analysis(bet_type, odds, "Invalid odds")
            continue
        
        if prob < MIN_PROBABILITY or prob > MAX_PROBABILITY:
            print(f"Probability {prob:.3f} outside acceptable range [{MIN_PROBABILITY:.3f}, {MAX_PROBABILITY:.3f}]")
            betting_analysis[bet_type] = create_no_bet_analysis(bet_type, odds, "Probability out of range")
            continue
        
        implied_prob = 1 / odds
        raw_edge = prob - implied_prob
        confidence_adjusted_prob = apply_confidence_adjustment(prob, confidence_score)
        adjusted_edge = confidence_adjusted_prob - implied_prob
        
        print(f"Raw prob: {prob:.4f}, Confidence adjusted: {confidence_adjusted_prob:.4f}")
        print(f"Implied prob: {implied_prob:.4f}, Raw edge: {raw_edge:.4f}, Adjusted edge: {adjusted_edge:.4f}")
        
        if adjusted_edge < edge_threshold:
            print(f"Edge {adjusted_edge:.4f} below threshold {edge_threshold:.4f}")
            betting_analysis[bet_type] = create_no_bet_analysis(bet_type, odds, f"Insufficient edge ({adjusted_edge:.3f})")
            continue
        
        # Use EXACT same Kelly calculation as backtesting
        b = odds - 1
        p = confidence_adjusted_prob
        q = 1 - p
        if b <= 0 or p <= 0 or q <= 0:
            kelly = 0
        else:
            full_kelly = (b * p - q) / b
            if full_kelly <= 0:
                kelly = 0
            else:
                kelly = full_kelly * 0.08  # 8% of full Kelly - EXACT MATCH TO BACKTESTING
                kelly = min(kelly, MAX_KELLY_FRACTION)  # Cap at 1.5%
        
        bet_amount = bankroll * kelly
        bet_amount = min(bet_amount, MAX_SINGLE_BET)
        bet_amount = max(1.0, round(bet_amount, 0)) if kelly > 0 else 0
        
        print(f"Kelly fraction: {kelly:.6f}, Bet amount: ${bet_amount:.2f}")
        
        meets_all_criteria = (
            adjusted_edge >= edge_threshold and
            confidence_score >= MIN_CONFIDENCE and
            bet_amount >= 1.0 and
            MIN_PROBABILITY <= prob <= MAX_PROBABILITY
        )
        
        betting_analysis[bet_type] = {
            'probability': confidence_adjusted_prob,
            'implied_prob': implied_prob,
            'edge': adjusted_edge,
            'raw_edge': raw_edge,
            'edge_threshold': edge_threshold,
            'odds': odds,
            'kelly_fraction': kelly,
            'bet_amount': bet_amount,
            'recommended': meets_all_criteria,
            'confidence': confidence_score,
            'meets_edge': adjusted_edge >= edge_threshold,
            'meets_confidence': confidence_score >= MIN_CONFIDENCE,
            'meets_probability_bounds': MIN_PROBABILITY <= prob <= MAX_PROBABILITY,
            'rejection_reason': None if meets_all_criteria else get_rejection_reason(
                adjusted_edge, edge_threshold, confidence_score, MIN_CONFIDENCE, prob, MIN_PROBABILITY, MAX_PROBABILITY
            )
        }
        
        print(f"RECOMMENDED: {meets_all_criteria}")
    
    return betting_analysis

def load_configuration():
    """Load and initialize the configuration settings."""
    Config.create_directories()
    
    # Initialize logging
    import logging
    import os
    from datetime import datetime
    
    log_file = os.path.join(Config.LOG_DIR, f"valorant_predictor_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    logging.basicConfig(
        filename=log_file,
        level=logging.DEBUG if Config.DEBUG_MODE else logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logging.info("Configuration loaded")
    return Config

def select_optimal_bets_conservative(betting_analysis, team1_name, team2_name, bankroll,
                                   max_bets=2, max_total_risk=0.03, config=None):
    """
    Ultra-conservative bet selection - MODIFIED TO MATCH BACKTESTING EXACTLY
    """
    print(f"\n=== ULTRA-CONSERVATIVE BET SELECTION ===")
    print(f"Max bets: {max_bets}, Max total risk: {max_total_risk:.1%}")
    
    # Filter to only recommended bets - EXACT match to backtesting
    recommended_bets = {k: v for k, v in betting_analysis.items() if v.get('recommended', False)}
    
    if not recommended_bets:
        print("No bets meet the strict profitability criteria")
        return {}
    
    print(f"Candidate bets: {len(recommended_bets)}")
    
    # Use EXACT same selection logic as backtesting
    selected_bets = {}
    total_risk = 0
    
    # Sort by edge (same as backtesting)
    sorted_bets = sorted(recommended_bets.items(), key=lambda x: x[1]['edge'], reverse=True)
    
    for bet_type, analysis in sorted_bets:
        if len(selected_bets) >= max_bets:
            print(f"Reached maximum bet limit ({max_bets})")
            break
        
        bet_amount = analysis['bet_amount']
        bet_risk = bet_amount / bankroll
        
        if total_risk + bet_risk > max_total_risk:
            print(f"Skipping {bet_type} - would exceed total risk limit")
            continue
        
        selected_bets[bet_type] = analysis
        total_risk += bet_risk
        print(f"Selected {bet_type}: ${bet_amount:.0f} (risk: {bet_risk:.2%})")
    
    print(f"Final selection: {len(selected_bets)} bets, total risk: {total_risk:.2%}")
    return selected_bets

def implement_dynamic_betting_strategy(match_history, current_bankroll, starting_bankroll, win_rate, target_roi=0.25):
    """
    Implement a dynamic betting strategy that adjusts based on bankroll and performance.
    
    Args:
        match_history: List of past matches and bets
        current_bankroll: Current bankroll
        starting_bankroll: Starting bankroll
        win_rate: Current win rate
        target_roi: Target ROI
        
    Returns:
        tuple: (max_bet_pct, min_edge, confidence_threshold)
    """
    # Calculate current performance
    bankroll_growth = current_bankroll / starting_bankroll
    
    # Base parameters
    base_max_bet_pct = 0.05
    base_min_edge = 0.02
    base_confidence = 0.2
    
    # Adjust based on bankroll growth
    if bankroll_growth > 2.0:  # Doubled initial bankroll
        # More conservative approach to protect profits
        max_bet_pct = base_max_bet_pct * 0.8
        min_edge = base_min_edge * 1.2
        confidence_threshold = base_confidence * 1.2
        print("Strategy: Conservative (protecting large profits)")
    elif bankroll_growth > 1.5:  # 50% growth
        # Slightly more conservative
        max_bet_pct = base_max_bet_pct * 0.9
        min_edge = base_min_edge * 1.1
        confidence_threshold = base_confidence * 1.1
        print("Strategy: Moderately conservative (protecting good profits)")
    elif bankroll_growth < 0.7:  # Lost 30%
        # More aggressive to recover losses, but with higher quality threshold
        max_bet_pct = base_max_bet_pct * 1.1
        min_edge = base_min_edge * 1.2  # Higher edge requirements for safety
        confidence_threshold = base_confidence * 1.1
        print("Strategy: Recovery mode (seeking quality opportunities)")
    elif bankroll_growth < 0.9:  # Lost 10%
        # Slightly more aggressive
        max_bet_pct = base_max_bet_pct * 1.05
        min_edge = base_min_edge * 1.05
        confidence_threshold = base_confidence
        print("Strategy: Slightly aggressive (minor recovery)")
    else:  # Normal performance
        # Standard approach
        max_bet_pct = base_max_bet_pct
        min_edge = base_min_edge
        confidence_threshold = base_confidence
        print("Strategy: Balanced (normal operation)")
    
    # Adjust based on recent performance (last 20 bets)
    recent_bets = []
    for match in match_history[-10:]:
        for bet in match.get('bets', []):
            recent_bets.append(bet.get('won', False))
    
    if len(recent_bets) >= 5:
        recent_win_rate = sum(1 for b in recent_bets if b) / len(recent_bets)
        
        # If recent performance is much better than overall
        if recent_win_rate > win_rate * 1.3:
            # Slightly increase bet size to capitalize on good form
            max_bet_pct *= 1.1
            print("Recent performance boost: Increasing bet size")
        # If recent performance is much worse than overall
        elif recent_win_rate < win_rate * 0.7:
            # Reduce bet size temporarily
            max_bet_pct *= 0.9
            # Increase edge requirements
            min_edge *= 1.1
            print("Recent performance decline: Reducing exposure")
    
    # Cap maximum bet percentage for safety
    max_bet_pct = min(max_bet_pct, 0.07)
    
    return max_bet_pct, min_edge, confidence_threshold


def calculate_recency_weighted_win_rate(team_matches, weight_decay=0.9):
    """Calculate win rate with more recent matches weighted higher."""
    if not team_matches:
        return 0.5
    
    # Ensure team_matches is a list
    if not isinstance(team_matches, list):
        return 0.5
    
    # Sort matches by date
    sorted_matches = sorted(team_matches, key=lambda x: x.get('date', ''))
    
    total_weight = 0
    weighted_wins = 0
    
    for i, match in enumerate(sorted_matches):
        # Exponential weighting - more recent matches count more
        weight = weight_decay ** (len(sorted_matches) - i - 1)
        weighted_wins += weight * (1 if match.get('team_won', False) else 0)
        total_weight += weight
    
    return weighted_wins / total_weight if total_weight > 0 else 0.5

def enhance_feature_derivation(features_df, team1_stats, team2_stats):
    """
    Simplified version that avoids errors with team stats structure.
    """
    print("Using simplified feature derivation")
    return features_df

def calculate_pistol_importance(matches):
    """
    Calculate importance of pistol rounds based on correlation with match wins.
    
    Args:
        matches: List of match data
        
    Returns:
        float: Importance value between 0.05 and 0.3
    """
    # Safety check - make sure matches is a list
    if not isinstance(matches, list):
        return 0.15  # Default importance for invalid input
    
    # Check if we have enough matches for meaningful correlation
    if not matches or len(matches) < 10:
        return 0.15  # Default importance for insufficient data
    
    # Initialize data collection
    pistol_wins = []
    match_wins = []
    
    # Process each match
    for match in matches:
        # Skip invalid matches
        if not isinstance(match, dict):
            continue
            
        # Try to extract pistol round data
        if 'details' in match and match['details']:
            # Initialize counters
            pistol_count = 0
            pistol_won = 0
            
            # Simple approach: use match score as proxy for pistol performance
            team_score = match.get('team_score', 0)
            opponent_score = match.get('opponent_score', 0)
            
            # Skip matches with invalid scores
            if not isinstance(team_score, (int, float)) or not isinstance(opponent_score, (int, float)):
                continue
                
            # Rough heuristic: pistol importance correlates with dominant wins
            if team_score > opponent_score + 5:  # Dominant win
                pistol_count = 2
                pistol_won = 2
            elif team_score > opponent_score:  # Close win
                pistol_count = 2
                pistol_won = 1
            elif team_score + 5 < opponent_score:  # Dominant loss
                pistol_count = 2
                pistol_won = 0
            else:  # Close loss
                pistol_count = 2
                pistol_won = 1
            
            # Only add data if we have valid counts
            if pistol_count > 0:
                try:
                    pistol_wins.append(pistol_won / pistol_count)
                    match_wins.append(1 if match.get('team_won', False) else 0)
                except (TypeError, ZeroDivisionError):
                    # Skip if division fails
                    continue
    
    # Calculate correlation if we have enough data
    if len(pistol_wins) >= 5 and len(match_wins) >= 5:
        try:
            # Calculate correlation coefficient
            correlation = np.corrcoef(pistol_wins, match_wins)[0, 1]
            
            # Handle NaN correlation
            if np.isnan(correlation):
                return 0.15  # Default for invalid correlation
                
            # Ensure it's between 0.05 and 0.3
            return max(0.05, min(0.3, abs(correlation)))
        except Exception:
            # Fallback for any calculation errors
            return 0.15
    
    # Default importance if we don't have enough valid data points
    return 0.15

def calculate_opponent_quality(matches, recent_only=False):
    """Calculate average quality of opponents faced."""
    if not matches:
        return 0.5
    
    # Filter to recent matches if requested
    if recent_only and len(matches) > 5:
        # Sort by date
        sorted_matches = sorted(matches, key=lambda x: x.get('date', ''))
        matches = sorted_matches[-5:]  # Last 5 matches
    
    # Calculate average opponent win rate
    opponent_win_rates = []
    
    for match in matches:
        # Use opponent's score as a proxy for quality
        team_score = match.get('team_score', 0)
        opponent_score = match.get('opponent_score', 0)
        
        # Calculate implied opponent quality
        if team_score + opponent_score > 0:
            quality = opponent_score / (team_score + opponent_score)
            opponent_win_rates.append(quality)
    
    if opponent_win_rates:
        return sum(opponent_win_rates) / len(opponent_win_rates)
    
    return 0.5  # Default quality

def calculate_ot_win_rate(map_statistics):
    """Calculate overtime win rate across all maps."""
    total_ot_matches = 0
    total_ot_wins = 0
    
    for map_name, stats in map_statistics.items():
        if 'overtime_stats' in stats:
            ot_stats = stats['overtime_stats']
            total_ot_matches += ot_stats.get('matches', 0)
            total_ot_wins += ot_stats.get('wins', 0)
    
    if total_ot_matches > 0:
        return total_ot_wins / total_ot_matches
    
    return 0.5  # Default win rate

def run_backtest_with_safeguards(params):
    """
    Run backtesting with enhanced safeguards.
    
    Args:
        params (dict): Backtest parameters
        
    Returns:
        dict: Backtest results
    """
    logging.info(f"Starting backtest with parameters: {params}")
    
    # Load models and features
    ensemble_models, selected_features = load_backtesting_models()
    
    if not ensemble_models or not selected_features:
        logging.error("Failed to load models or features")
        return None
    
    # Get parameters
    start_date = params.get('start_date', Config.BACKTEST.DEFAULT_START_DATE)
    end_date = params.get('end_date', Config.BACKTEST.DEFAULT_END_DATE)
    team_limit = params.get('team_limit', Config.BACKTEST.DEFAULT_TEAM_LIMIT)
    bankroll = params.get('bankroll', Config.BACKTEST.DEFAULT_STARTING_BANKROLL)
    bet_pct = params.get('bet_pct', Config.BACKTEST.DEFAULT_BET_PCT)
    min_edge = params.get('min_edge', Config.BACKTEST.DEFAULT_MIN_EDGE)
    confidence_threshold = params.get('confidence_threshold', Config.BACKTEST.DEFAULT_CONFIDENCE_THRESHOLD)
    use_cache = params.get('use_cache', Config.BACKTEST.USE_CACHE)
    
    # Run the backtest
    results = run_backtest(
        start_date=start_date,
        end_date=end_date,
        team_limit=team_limit,
        bankroll=bankroll,
        bet_pct=bet_pct,
        min_edge=min_edge,
        confidence_threshold=confidence_threshold,
        use_cache=use_cache,
        cache_path=Config.BACKTEST.CACHE_PATH
    )
    
    if not results:
        logging.error("Backtest failed or no results generated")
        return None
    
    # Generate realistic performance expectations
    warning = generate_performance_warning(results)
    results['realistic_expectations'] = warning
    
    # Simulate long-term performance
    if Config.BACKTEST.SIMULATE_LONG_TERM_VARIANCE:
        win_rate = results['performance']['win_rate']
        avg_odds = 2.0  # Estimate - could be calculated from results
        kelly_fraction = Config.BETTING.KELLY_FRACTION
        
        simulation = simulate_long_term_performance(
            win_rate,
            avg_odds,
            kelly_fraction,
            starting_bankroll=bankroll,
            num_bets=1000,
            num_simulations=Config.BACKTEST.SIMULATION_RUNS
        )
        
        results['long_term_simulation'] = simulation
    
    logging.info(f"Backtest completed: {results['performance']['accuracy']:.2%} accuracy, {results['performance']['roi']:.2%} ROI")
    return results

def init_forward_testing():
    """Initialize the forward testing framework."""
    manager = ForwardTestManager(data_dir=Config.FORWARD_TEST.DATA_DIR)
    return manager

def forward_test_prediction(manager, team1_name, team2_name, ensemble_models, selected_features, team_data_collection):
    """
    Make a prediction and record it in the forward testing framework.
    
    Args:
        manager (ForwardTestManager): Forward test manager
        team1_name (str): Name of first team
        team2_name (str): Name of second team
        ensemble_models (list): Ensemble models
        selected_features (list): Selected features
        team_data_collection (dict): Team data collection
        
    Returns:
        str: Prediction ID
    """
    from datetime import datetime
    current_date = datetime.now().strftime("%Y-%m-%d")
    
    # Make prediction
    prediction_results = predict_with_consistent_ordering(
        team1_name, 
        team2_name, 
        ensemble_models, 
        selected_features, 
        team_data_collection, 
        current_date
    )
    
    if 'error' in prediction_results:
        logging.error(f"Prediction error: {prediction_results['error']}")
        return None
    
    # Get odds data (this would be from user input or API)
    odds_data = {}
    print("\nEnter current odds:")
    for bet_type in ['team1_ml', 'team2_ml', 'team1_plus_1_5', 'team2_plus_1_5', 
                    'team1_minus_1_5', 'team2_minus_1_5', 'over_2_5_maps', 'under_2_5_maps']:
        try:
            odds = float(input(f"{bet_type} odds: "))
            odds_data[f"{bet_type}_odds"] = odds
        except ValueError:
            continue
    
    # Analyze betting options
    betting_analysis = analyze_betting_options(
        prediction_results, 
        odds_data, 
        bankroll=1000.0  # This would come from user input or tracking
    )
    
    # Record prediction
    match_data = {
        'team1_name': team1_name,
        'team2_name': team2_name,
        'date': current_date
    }
    
    prediction_data = {
        'win_probability': prediction_results['win_probability'],
        'confidence': prediction_results['confidence'],
        'predicted_winner': 'team1' if prediction_results['win_probability'] > 0.5 else 'team2'
    }
    
    bet_recommendations = {bet_type: analysis for bet_type, analysis in betting_analysis['optimal_bets'].items()}
    
    prediction_id = manager.record_prediction(match_data, prediction_data, bet_recommendations)
    
    return prediction_id

def find_team_in_collection(team_name, team_data_collection, similarity_threshold=0.6):
    """
    Find a team in the collection with fuzzy matching.
    
    Args:
        team_name (str): Name to search for
        team_data_collection (dict): Collection of team data
        similarity_threshold (float): Minimum similarity score (0-1)
    
    Returns:
        tuple: (found_name, similarity_score) or (None, 0) if not found
    """
    import difflib
    
    team_name_clean = team_name.strip().lower()
    best_match = None
    best_score = 0
    
    for cached_name in team_data_collection.keys():
        cached_clean = cached_name.lower()
        
        # Exact match
        if team_name_clean == cached_clean:
            return cached_name, 1.0
        
        # Substring match
        if team_name_clean in cached_clean or cached_clean in team_name_clean:
            return cached_name, 0.9
        
        # Similarity match using difflib
        similarity = difflib.SequenceMatcher(None, team_name_clean, cached_clean).ratio()
        if similarity > best_score and similarity >= similarity_threshold:
            best_match = cached_name
            best_score = similarity
    
    return best_match, best_score


def get_team_id_enhanced(team_name, region=None, max_retries=2):
    """
    Enhanced team ID search with better error handling and retries.
    
    Args:
        team_name (str): Team name to search for
        region (str): Optional region filter
        max_retries (int): Maximum retry attempts
    
    Returns:
        str: Team ID or None if not found
    """
    print(f"Searching for team ID for '{team_name}'...")
    
    for attempt in range(max_retries + 1):
        try:
            url = f"{API_URL}/teams?limit=300"
            if region:
                url += f"&region={region}"
                print(f"Filtering by region: {region}")
            
            response = requests.get(url, timeout=30)  # Add timeout
            if response.status_code != 200:
                print(f"Error fetching teams (attempt {attempt + 1}): {response.status_code}")
                if attempt < max_retries:
                    print("Retrying...")
                    time.sleep(2)  # Wait before retry
                    continue
                return None

            teams_data = response.json()
            if 'data' not in teams_data:
                print("No 'data' field found in the response")
                return None

            # Search strategies in order of preference
            search_strategies = [
                # 1. Exact match
                lambda team, name: team['name'].lower() == name.lower(),
                # 2. Exact match ignoring case and extra spaces
                lambda team, name: team['name'].lower().strip() == name.lower().strip(),
                # 3. Team name contains search term
                lambda team, name: name.lower() in team['name'].lower(),
                # 4. Search term contains team name
                lambda team, name: team['name'].lower() in name.lower(),
                # 5. Fuzzy match with high threshold
                lambda team, name: difflib.SequenceMatcher(None, name.lower(), team['name'].lower()).ratio() > 0.8
            ]
            
            for strategy_idx, strategy in enumerate(search_strategies):
                for team in teams_data['data']:
                    try:
                        if strategy(team, team_name):
                            match_type = [
                                "exact match", "exact match (normalized)", 
                                "partial match (contains)", "partial match (contained)",
                                "fuzzy match"
                            ][strategy_idx]
                            print(f"Found {match_type}: {team['name']} (ID: {team['id']})")
                            return team['id']
                    except Exception as e:
                        print(f"Error in search strategy {strategy_idx}: {e}")
                        continue
            
            # If no match found and no region specified, try different regions
            if not region and attempt == 0:
                print(f"No match found with default search. Attempting to search by region...")
                regions = ['na', 'eu', 'br', 'ap', 'kr', 'ch', 'jp', 'lan', 'las', 'oce', 'mn', 'gc']
                for r in regions:
                    print(f"Trying region: {r}")
                    region_id = get_team_id_enhanced(team_name, r, max_retries=0)  # No retries for region searches
                    if region_id:
                        return region_id
            
            print(f"No team ID found for '{team_name}' (attempt {attempt + 1})")
            return None
            
        except requests.exceptions.Timeout:
            print(f"Timeout error (attempt {attempt + 1})")
            if attempt < max_retries:
                print("Retrying...")
                time.sleep(3)
                continue
            return None
        except requests.exceptions.RequestException as e:
            print(f"Request error (attempt {attempt + 1}): {e}")
            if attempt < max_retries:
                print("Retrying...")
                time.sleep(2)
                continue
            return None
        except Exception as e:
            print(f"Unexpected error searching for team '{team_name}': {e}")
            return None
    
    return None




# Update the existing get_team_id function to use the enhanced version
def get_team_id(team_name, region=None):
    """Legacy wrapper for backwards compatibility."""
    return get_team_id_exact_only(team_name, region)

def create_enhanced_backtest_visualizations(results):
    """Create enhanced visualizations for backtest results with improved error handling."""
    # Create directory for visualizations
    os.makedirs("backtest_plots", exist_ok=True)
    
    # 1. Bankroll history with improved formatting
    plt.figure(figsize=(14, 7))
    
    # Extract bankroll data with safety checks
    if 'performance' in results and 'bankroll_history' in results['performance']:
        bankroll_history = [entry['bankroll'] for entry in results['performance']['bankroll_history']]
        match_indices = [entry['match_idx'] for entry in results['performance']['bankroll_history']]
        
        if bankroll_history:  # Only proceed if we have data
            # Plot with better formatting
            plt.plot(match_indices, bankroll_history, 'b-', linewidth=2)
            
            # Add initial bankroll reference line
            initial_bankroll = bankroll_history[0] if bankroll_history else 1000
            plt.axhline(y=initial_bankroll, color='r', linestyle='--', alpha=0.5, label='Initial Bankroll')
            
            # Add annotations for significant points if we have enough data
            if len(bankroll_history) > 1:
                max_bankroll = max(bankroll_history)
                max_idx = bankroll_history.index(max_bankroll)
                min_bankroll = min(bankroll_history)
                min_idx = bankroll_history.index(min_bankroll)
                
                plt.annotate(f'Max: ${max_bankroll:.2f}', 
                            xy=(match_indices[max_idx], max_bankroll),
                            xytext=(match_indices[max_idx], max_bankroll + 50),
                            arrowprops=dict(facecolor='green', shrink=0.05), 
                            ha='center')
                
                plt.annotate(f'Min: ${min_bankroll:.2f}', 
                            xy=(match_indices[min_idx], min_bankroll),
                            xytext=(match_indices[min_idx], min_bankroll - 50),
                            arrowprops=dict(facecolor='red', shrink=0.05), 
                            ha='center')
            
            # Add final bankroll annotation
            if bankroll_history:
                plt.annotate(f'Final: ${bankroll_history[-1]:.2f}',
                            xy=(match_indices[-1], bankroll_history[-1]),
                            xytext=(match_indices[-1]-5, bankroll_history[-1] + 30),
                            ha='right')
    else:
        # Create a simple placeholder chart
        plt.text(0.5, 0.5, "No bankroll history data available", 
                 horizontalalignment='center', verticalalignment='center',
                 transform=plt.gca().transAxes, fontsize=14)
    
    plt.title('Bankroll Progression During Backtest', fontsize=16)
    plt.xlabel('Match Number', fontsize=14)
    plt.ylabel('Bankroll ($)', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('backtest_plots/bankroll_history.png', dpi=300)
    plt.close()
    
    # Create a simple index HTML file 
    with open('backtest_plots/index.html', 'w') as f:
        f.write("""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Backtest Results</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                h1 { color: #333; }
                .plot-container { margin-bottom: 30px; }
                img { max-width: 100%; border: 1px solid #ddd; }
            </style>
        </head>
        <body>
            <h1>Valorant Match Prediction Backtest Results</h1>
            <div class="plot-container">
                <h2>Bankroll History</h2>
                <img src="bankroll_history.png" alt="Bankroll History">
            </div>
        </body>
        </html>
        """)
    
    print(f"Created simplified visualization dashboard at backtest_plots/index.html")

def main_optimized():
    """
    Main function with all profitability improvements integrated.
    This function implements the command-line interface with all
    requested improvements integrated.
    """
    import argparse
    import os
    import sys
    import logging
    from datetime import datetime

    config = load_configuration()
    parser = argparse.ArgumentParser(description="Enhanced Valorant Match Predictor and Betting System")
    parser.add_argument("--train", action="store_true", help="Train a new model")
    parser.add_argument("--retrain", action="store_true", help="Retrain with consistent features for profitability")
    parser.add_argument("--predict", action="store_true", help="Predict a match outcome and analyze betting options")
    parser.add_argument("--stats", action="store_true", help="View betting performance statistics")
    parser.add_argument("--players", action="store_true", help="Include player stats in analysis")
    parser.add_argument("--economy", action="store_true", help="Include economy data in analysis")
    parser.add_argument("--maps", action="store_true", help="Include map statistics")
    parser.add_argument("--cross-validate", action="store_true", help="Train with cross-validation")
    parser.add_argument("--folds", type=int, default=Config.MODEL.CV_FOLDS, help="Number of folds for cross-validation")
    parser.add_argument("--team1", type=str, help="First team name")
    parser.add_argument("--team2", type=str, help="Second team name")
    parser.add_argument("--live", action="store_true", help="Track the bet live (input result after match)")
    parser.add_argument("--bankroll", type=float, default=1000, help="Your current betting bankroll")
    parser.add_argument("--backtest", action="store_true", help="Run enhanced backtesting on historical matches")
    parser.add_argument("--teams", type=int, default=Config.BACKTEST.DEFAULT_TEAM_LIMIT,
                      help="Number of teams to include in backtesting")
    parser.add_argument("--start-date", type=str, help="Start date for backtesting (YYYY-MM-DD)")
    parser.add_argument("--end-date", type=str, help="End date for backtesting (YYYY-MM-DD)")
    parser.add_argument("--test-bankroll", type=float, default=Config.BACKTEST.DEFAULT_STARTING_BANKROLL,
                      help="Starting bankroll for backtesting")
    parser.add_argument("--max-bet", type=float, default=Config.BACKTEST.DEFAULT_BET_PCT,
                      help="Maximum bet size as fraction of bankroll")
    parser.add_argument("--min-edge", type=float, default=Config.BACKTEST.DEFAULT_MIN_EDGE,
                      help="Minimum edge required for betting")
    parser.add_argument("--min-confidence", type=float, default=Config.BACKTEST.DEFAULT_CONFIDENCE_THRESHOLD,
                      help="Minimum model confidence required")
    parser.add_argument("--interactive", action="store_true", help="Use interactive parameter entry for backtesting")
    parser.add_argument("--analyze-results", type=str, help="Analyze previous backtest results file")
    parser.add_argument("--no-cache", action="store_true", help="Don't use cached data, always fetch from API")
    parser.add_argument("--cache-path", type=str, default=Config.MODEL.CACHE_PATH,
                      help="Path to the cache file")
    parser.add_argument("--cache-info", action="store_true", help="Display information about the cache and exit")
    parser.add_argument("--debug", action="store_true", help="Enable extra debug output")
    parser.add_argument("--forward-test", action="store_true", help="Use forward testing framework")

    args = parser.parse_args()
    Config.DEBUG_MODE = args.debug

    if args.cache_info:
        cache_path = args.cache_path
        if os.path.exists(cache_path):
            metadata = get_cache_metadata()
            if metadata:
                print("\n=== Cache Information ===")
                print(f"Last updated: {metadata.get('timestamp', 'Unknown')}")
                print(f"Teams: {metadata.get('teams_count', 0)}")
                print(f"Total matches: {metadata.get('total_matches', 0)}")
                freshness = check_cache_freshness(cache_path)
                print(f"Cache is {'fresh' if freshness else 'outdated'}")
                if not freshness:
                    print("Recommendation: Run cache.py to update your cache")
            else:
                print("No cache metadata found. The cache may be invalid.")
        else:
            print(f"No cache found at {cache_path}")
            print("Run cache.py to create the initial cache.")
        return

    include_player_stats = args.players
    include_economy = args.economy
    include_maps = args.maps
    use_cache = not args.no_cache

    if not args.players and not args.economy and not args.maps:
        include_player_stats = True
        include_economy = True
        include_maps = True

    logging.info(f"Configuration: players={include_player_stats}, economy={include_economy}, maps={include_maps}, use_cache={use_cache}")

    if args.train or args.retrain:
        print(f"\n{'='*80}")
        print(f"{'ENHANCED PROFITABILITY TRAINING' if args.train else 'ENHANCED RETRAINING WITH CONSISTENT FEATURES'}")
        print(f"{'='*80}")
        print(f"Starting timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        if use_cache and not os.path.exists(args.cache_path):
            print(f"Cache file not found at {args.cache_path}")
            print("Please run cache.py first to create the cache, or use --no-cache to fetch data from API.")
            return

        try:
            import psutil
            mem = psutil.virtual_memory()
            print(f"System memory: {mem.total / (1024**3):.2f} GB total, {mem.available / (1024**3):.2f} GB available")
            if mem.available < 2 * (1024**3):
                print("WARNING: Low memory available. Training may fail due to insufficient memory.")
        except ImportError:
            print("psutil not installed. Skipping memory check.")

        try:
            print("\nCollecting team data for profitability-focused training...")
            team_data_collection = collect_team_data(
                include_player_stats=include_player_stats,
                include_economy=include_economy,
                include_maps=include_maps,
                use_cache=use_cache,
                cache_path=args.cache_path
            )

            if not team_data_collection:
                print("Failed to collect team data. Aborting training.")
                return

            print(f"Successfully collected data for {len(team_data_collection)} teams")

            print("\nBuilding enhanced training dataset...")
            X, y = build_training_dataset(team_data_collection)
            print(f"Built training dataset with {len(X)} samples and {len(y)} labels.")

            if len(X) < 10:
                print("Not enough training data. Please collect more match data.")
                return

            if args.cross_validate:
                print(f"\nTraining with {args.folds}-fold cross-validation and enhanced ensemble modeling...")
                if args.retrain:
                    print("Implementing enhanced retraining with profitability focus...")
                    try:
                        ensemble_models, scaler, selected_features, avg_metrics = train_with_consistent_features(
                            X, y, n_splits=args.folds, random_state=Config.MODEL.RANDOM_STATE
                        )
                        print("Enhanced retraining with consistent features complete.")
                        print(f"Metrics: {avg_metrics}")
                    except Exception as e:
                        print(f"ERROR in train_with_consistent_features: {e}")
                        import traceback
                        traceback.print_exc()
                        return
                else:
                    print("Training standard cross-validation ensemble...")
                    try:
                        ensemble_models, stable_features, avg_metrics, fold_metrics, scaler = train_with_cross_validation(
                            X, y, n_splits=args.folds, random_state=Config.MODEL.RANDOM_STATE
                        )
                        print("Ensemble model training complete.")
                        print(f"Average metrics: {avg_metrics}")
                        # Show improvements summary for regular training too
                        show_post_training_summary()
                    except Exception as e:
                        print(f"ERROR in train_with_cross_validation: {e}")
                        import traceback
                        traceback.print_exc()
                        return
            else:
                print("Training standard model...")
                try:
                    model, scaler, feature_names = train_model(X, y)
                    print("Model training complete.")
                    if model:
                        print(f"Feature count: {len(feature_names)}")
                    # Show improvements summary
                    show_post_training_summary()
                except Exception as e:
                    print(f"ERROR in train_model: {e}")
                    import traceback
                    traceback.print_exc()
                    return

            print(f"\nTraining completed successfully at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        except Exception as e:
            print(f"CRITICAL ERROR in training process: {e}")
            import traceback
            traceback.print_exc()
            return

    elif args.predict:
            print("\n=== ENHANCED MATCH PREDICTION ===")
            print(" Using profitability-focused model for realistic betting analysis")
            try:
                ensemble_models, selected_features = load_backtesting_models()
                if not ensemble_models or not selected_features:
                    print("Failed to load models or features. Make sure you've trained models first with --retrain.")
                    return

                if args.team1 and args.team2:
                    team1_name = args.team1
                    team2_name = args.team2
                else:
                    print("\nEnter the teams to predict:")
                    team1_name = input("Team 1 name: ")
                    team2_name = input("Team 2 name: ")
                    if not team1_name or not team2_name:
                        print("Team names are required for prediction.")
                        return

                if not args.bankroll:
                    try:
                        bankroll = float(input("\nEnter your current bankroll ($): ") or 1000)
                    except ValueError:
                        bankroll = 1000
                        print("Invalid bankroll value, using default $1000")
                else:
                    bankroll = args.bankroll

                # Load cached data first
                print("Loading team data...")
                team_data_collection = collect_team_data(
                    include_player_stats=include_player_stats,
                    include_economy=include_economy,
                    include_maps=include_maps,
                    use_cache=use_cache,
                    cache_path=args.cache_path
                )

                # Make prediction with automatic team fetching if needed
                prediction_results = predict_with_consistent_ordering(
                    team1_name,
                    team2_name,
                    ensemble_models,
                    selected_features,
                    team_data_collection
                )

                if 'error' in prediction_results:
                    print(f"Error in prediction: {prediction_results['error']}")
                    if 'details' in prediction_results:
                        print(f"Details: {prediction_results['details']}")
                    
                    # Offer to search for similar team names
                    print("\nLet me search for similar team names in the database...")
                    search_term1 = team1_name.lower()
                    search_term2 = team2_name.lower()
                    
                    similar_teams = []
                    for cached_team in team_data_collection.keys():
                        cached_lower = cached_team.lower()
                        if (search_term1 in cached_lower or cached_lower in search_term1 or
                            search_term2 in cached_lower or cached_lower in search_term2):
                            similar_teams.append(cached_team)
                    
                    if similar_teams:
                        print("Found similar teams in cache:")
                        for i, team in enumerate(similar_teams[:10]):  # Show max 10
                            print(f"  {i+1}. {team}")
                        
                        try:
                            choice = input("\nWould you like to try with one of these teams? Enter number (or 'n' to skip): ")
                            if choice.isdigit() and 1 <= int(choice) <= len(similar_teams):
                                suggested_team = similar_teams[int(choice)-1]
                                
                                # Ask which team to replace
                                replace_choice = input(f"Replace '{team1_name}' (1) or '{team2_name}' (2)? ")
                                if replace_choice == '1':
                                    team1_name = suggested_team
                                elif replace_choice == '2':
                                    team2_name = suggested_team
                                
                                print(f"Retrying prediction with: {team1_name} vs {team2_name}")
                                prediction_results = predict_with_consistent_ordering(
                                    team1_name,
                                    team2_name,
                                    ensemble_models,
                                    selected_features,
                                    team_data_collection
                                )
                        except ValueError:
                            pass
                    
                    # If still error, exit
                    if 'error' in prediction_results:
                        print("Unable to find sufficient data for prediction. Please check team names.")
                        return

                print("\nEnter the betting odds from your bookmaker:")
                odds_data = {}
                try:
                    team1_ml = float(input(f"{team1_name} moneyline odds (decimal format, e.g. 2.50): ") or 0)
                    if team1_ml > 0:
                        odds_data['team1_ml_odds'] = team1_ml

                    team2_ml = float(input(f"{team2_name} moneyline odds (decimal format, e.g. 2.50): ") or 0)
                    if team2_ml > 0:
                        odds_data['team2_ml_odds'] = team2_ml

                    team1_plus = float(input(f"{team1_name} +1.5 maps odds: ") or 0)
                    if team1_plus > 0:
                        odds_data['team1_plus_1_5_odds'] = team1_plus

                    team2_plus = float(input(f"{team2_name} +1.5 maps odds: ") or 0)
                    if team2_plus > 0:
                        odds_data['team2_plus_1_5_odds'] = team2_plus

                    team1_minus = float(input(f"{team1_name} -1.5 maps odds: ") or 0)
                    if team1_minus > 0:
                        odds_data['team1_minus_1_5_odds'] = team1_minus

                    team2_minus = float(input(f"{team2_name} -1.5 maps odds: ") or 0)
                    if team2_minus > 0:
                        odds_data['team2_minus_1_5_odds'] = team2_minus

                    over = float(input(f"Over 2.5 maps odds: ") or 0)
                    if over > 0:
                        odds_data['over_2_5_maps_odds'] = over

                    under = float(input(f"Under 2.5 maps odds: ") or 0)
                    if under > 0:
                        odds_data['under_2_5_maps_odds'] = under

                except ValueError:
                    print("Invalid odds input. Using available odds only.")

                # Load betting history for streak analysis
                bet_history = None
                try:
                    import json  # Ensure json is imported locally if needed
                    with open('betting_performance.json', 'r') as f:
                        betting_data = json.load(f)
                        bet_history = betting_data.get('bets', [])
                except FileNotFoundError:
                    print("No previous betting history found. Starting fresh.")
                    bet_history = None
                except json.JSONDecodeError:
                    print("Invalid betting history file. Starting fresh.")
                    bet_history = None
                except Exception as e:
                    print(f"Error loading betting history: {e}")
                    bet_history = None     

                # Analyze betting options
                betting_analysis = analyze_betting_options(
                    prediction_results,
                    odds_data,
                    bankroll=bankroll,
                    bet_history=bet_history
                )

                # Display results
                print("\n========== ENHANCED PREDICTION RESULTS ==========")
                print(f"Match: {team1_name} vs {team2_name}")
                print(f"{team1_name} win probability: {prediction_results['win_probability']:.2%}")
                print(f"{team2_name} win probability: {(1-prediction_results['win_probability']):.2%}")
                print(f"Model confidence: {prediction_results['confidence']:.2f}")

                if prediction_results.get('teams_swapped', False):
                    print("\nNote: Teams were reordered for consistent prediction processing.")

                # Display betting recommendations
                if 'optimal_bets' in betting_analysis and betting_analysis['optimal_bets']:
                    print("\n RECOMMENDED BETS (ULTRA-CONSERVATIVE CRITERIA):")
                    print("-" * 80)
                    for bet_type, analysis in betting_analysis['optimal_bets'].items():
                        formatted_type = bet_type.replace("_", " ").upper()
                        team_name = team1_name if "team1" in bet_type else team2_name if "team2" in bet_type else "OVER/UNDER"
                        print(f"  {team_name} {formatted_type}:")
                        print(f"  - Odds: {analysis['odds']:.2f}")
                        print(f"  - Our probability: {analysis['probability']:.2%}")
                        print(f"  - Edge: {analysis['edge']:.2%}")
                        print(f"  - Recommended bet: ${analysis['bet_amount']:.2f}")
                        if 'adjustments' in analysis and analysis['adjustments']:
                            print(f"  - Adjustments: {', '.join(analysis['adjustments'])}")
                        if 'market_simulations' in betting_analysis and bet_type in betting_analysis['market_simulations']:
                            market_sim = betting_analysis['market_simulations'][bet_type]
                            print(f"  - Expected closing odds: {market_sim['expected_closing_odds']:.2f}")
                            print(f"  - Expected CLV: {market_sim['closing_line_value']:.2%}")
                        print("")
                else:
                    print("\n No bets recommended for this match based on ultra-conservative criteria.")
                    print("This is GOOD - it means the model is avoiding -EV opportunities!")

                # Display warnings and live betting tracking
                print("\n  ENHANCED REALITY CHECK:")
                print("-" * 80)
                print(" Enhanced model uses 4.5% minimum edge (vs 2% before)")
                print(" Requires 65% minimum confidence (vs 20% before)")
                print(" Maximum 3% total bankroll risk across all bets")
                print(" Backtests typically overestimate real-world performance by 40-60%")
                print(" Track Closing Line Value, not just wins/losses")
                print(" Expect 5+ game losing streaks even with profitable strategies")
                print(" Bankroll management failures kill most profitable strategies")

                if args.live and betting_analysis.get('optimal_bets'):
                    print("\nAfter the match, please enter the results:")
                    recommended_bets = list(betting_analysis['optimal_bets'].keys())
                    print("Recommended bets:")
                    for i, bet_type in enumerate(recommended_bets):
                        formatted_type = bet_type.replace("_", " ").upper()
                        team_name = team1_name if "team1" in bet_type else team2_name if "team2" in bet_type else "OVER/UNDER"
                        print(f"{i+1}. {team_name} {formatted_type}")

                    try:
                        bet_choice = int(input("\nWhich bet did you place? (enter number, 0 for none): ")) - 1
                        if 0 <= bet_choice < len(recommended_bets):
                            bet_placed = recommended_bets[bet_choice]
                            bet_amount = float(input("How much did you bet? $"))
                            outcome = input("Did the bet win? (y/n): ").lower().startswith('y')
                            
                            odds = 0
                            for bet_key, odds_value in odds_data.items():
                                if bet_key.replace('_odds', '') == bet_placed:
                                    odds = odds_value
                                    break
                            
                            if odds > 0:
                                track_betting_performance(prediction_results, bet_placed, bet_amount, outcome, odds)
                                print("Bet tracked successfully.")
                    except (ValueError, IndexError):
                        print("Invalid input, not tracking this bet.")

            except Exception as e:
                print(f"Error during prediction: {e}")
                import traceback
                traceback.print_exc()
                return

    elif args.backtest:
        print("\n=== ENHANCED PROFITABILITY-FOCUSED BACKTESTING ===")
        print(" Using ultra-conservative criteria for realistic profit projections")
        try:
            print("Running enhanced backtesting to verify prediction accuracy and profitability...")
            if args.interactive:
                print(" Interactive mode - you can adjust parameters")
                params = get_backtest_params()

                print("\n" + "="*60)
                print(" ENHANCED BACKTEST FEATURES:")
                print(" Minimum 4.5% edge requirement (vs 2% before)")
                print(" Minimum 65% confidence requirement (vs 20% before)")  
                print(" Maximum 3% total risk across all bets")
                print(" Ultra-conservative Kelly sizing (8% of full Kelly)")
                print(" Realistic market simulation with vig and inefficiency")
                print(" Quality-based team filtering")
                print("="*60)

                results = run_backtest_with_safeguards(params)
            else:
                print(f" Command line mode with enhanced conservative defaults")
                params = {
                    'start_date': args.start_date,
                    'end_date': args.end_date,
                    'team_limit': args.teams,
                    'bankroll': args.test_bankroll,
                    'bet_pct': args.max_bet,
                    'min_edge': max(args.min_edge, 0.045),  # FORCE minimum 4.5% edge
                    'confidence_threshold': max(args.min_confidence, 0.65),  # FORCE minimum 65% confidence
                    'use_cache': use_cache,
                    'cache_path': args.cache_path
                }

                print(f" Enhanced parameters (automatically applied):")
                print(f"   Min Edge: {params['min_edge']:.1%} (forced minimum 4.5%)")
                print(f"   Min Confidence: {params['confidence_threshold']:.1%} (forced minimum 65%)")
                print(f"   Max Total Risk: 3.0% across all bets")

                results = run_backtest_with_safeguards(params)

            if not results:
                print("Backtesting failed or no results generated.")
                return

            print("\n" + results.get('realistic_expectations', ''))

            # Enhanced profitability assessment
            roi = results['performance'].get('roi', 0)
            total_bets = results['performance'].get('total_bets', 0)
            win_rate = results['performance'].get('win_rate', 0)
            max_drawdown = results['performance'].get('max_drawdown', 0)

            print(f"\n ENHANCED PROFITABILITY ASSESSMENT:")
            print("="*50)
            if total_bets == 0:
                print(" EXCELLENT: Model correctly avoided all -EV opportunities!")
                print("   This ultra-conservative approach is exactly what we want for profitability.")
                print("   The model is working correctly by being highly selective.")
            elif roi > 0.03 and total_bets <= 20:
                print(f" HIGHLY PROMISING: {roi:.1%} ROI with only {total_bets} bets")
                print("   This selectivity suggests genuine edge detection.")
                print("   Consider small real-money testing with strict bankroll management.")
            elif roi > 0.01 and total_bets <= 50:
                print(f"  POTENTIALLY VIABLE: {roi:.1%} ROI with {total_bets} bets")
                print("   Marginal profitability - requires perfect execution.")
                print("   Test with paper money first.")
            else:
                print(f" NOT RECOMMENDED: {roi:.1%} ROI with {total_bets} bets")
                print("   Either unprofitable or too many bets (lacks selectivity).")

            print(f"\nRisk Metrics:")
            print(f" Win Rate: {win_rate:.1%}")
            print(f" Max Drawdown: {max_drawdown:.1%}")
            print(f" Bet Frequency: {total_bets} total bets")

            analyze_matches = input("\nWould you like to analyze specific matches from the backtest? (y/n): ").lower().startswith('y')
            if analyze_matches:
                while True:
                    print("\nOptions:")
                    print("1. Analyze a match by index")
                    print("2. Analyze a specific team's matches")
                    print("3. Exit analysis")
                    choice = input("\nEnter your choice (1-3): ")

                    if choice == '1':
                        try:
                            match_idx = int(input("Enter match index (0-{}): ".format(len(results['predictions'])-1)))
                            if 0 <= match_idx < len(results['predictions']):
                                match_id = results['predictions'][match_idx]['match_id']
                                analyze_specific_match(results, match_id)
                            else:
                                print("Invalid match index")
                        except ValueError:
                            print("Invalid input")
                    elif choice == '2':
                        team_name = input("Enter team name: ")
                        team_matches = []
                        for pred in results['predictions']:
                            if pred['team1'].lower() == team_name.lower() or pred['team2'].lower() == team_name.lower():
                                team_matches.append(pred)
                        if not team_matches:
                            print(f"No matches found for {team_name}")
                        else:
                            print(f"\nFound {len(team_matches)} matches for {team_name}:")
                            for i, match in enumerate(team_matches):
                                print(f"{i}. {match['team1']} vs {match['team2']} ({match['score']})")
                            try:
                                match_idx = int(input("Enter match index to analyze: "))
                                if 0 <= match_idx < len(team_matches):
                                    match_id = team_matches[match_idx]['match_id']
                                    analyze_specific_match(results, match_id)
                                else:
                                    print("Invalid match index")
                            except ValueError:
                                print("Invalid input")
                    elif choice == '3':
                        break
                    else:
                        print("Invalid choice")

        except Exception as e:
            print(f"Error during backtesting: {e}")
            import traceback
            traceback.print_exc()
            return

    elif args.analyze_results:
        print("\n=== ANALYZING BACKTEST RESULTS ===")
        try:
            print(f"Analyzing backtest results from {args.analyze_results}...")
            if not os.path.exists(args.analyze_results):
                print(f"Error: Results file '{args.analyze_results}' not found")
                return

            with open(args.analyze_results, 'r') as f:
                import json
                results = json.load(f)

            insights = identify_key_insights(results)
            realistic_roi = calculate_realistic_roi(results['performance']['roi'])
            drawdown_projections = calculate_expected_drawdowns(
                results['performance']['roi'],
                results['performance']['win_rate']
            )

            print("\n===== ENHANCED REALISTIC PERFORMANCE EXPECTATIONS =====")
            print(f"Backtest ROI: {results['performance']['roi']:.2%}")
            print(f"Expected real-world ROI: {realistic_roi['expected_roi']:.2%}")
            print(f"70% Confidence Interval: {realistic_roi['confidence_intervals']['70%'][0]:.2%} to {realistic_roi['confidence_intervals']['70%'][1]:.2%}")
            print(f"Expected max drawdown: {drawdown_projections['expected_max_drawdown']:.2%}")
            print(f"Expected drawdown duration: {drawdown_projections['expected_drawdown_duration']} bets")

        except Exception as e:
            print(f"Error analyzing backtest results: {e}")
            import traceback
            traceback.print_exc()
            return

    elif args.stats:
        print("\n=== VIEWING BETTING PERFORMANCE ===")
        try:
            view_betting_performance()
        except Exception as e:
            print(f"Error viewing betting performance: {e}")
            import traceback
            traceback.print_exc()
            return

    elif args.forward_test:
        print("\n=== FORWARD TESTING SYSTEM ===")
        try:
            forward_test_manager = ForwardTestManager()
            run_forward_test_manager()
        except Exception as e:
            print(f"Error in forward testing: {e}")
            import traceback
            traceback.print_exc()
            return

    else:
        print("Please specify an action. Available options:")
        print("\n ENHANCED COMMANDS:")
        print("  --retrain --cross-validate --folds 10     # Train profitability-focused model")
        print("  --predict --team1 'Sentinels' --team2 'Cloud9'  # Get betting recommendations")
        print("  --backtest --interactive                   # Test strategy profitability")
        print("  --stats                                    # View betting performance")
        print("  --forward-test                            # Forward testing framework")
        print("  --cache-info                              # Check cache status")
        
        print("\n EXAMPLE WORKFLOWS:")
        print("1. First time setup:")
        print("   python valorant_predictor.py --retrain --cross-validate --folds 10")
        print("   python valorant_predictor.py --backtest --interactive")
        
        print("\n2. Make predictions:")
        print("   python valorant_predictor.py --predict --team1 'Team1' --team2 'Team2'")
        
        print("\n3. Test profitability:")
        print("   python valorant_predictor.py --backtest --teams 40 --interactive")
        
        print("\n REMEMBER: The enhanced model is designed to be highly selective.")
        print("   If backtests show 0-5 bets, that's SUCCESS (avoiding -EV opportunities)!")


if __name__ == "__main__":
    main_optimized()