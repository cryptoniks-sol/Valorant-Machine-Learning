

#!/usr/bin/env python3 print("Starting Deep Learning Valorant Match Predictor...") import requests import json import os import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime import argparse import pickle import time import re from tqdm import tqdm #

pasted

you are a professional data scientist with machine laerning experience that is going to help me with my machine learning project.

Here is my script. as a professional data scientist, what are the suggestions you have to increase accuracy, validation accuracy, etc.

Edit
I've reviewed your Valorant match prediction script, and as a professional data scientist, I can offer several suggestions to improve model accuracy and validation performance. Your code already implements many good practices, but here are some areas for enhancement:

1. Data Processing Improvements
Feature Engineering
Temporal Features: Add more time-based features such as team momentum (recent performance trends weighted by recency), or seasonal effects if applicable
Feature Interactions: You're already creating some interaction terms, but consider more systematic interactions between key predictors
Non-linear Transformations: Apply transformations (log, square root, etc.) to features with skewed distributions
Anomaly Detection: Implement techniques to identify and handle outlier matches
Data Quality
Missing Value Strategy: Implement more sophisticated imputation methods (MICE, KNN imputation) rather than simple zero-filling
Data Leakage Prevention: Ensure strict temporal ordering when preparing training data to avoid including future information
Feature Selection: Use more rigorous feature selection techniques like recursive feature elimination (RFE) with cross-validation
2. Model Architecture Improvements
Neural Network Design
Architecture Search: Test different layer configurations systematically (number of layers, neurons per layer)
Residual Connections: Add residual connections between layers to help with gradient flow
Ensemble Approaches: Create an ensemble of models (e.g., combine neural networks with gradient boosting)
Hyperparameter Tuning: Use Bayesian optimization for hyperparameter tuning instead of manual selection
Training Process
Learning Rate Scheduling: Implement a more sophisticated learning rate schedule (cosine annealing, one-cycle policy)
Regularization: Test different combinations of L1/L2 penalties, dropout rates, and batch normalization
Balanced Batching: Ensure training batches have balanced class representation
Gradient Accumulation: For more stable updates, especially with imbalanced data
3. Validation Improvements
Cross-Validation Strategy
Time Series Cross-Validation: Implement temporal cross-validation instead of random splits
Group-Based CV: Ensure teams don't leak between training and validation sets
Multi-Metric Evaluation: Add calibration metrics (Brier score, log loss) in addition to accuracy
Monitoring & Diagnostics
Learning Curves: Implement detailed learning curves to detect overfitting earlier
Feature Importance Stability: Track feature importance across CV folds to ensure stability
Prediction Confidence Analysis: Analyze prediction confidence vs. accuracy relationship
4. Specific Technical Modifications
python
# Add a calibration layer to improve probability estimates
def create_deep_learning_model_with_economy(input_dim):
    # [existing model code]
    
    # Add temperature scaling for better calibration
    temperature = tf.Variable(1.0, trainable=True)
    outputs = Dense(1, activation='sigmoid')(x)
    calibrated_outputs = outputs / temperature
    
    # Create model with calibration
    model = Model(inputs=inputs, outputs=calibrated_outputs)
    
    # [rest of existing code]
python
# Implement a more robust SMOTE approach with combined resampling
def train_model(X, y, test_size=0.2, random_state=42):
    # [existing code]
    
    # More sophisticated handling of class imbalance
    if np.min(class_counts) / np.sum(class_counts) < 0.4:
        from imblearn.combine import SMOTETomek
        try:
            resampler = SMOTETomek(random_state=random_state)
            X_train, y_train = resampler.fit_resample(X_train, y_train)
            print(f"After SMOTETomek resampling: X_train shape: {X_train.shape}")
        except Exception as e:
            print(f"Error applying SMOTETomek: {e}")
    
    # [rest of existing code]
python
# Add focal loss to handle class imbalance better
def focal_loss(gamma=2.0, alpha=0.25):
    def focal_loss_fixed(y_true, y_pred):
        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))
        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))
        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))
    return focal_loss_fixed

# Then in your model compilation:
model.compile(loss=focal_loss(gamma=2.0, alpha=0.25), 
             optimizer=Adam(learning_rate=0.0005),
             metrics=['accuracy', AUC(name='auc')])
5. Advanced Techniques to Consider
Transfer Learning: Train base model on all matches, then fine-tune on specific tournaments/regions
Uncertainty Estimation: Implement Monte Carlo dropout or deep ensembles for uncertainty quantification
Adversarial Training: Add slight perturbations to input features during training for better robustness
Meta-Learning: Train models to predict which prediction models will perform best under different circumstances
6. Evaluation & Monitoring Improvements
Confusion Matrix Analysis: Analyze confusion matrices for specific team matchups
Performance by Context: Track model performance across different contexts (tournaments, regions, online vs. LAN)
Betting Simulation: Implement Kelly criterion or other sophisticated betting strategies in backtesting
Model Explainability: Add SHAP or LIME analysis to better understand individual predictions
These suggestions range from simple modifications to more advanced techniques. I recommend implementing them incrementally, measuring the impact on validation metrics after each change. The most effective approach will depend on your specific dataset characteristics and the main performance bottlenecks of your current implementation.




which of these improvements will give me likely the biggest percentage of improvements

Edit
Based on my review of your Valorant match prediction code, here are the improvements most likely to give you the biggest percentage gains in model performance, listed in order of expected impact:

Feature Engineering & Selection
Creating more powerful interaction terms between key predictors (particularly between player stats and economy features)
Implementing proper feature selection to remove noise and focus on the most predictive variables
This alone could yield a 5-10% improvement as your model seems to have many features that might be introducing noise
Improved Cross-Validation Strategy
Implementing temporal cross-validation instead of random splits
Ensuring no team data leaks between training and validation
This could give you a 3-7% more realistic assessment of model performance and improve generalization
Better Handling of Class Imbalance
Implementing SMOTETomek or other advanced resampling techniques
Adding a focal loss function to emphasize difficult examples
This could improve performance by 3-8%, especially for predicting upset victories
Model Calibration
Adding a calibration layer or using techniques like Platt scaling
Implementing temperature scaling to improve probability estimates
This could give you a 2-6% improvement in prediction confidence reliability
Learning Rate Scheduling
Implementing a more sophisticated learning rate schedule
Using techniques like cosine annealing with restarts
This could improve convergence by 2-5% and help escape local minima
These five improvements, when implemented systematically, are likely to give you the biggest return on investment. The single most impactful improvement would probably be better feature engineering and selection, as neural networks are very sensitive to the quality of input features, especially when dealing with relatively small datasets (which is common in esports prediction tasks).




I am having a problem with overfitting, so can we start with Learning Curves: Implement detailed learning curves to detect overfitting earlier

Edit
I'll focus on implementing learning curves to help you detect and address overfitting earlier in your Valorant match prediction model. Learning curves are an excellent diagnostic tool for understanding model performance during training.

Here's how we can implement detailed learning curves to monitor overfitting:

python
def train_model_with_learning_curves(X, y, test_size=0.2, random_state=42):
    """Train the deep learning model with detailed learning curves for early overfitting detection."""
    # Check if we have data
    if not X or len(X) == 0:
        print("Error: No training data available")
        return None, None, None, None
        
    # Convert feature dictionary to DataFrame and then to numpy array
    df = pd.DataFrame(X)
    
    # Fill NA values with 0
    df = df.fillna(0)
    
    # Print column info for debugging
    print("\nFeature columns and their types:")
    for col in df.columns:
        print(f"{col}: {df[col].dtype}")
        # Print a few examples if the type is object (non-numeric)
        if df[col].dtype == 'object':
            print(f"  Examples: {df[col].head(3).tolist()}")
            # Convert objects to numeric if possible, otherwise drop
            try:
                df[col] = df[col].astype(float)
                print(f"  Converted {col} to float")
            except (ValueError, TypeError):
                print(f"  Dropping column {col} due to non-numeric values")
                df = df.drop(columns=[col])
    
    # Check if DataFrame is empty after cleaning
    if df.empty:
        print("Error: Empty feature dataframe after cleaning")
        return None, None, None, None
    
    # Convert to numpy array
    X_arr = df.values
    y_arr = np.array(y)
    
    print(f"\nFinal feature matrix shape: {X_arr.shape}")
    
    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_arr)
    
    # Split data into training and validation sets
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y_arr, test_size=test_size, random_state=random_state, stratify=y_arr
    )
    
    # Further split training data to create a smaller training subset for learning curves
    X_train_subset, _, y_train_subset, _ = train_test_split(
        X_train, y_train, train_size=0.5, random_state=random_state, stratify=y_train
    )
    
    # Check for class imbalance
    class_counts = np.bincount(y_train)
    print(f"Class distribution: {class_counts}")
    
    # Handle class imbalance if necessary
    if np.min(class_counts) / np.sum(class_counts) < 0.4:  # If imbalanced
        print("Detected class imbalance, applying SMOTE...")
        try:
            if np.min(class_counts) < 5:
                print("Not enough samples in minority class for SMOTE. Using original data.")
            else:
                min_samples = np.min(class_counts)
                k_neighbors = min(5, min_samples-1)
                print(f"Using k_neighbors={k_neighbors} for SMOTE (min_samples={min_samples})")
                
                smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
                X_train, y_train = smote.fit_resample(X_train, y_train)
                X_train_subset, y_train_subset = smote.fit_resample(X_train_subset, y_train_subset)
                print(f"After SMOTE: X_train shape: {X_train.shape}, X_train_subset shape: {X_train_subset.shape}")
        except Exception as e:
            print(f"Error applying SMOTE: {e}")
            print("Continuing with original data.")
    
    # Create and train model
    input_dim = X_train.shape[1]
    model = create_deep_learning_model_with_economy(input_dim)
    
    # Define callbacks
    early_stopping = EarlyStopping(
        monitor='val_loss', patience=15, restore_best_weights=True, verbose=1
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1
    )
    
    model_checkpoint = ModelCheckpoint(
        'best_valorant_model.h5', 
        save_best_only=True, 
        monitor='val_accuracy'
    )
    
    # Lists to store metrics for learning curves
    training_losses = []
    validation_losses = []
    training_accuracies = []
    validation_accuracies = []
    subset_training_losses = []
    subset_training_accuracies = []
    epochs_completed = 0
    
    class LearningCurvesCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            nonlocal epochs_completed
            epochs_completed = epoch + 1
            
            # Store main metrics
            training_losses.append(logs.get('loss'))
            validation_losses.append(logs.get('val_loss'))
            training_accuracies.append(logs.get('accuracy'))
            validation_accuracies.append(logs.get('val_accuracy'))
            
            # Evaluate on training subset (to diagnose bias/variance)
            subset_metrics = self.model.evaluate(X_train_subset, y_train_subset, verbose=0)
            subset_training_losses.append(subset_metrics[0])
            subset_training_accuracies.append(subset_metrics[1])
            
            # Print overfitting diagnostic
            train_val_loss_gap = logs.get('loss') - logs.get('val_loss')
            train_val_acc_gap = logs.get('accuracy') - logs.get('val_accuracy')
            
            overfitting_status = "SEVERE OVERFITTING" if train_val_acc_gap > 0.15 else \
                               "Moderate Overfitting" if train_val_acc_gap > 0.08 else \
                               "Slight Overfitting" if train_val_acc_gap > 0.03 else \
                               "Good Fit"
            
            print(f"\nEpoch {epoch+1} Overfitting Status: {overfitting_status}")
            print(f"Train-Val Accuracy Gap: {train_val_acc_gap:.4f}, Loss Gap: {train_val_loss_gap:.4f}")
    
    learning_curves_callback = LearningCurvesCallback()
    
    # Train model
    history = model.fit(
        X_train, y_train,
        epochs=100,
        batch_size=32,
        validation_data=(X_val, y_val),
        callbacks=[early_stopping, reduce_lr, model_checkpoint, learning_curves_callback],
        verbose=1
    )
    
    # Plot detailed learning curves
    plt.figure(figsize=(20, 10))
    
    # Loss curves - Main plot
    plt.subplot(2, 2, 1)
    plt.plot(range(1, epochs_completed + 1), training_losses, label='Training Loss', marker='o')
    plt.plot(range(1, epochs_completed + 1), validation_losses, label='Validation Loss', marker='o')
    plt.title('Model Loss', fontsize=14)
    plt.ylabel('Loss', fontsize=12)
    plt.xlabel('Epoch', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    # Accuracy curves - Main plot
    plt.subplot(2, 2, 2)
    plt.plot(range(1, epochs_completed + 1), training_accuracies, label='Training Accuracy', marker='o')
    plt.plot(range(1, epochs_completed + 1), validation_accuracies, label='Validation Accuracy', marker='o')
    plt.title('Model Accuracy', fontsize=14)
    plt.ylabel('Accuracy', fontsize=12)
    plt.xlabel('Epoch', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    # Training Loss comparison (full vs subset)
    plt.subplot(2, 2, 3)
    plt.plot(range(1, epochs_completed + 1), training_losses, label='Full Training Set Loss', marker='o')
    plt.plot(range(1, epochs_completed + 1), subset_training_losses, label='Training Subset Loss', marker='o')
    plt.title('Training Loss: Full Set vs Subset', fontsize=14)
    plt.ylabel('Loss', fontsize=12)
    plt.xlabel('Epoch', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    # Plot accuracy gaps (to visualize overfitting)
    plt.subplot(2, 2, 4)
    acc_gaps = [train - val for train, val in zip(training_accuracies, validation_accuracies)]
    plt.plot(range(1, epochs_completed + 1), acc_gaps, label='Train-Validation Accuracy Gap', marker='o', color='red')
    plt.axhline(y=0.05, color='green', linestyle='--', label='Acceptable Gap Threshold')
    plt.axhline(y=0.10, color='orange', linestyle='--', label='Moderate Overfitting Threshold')
    plt.axhline(y=0.15, color='red', linestyle='--', label='Severe Overfitting Threshold')
    plt.title('Overfitting Analysis', fontsize=14)
    plt.ylabel('Accuracy Gap', fontsize=12)
    plt.xlabel('Epoch', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig('detailed_learning_curves.png', dpi=300)
    plt.show()
    
    # Create overfitting diagnosis report
    final_train_acc = training_accuracies[-1]
    final_val_acc = validation_accuracies[-1]
    final_acc_gap = final_train_acc - final_val_acc
    
    max_val_acc = max(validation_accuracies)
    max_val_acc_epoch = validation_accuracies.index(max_val_acc) + 1
    
    optimal_stopping_epoch = early_stopping.best_epoch + 1 if hasattr(early_stopping, 'best_epoch') else max_val_acc_epoch
    
    print("\n" + "="*60)
    print("OVERFITTING DIAGNOSIS REPORT")
    print("="*60)
    
    print(f"Final Training Accuracy: {final_train_acc:.4f}")
    print(f"Final Validation Accuracy: {final_val_acc:.4f}")
    print(f"Final Accuracy Gap: {final_acc_gap:.4f}")
    
    print(f"\nHighest Validation Accuracy: {max_val_acc:.4f} (Epoch {max_val_acc_epoch})")
    print(f"Optimal Stopping Epoch: {optimal_stopping_epoch}")
    
    if final_acc_gap > 0.15:
        overfitting_diagnosis = "SEVERE OVERFITTING"
        recommendations = [
            "1. Increase regularization (L1, L2, or both)",
            "2. Increase dropout rate",
            "3. Reduce model complexity (fewer layers/neurons)",
            "4. Apply early stopping at epoch " + str(optimal_stopping_epoch),
            "5. Collect more training data if possible",
            "6. Try feature selection to reduce dimensionality"
        ]
    elif final_acc_gap > 0.08:
        overfitting_diagnosis = "MODERATE OVERFITTING"
        recommendations = [
            "1. Slightly increase regularization",
            "2. Slightly increase dropout rate",
            "3. Apply early stopping at epoch " + str(optimal_stopping_epoch),
            "4. Consider adding more diverse data augmentation"
        ]
    elif final_acc_gap > 0.03:
        overfitting_diagnosis = "SLIGHT OVERFITTING"
        recommendations = [
            "1. Apply early stopping at epoch " + str(optimal_stopping_epoch),
            "2. Consider adding small amounts of regularization",
            "3. Feature selection might help"
        ]
    else:
        overfitting_diagnosis = "GOOD FIT"
        recommendations = [
            "1. Model is well-balanced",
            "2. Consider training longer if validation accuracy is still improving",
            "3. Try increasing model capacity slightly to improve performance"
        ]
    
    print(f"\nDiagnosis: {overfitting_diagnosis}")
    print("\nRecommendations:")
    for rec in recommendations:
        print(rec)
    
    print("\nAdvanced Analysis:")
    if max(acc_gaps) > 0.15 and max(validation_accuracies) < 0.65:
        print("- Model shows signs of SEVERE overfitting but low overall performance")
        print("- This suggests the model is memorizing noise in a difficult dataset")
        print("- Consider better feature engineering before increasing regularization")
    elif max_val_acc < 0.6:
        print("- Model isn't achieving high validation accuracy")
        print("- This suggests underfitting or poor feature quality")
        print("- Consider better feature engineering or a more complex model")
    
    # Analyze learning rate effects
    if hasattr(history.history, 'lr'):
        lr_changes = [i for i, (lr1, lr2) in enumerate(zip(history.history['lr'][:-1], history.history['lr'][1:])) if lr1 != lr2]
        if lr_changes:
            print("\nLearning Rate Effect Analysis:")
            for epoch in lr_changes:
                print(f"- Learning rate changed at epoch {epoch+1}")
                print(f"  Before change: Val Acc = {validation_accuracies[epoch]:.4f}, Val Loss = {validation_losses[epoch]:.4f}")
                print(f"  After change: Val Acc = {validation_accuracies[epoch+1]:.4f}, Val Loss = {validation_losses[epoch+1]:.4f}")
    
    # Save learning curve data for future analysis
    learning_curve_data = {
        'epochs': list(range(1, epochs_completed + 1)),
        'training_loss': training_losses,
        'validation_loss': validation_losses,
        'training_accuracy': training_accuracies,
        'validation_accuracy': validation_accuracies,
        'subset_training_loss': subset_training_losses,
        'subset_training_accuracy': subset_training_accuracies,
        'accuracy_gap': acc_gaps,
        'diagnosis': overfitting_diagnosis,
        'recommendations': recommendations
    }
    
    with open('learning_curve_data.pkl', 'wb') as f:
        pickle.dump(learning_curve_data, f)
    
    # Evaluate on test set
    y_pred_proba = model.predict(X_val)
    y_pred = (y_pred_proba > 0.5).astype(int).flatten()
    
    # Calculate metrics
    accuracy = accuracy_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)
    auc = roc_auc_score(y_val, y_pred_proba)
    
    print(f"\nModel Evaluation on Validation Set:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"AUC: {auc:.4f}")
    print("="*60)
    
    # Save model artifacts
    model.save('valorant_model.h5')
    
    # Save scaler for future use
    with open('feature_scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    
    # Save feature names
    with open('feature_names.pkl', 'wb') as f:
        pickle.dump(list(df.columns), f)
    
    return model, scaler, list(df.columns), learning_curve_data
Now, to implement regularization improvements based on the learning curves diagnosis, let's modify your model architecture:

python
def create_deep_learning_model_with_economy_and_regularization(input_dim, regularization_strength=0.001, dropout_rate=0.4):
    """Create an enhanced deep learning model with adjustable regularization parameters."""
    # Define inputs
    inputs = Input(shape=(input_dim,))
    
    # First layer - shared feature processing
    x = Dense(256, activation='relu', 
              kernel_regularizer=l2(regularization_strength),
              kernel_initializer='he_normal')(inputs)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate)(x)
    
    # Second layer - deeper processing
    x = Dense(128, activation='relu', 
              kernel_regularizer=l2(regularization_strength/2),
              kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate * 0.75)(x)
    
    # Player stats pathway with additional neurons
    x = Dense(96, activation='relu', 
              kernel_regularizer=l2(regularization_strength/4),
              kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate * 0.6)(x)
    
    # Economy-specific pathway with expanded capacity
    x = Dense(64, activation='relu', 
              kernel_regularizer=l2(regularization_strength/8),
              kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate * 0.5)(x)
    
    # Combined pathway
    x = Dense(32, activation='relu', 
              kernel_regularizer=l2(regularization_strength/16),
              kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate * 0.4)(x)
    
    # Output
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Compile model
    model.compile(loss='binary_crossentropy', 
                 optimizer=Adam(learning_rate=0.0005),
                 metrics=['accuracy'])
    
    # Print model summary to see the expanded architecture
    print("\nModel Architecture with Regularization:")
    model.summary()
    
    return model
To systematically test different regularization levels based on the learning curves, here's a function to find the optimal regularization parameters:

python
def find_optimal_regularization(X, y, test_size=0.2, random_state=42):
    """Find optimal regularization parameters to combat overfitting."""
    print("\n" + "="*60)
    print("FINDING OPTIMAL REGULARIZATION PARAMETERS")
    print("="*60)
    
    # Setup data same as in train_model_with_learning_curves
    df = pd.DataFrame(X).fillna(0)
    
    # Handle non-numeric columns
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                df[col] = df[col].astype(float)
            except (ValueError, TypeError):
                df = df.drop(columns=[col])
    
    if df.empty:
        print("Error: Empty feature dataframe after cleaning")
        return None
    
    # Convert to numpy array and scale
    X_arr = df.values
    y_arr = np.array(y)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_arr)
    
    # Split data
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y_arr, test_size=test_size, random_state=random_state, stratify=y_arr
    )
    
    # Handle class imbalance if needed
    class_counts = np.bincount(y_train)
    if np.min(class_counts) / np.sum(class_counts) < 0.4:
        try:
            if np.min(class_counts) >= 5:
                min_samples = np.min(class_counts)
                k_neighbors = min(5, min_samples-1)
                smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
                X_train, y_train = smote.fit_resample(X_train, y_train)
        except Exception as e:
            print(f"Error applying SMOTE: {e}")
    
    # Define parameter grid
    regularization_strengths = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]
    dropout_rates = [0.2, 0.3, 0.4, 0.5, 0.6]
    
    # Store results
    results = []
    
    # Setup early stopping
    early_stopping = EarlyStopping(
        monitor='val_loss', patience=10, restore_best_weights=True, verbose=0
    )
    
    # Try different combinations
    input_dim = X_train.shape[1]
    total_combos = len(regularization_strengths) * len(dropout_rates)
    combo_count = 0
    
    for reg_strength in regularization_strengths:
        for dropout_rate in dropout_rates:
            combo_count += 1
            print(f"\nTesting combination {combo_count}/{total_combos}: L2={reg_strength}, Dropout={dropout_rate}")
            
            # Create and train model
            model = create_deep_learning_model_with_economy_and_regularization(
                input_dim, regularization_strength=reg_strength, dropout_rate=dropout_rate
            )
            
            history = model.fit(
                X_train, y_train,
                epochs=30,  # Reduced epochs for faster testing
                batch_size=32,
                validation_data=(X_val, y_val),
                callbacks=[early_stopping],
                verbose=0
            )
            
            # Evaluate results
            train_acc = history.history['accuracy'][-1]
            val_acc = history.history['val_accuracy'][-1]
            train_loss = history.history['loss'][-1]
            val_loss = history.history['val_loss'][-1]
            
            acc_gap = train_acc - val_acc
            epochs_trained = len(history.history['accuracy'])
            
            results.append({
                'reg_strength': reg_strength,
                'dropout_rate': dropout_rate,
                'train_acc': train_acc,
                'val_acc': val_acc,
                'train_loss': train_loss,
                'val_loss': val_loss,
                'acc_gap': acc_gap,
                'epochs_trained': epochs_trained
            })
            
            print(f"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Gap: {acc_gap:.4f}, Epochs: {epochs_trained}")
    
    # Find best parameters
    # Sort by validation accuracy, with a penalty for large gaps
    df_results = pd.DataFrame(results)
    df_results['score'] = df_results['val_acc'] - 0.5 * df_results['acc_gap']
    df_results = df_results.sort_values('score', ascending=False)
    
    best_params = df_results.iloc[0]
    print("\nBest regularization parameters found:")
    print(f"L2 Regularization Strength: {best_params['reg_strength']}")
    print(f"Dropout Rate: {best_params['dropout_rate']}")
    print(f"Validation Accuracy: {best_params['val_acc']:.4f}")
    print(f"Train-Val Accuracy Gap: {best_params['acc_gap']:.4f}")
    
    # Visualize results
    plt.figure(figsize=(15, 10))
    
    # Plot the effect of regularization strength
    plt.subplot(2, 2, 1)
    reg_effect = df_results.groupby('reg_strength')[['val_acc', 'acc_gap']].mean()
    reg_effect.plot(kind='line', marker='o', ax=plt.gca())
    plt.title('Effect of L2 Regularization', fontsize=14)
    plt.xlabel('L2 Strength', fontsize=12)
    plt.ylabel('Metric Value', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    
    # Plot the effect of dropout rate
    plt.subplot(2, 2, 2)
    dropout_effect = df_results.groupby('dropout_rate')[['val_acc', 'acc_gap']].mean()
    dropout_effect.plot(kind='line', marker='o', ax=plt.gca())
    plt.title('Effect of Dropout Rate', fontsize=14)
    plt.xlabel('Dropout Rate', fontsize=12)
    plt.ylabel('Metric Value', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    
    # Plot validation accuracy heatmap
    plt.subplot(2, 2, 3)
    pivot_val_acc = pd.pivot_table(
        df_results, values='val_acc', index='dropout_rate', columns='reg_strength'
    )
    sns.heatmap(pivot_val_acc, annot=True, cmap='viridis', fmt='.3f')
    plt.title('Validation Accuracy by Regularization Parameters', fontsize=14)
    plt.xlabel('L2 Strength', fontsize=12)
    plt.ylabel('Dropout Rate', fontsize=12)
    
    # Plot accuracy gap heatmap
    plt.subplot(2, 2, 4)
    pivot_acc_gap = pd.pivot_table(
        df_results, values='acc_gap', index='dropout_rate', columns='reg_strength'
    )
    sns.heatmap(pivot_acc_gap, annot=True, cmap='coolwarm_r', fmt='.3f')
    plt.title('Accuracy Gap by Regularization Parameters', fontsize=14)
    plt.xlabel('L2 Strength', fontsize=12)
    plt.ylabel('Dropout Rate', fontsize=12)
    
    plt.tight_layout()
    plt.savefig('regularization_parameter_search.png', dpi=300)
    plt.show()
    
    # Return best parameters
    return best_params['reg_strength'], best_params['dropout_rate']
And here's an updated main function that uses these new functions to diagnose and fix overfitting:

python
def train_model_with_overfitting_detection(X, y, test_size=0.2, random_state=42):
    """Complete pipeline to train model with overfitting detection and mitigation."""
    # Step 1: Train with learning curves to diagnose overfitting
    print("\nStep 1: Training model with learning curves to diagnose overfitting...")
    _, _, _, learning_curve_data = train_model_with_learning_curves(X, y, test_size, random_state)
    
    # Check if severe overfitting was detected
    if learning_curve_data['diagnosis'] in ["SEVERE OVERFITTING", "MODERATE OVERFITTING"]:
        print(f"\nOverfitting detected: {learning_curve_data['diagnosis']}")
        
        # Step 2: Find optimal regularization parameters
        print("\nStep 2: Finding optimal regularization parameters...")
        best_reg_strength, best_dropout_rate = find_optimal_regularization(X, y, test_size, random_state)
        
        # Step 3: Retrain with optimal regularization
        print("\nStep 3: Retraining model with optimal regularization...")
        
        # Convert and prepare data
        df = pd.DataFrame(X).fillna(0)
        
        # Handle non-numeric columns
        for col in df.columns:
            if df[col].dtype == 'object':
                try:
                    df[col] = df[col].astype(float)
                except (ValueError, TypeError):
                    df = df.drop(columns=[col])
        
        X_arr = df.values
        y_arr = np.array(y)
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_arr)
        
        # Split data
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y_arr, test_size=test_size, random_state=random_state, stratify=y_arr
        )
        
        # Handle class imbalance if needed
        class_counts = np.bincount(y_train)
        if np.min(class_counts) / np.sum(class_counts) < 0.4:
            try:
                if np.min(class_counts) >= 5:
                    min_samples = np.min(class_counts)
                    k_neighbors = min(5, min_samples-1)
                    smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
                    X_train, y_train = smote.fit_resample(X_train, y_train)
            except Exception as e:
                print(f"Error applying SMOTE: {e}")
        
        # Train final model with optimal regularization
        input_dim = X_train.shape[1]
        model = create_deep_learning_model_with_economy_and_regularization(
            input_dim, regularization_strength=best_reg_strength, dropout_rate=best_dropout_rate
        )
        
        early_stopping = EarlyStopping(
            monitor='val_loss', patience=15, restore_best_weights=True, verbose=1
        )
        
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1
        )
        
        model_checkpoint = ModelCheckpoint(
            'best_valorant_model_final.h5', 
            save_best_only=True, 
            monitor='val_accuracy'
        )
        
        history = model.fit(
            X_train, y_train,
            epochs=100,
            batch_size=32,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping, reduce_lr, model_checkpoint],
            verbose=1
        )
        
        # Save model and artifacts
        model.save('valorant_model_regularized.h5')
        
        with open('feature_scaler.pkl', 'wb') as f:
            pickle.dump(scaler, f)
        
        with open('feature_names.pkl', 'wb') as f:
            pickle.dump(list(df.columns), f)
        
        # Evaluate final model
        y_pred_proba = model.predict(X_val)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred)
        recall = recall_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred)
        auc = r




Continue

Edit
Let me continue with the implementation of learning curves and overfitting detection for your Valorant match prediction model:

python
def train_model_with_overfitting_detection(X, y, test_size=0.2, random_state=42):
    """Complete pipeline to train model with overfitting detection and mitigation."""
    # Step 1: Train with learning curves to diagnose overfitting
    print("\nStep 1: Training model with learning curves to diagnose overfitting...")
    _, _, _, learning_curve_data = train_model_with_learning_curves(X, y, test_size, random_state)
    
    # Check if severe overfitting was detected
    if learning_curve_data['diagnosis'] in ["SEVERE OVERFITTING", "MODERATE OVERFITTING"]:
        print(f"\nOverfitting detected: {learning_curve_data['diagnosis']}")
        
        # Step 2: Find optimal regularization parameters
        print("\nStep 2: Finding optimal regularization parameters...")
        best_reg_strength, best_dropout_rate = find_optimal_regularization(X, y, test_size, random_state)
        
        # Step 3: Retrain with optimal regularization
        print("\nStep 3: Retraining model with optimal regularization...")
        
        # Convert and prepare data
        df = pd.DataFrame(X).fillna(0)
        
        # Handle non-numeric columns
        for col in df.columns:
            if df[col].dtype == 'object':
                try:
                    df[col] = df[col].astype(float)
                except (ValueError, TypeError):
                    df = df.drop(columns=[col])
        
        X_arr = df.values
        y_arr = np.array(y)
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_arr)
        
        # Split data
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y_arr, test_size=test_size, random_state=random_state, stratify=y_arr
        )
        
        # Handle class imbalance if needed
        class_counts = np.bincount(y_train)
        if np.min(class_counts) / np.sum(class_counts) < 0.4:
            try:
                if np.min(class_counts) >= 5:
                    min_samples = np.min(class_counts)
                    k_neighbors = min(5, min_samples-1)
                    smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
                    X_train, y_train = smote.fit_resample(X_train, y_train)
            except Exception as e:
                print(f"Error applying SMOTE: {e}")
        
        # Train final model with optimal regularization
        input_dim = X_train.shape[1]
        model = create_deep_learning_model_with_economy_and_regularization(
            input_dim, regularization_strength=best_reg_strength, dropout_rate=best_dropout_rate
        )
        
        early_stopping = EarlyStopping(
            monitor='val_loss', patience=15, restore_best_weights=True, verbose=1
        )
        
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1
        )
        
        model_checkpoint = ModelCheckpoint(
            'best_valorant_model_final.h5', 
            save_best_only=True, 
            monitor='val_accuracy'
        )
        
        history = model.fit(
            X_train, y_train,
            epochs=100,
            batch_size=32,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping, reduce_lr, model_checkpoint],
            verbose=1
        )
        
        # Save model and artifacts
        model.save('valorant_model_regularized.h5')
        
        with open('feature_scaler.pkl', 'wb') as f:
            pickle.dump(scaler, f)
        
        with open('feature_names.pkl', 'wb') as f:
            pickle.dump(list(df.columns), f)
        
        # Evaluate final model
        y_pred_proba = model.predict(X_val)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred)
        recall = recall_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred)
        auc = roc_auc_score(y_val, y_pred_proba)
        
        print("\n" + "="*60)
        print("FINAL MODEL EVALUATION (WITH REGULARIZATION)")
        print("="*60)
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(f"AUC: {auc:.4f}")
        
        # Compare with initial model
        train_acc = history.history['accuracy'][-1]
        val_acc = history.history['val_accuracy'][-1]
        final_acc_gap = train_acc - val_acc
        
        print(f"\nFinal Train-Val Accuracy Gap: {final_acc_gap:.4f}")
        print(f"Initial Accuracy Gap: {learning_curve_data['accuracy_gap'][-1]:.4f}")
        
        reduction = 100 * (1 - final_acc_gap / learning_curve_data['accuracy_gap'][-1])
        print(f"Overfitting Reduction: {reduction:.2f}%")
        
        # Plot final learning curves comparison
        plt.figure(figsize=(15, 10))
        
        # Compare original vs regularized validation accuracy
        plt.subplot(2, 2, 1)
        plt.plot(learning_curve_data['epochs'], learning_curve_data['validation_accuracy'], 
                 label='Original Model', marker='o')
        plt.plot(range(1, len(history.history['val_accuracy']) + 1), history.history['val_accuracy'], 
                 label='Regularized Model', marker='o')
        plt.title('Validation Accuracy Comparison', fontsize=14)
        plt.ylabel('Accuracy', fontsize=12)
        plt.xlabel('Epoch', fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.legend(fontsize=12)
        
        # Compare original vs regularized accuracy gaps
        plt.subplot(2, 2, 2)
        original_gaps = learning_curve_data['accuracy_gap']
        regularized_gaps = [train_acc - val_acc for train_acc, val_acc in 
                           zip(history.history['accuracy'], history.history['val_accuracy'])]
        
        plt.plot(learning_curve_data['epochs'], original_gaps, 
                 label='Original Model', marker='o')
        plt.plot(range(1, len(regularized_gaps) + 1), regularized_gaps, 
                 label='Regularized Model', marker='o')
        plt.axhline(y=0.05, color='green', linestyle='--', label='Acceptable Gap Threshold')
        plt.title('Overfitting Comparison', fontsize=14)
        plt.ylabel('Train-Val Accuracy Gap', fontsize=12)
        plt.xlabel('Epoch', fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.legend(fontsize=12)
        
        # Plot validation loss comparison
        plt.subplot(2, 2, 3)
        plt.plot(learning_curve_data['epochs'], learning_curve_data['validation_loss'], 
                 label='Original Model', marker='o')
        plt.plot(range(1, len(history.history['val_loss']) + 1), history.history['val_loss'], 
                 label='Regularized Model', marker='o')
        plt.title('Validation Loss Comparison', fontsize=14)
        plt.ylabel('Loss', fontsize=12)
        plt.xlabel('Epoch', fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.legend(fontsize=12)
        
        # Plot final metrics comparison
        plt.subplot(2, 2, 4)
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']
        original_metrics = [
            accuracy_score(y_val, (model.predict(X_val) > 0.5).astype(int).flatten()),
            precision_score(y_val, (model.predict(X_val) > 0.5).astype(int).flatten()),
            recall_score(y_val, (model.predict(X_val) > 0.5).astype(int).flatten()),
            f1_score(y_val, (model.predict(X_val) > 0.5).astype(int).flatten()),
            roc_auc_score(y_val, model.predict(X_val))
        ]
        
        regularized_metrics = [accuracy, precision, recall, f1, auc]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        plt.bar(x - width/2, original_metrics, width, label='Original Model')
        plt.bar(x + width/2, regularized_metrics, width, label='Regularized Model')
        
        plt.title('Performance Metrics Comparison', fontsize=14)
        plt.ylabel('Score', fontsize=12)
        plt.xticks(x, metrics)
        plt.grid(True, linestyle='--', alpha=0.6, axis='y')
        plt.legend(fontsize=12)
        
        plt.tight_layout()
        plt.savefig('regularization_comparison.png', dpi=300)
        plt.show()
        
        return model, scaler, list(df.columns)
    else:
        print(f"\nNo severe overfitting detected. Diagnosis: {learning_curve_data['diagnosis']}")
        print("Using original model without additional regularization.")
        
        # Load the original model
        model = load_model('valorant_model.h5')
        
        with open('feature_scaler.pkl', 'rb') as f:
            scaler = pickle.load(f)
        
        with open('feature_names.pkl', 'rb') as f:
            feature_names = pickle.load(f)
        
        return model, scaler, feature_names
Now, let's also add a feature selection component to reduce overfitting by focusing on the most informative features:

python
def select_optimal_features(X, y, test_size=0.2, random_state=42):
    """Perform feature selection to reduce overfitting and improve model performance."""
    print("\n" + "="*60)
    print("PERFORMING FEATURE SELECTION")
    print("="*60)
    
    # Convert data to DataFrame
    df = pd.DataFrame(X)
    
    # Fill missing values and handle non-numeric columns
    df = df.fillna(0)
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                df[col] = df[col].astype(float)
            except (ValueError, TypeError):
                df = df.drop(columns=[col])
    
    X_arr = df.values
    y_arr = np.array(y)
    
    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_arr)
    
    # Split data
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y_arr, test_size=test_size, random_state=random_state, stratify=y_arr
    )
    
    # Create baseline model
    input_dim = X_train.shape[1]
    baseline_model = create_deep_learning_model_with_economy(input_dim)
    
    # Evaluate importance of each feature using permutation importance
    from sklearn.inspection import permutation_importance
    from sklearn.ensemble import RandomForestClassifier
    
    print("Training a Random Forest model for feature importance analysis...")
    rf = RandomForestClassifier(n_estimators=100, random_state=random_state)
    rf.fit(X_train, y_train)
    
    # Compute feature importances
    importances = rf.feature_importances_
    feature_importances = list(zip(df.columns, importances))
    feature_importances.sort(key=lambda x: x[1], reverse=True)
    
    # Print top 20 important features
    print("\nTop 20 most important features:")
    for feature, importance in feature_importances[:20]:
        print(f"{feature}: {importance:.4f}")
    
    # Try different feature count thresholds
    feature_counts = [int(input_dim * ratio) for ratio in [0.25, 0.5, 0.75, 1.0]]
    feature_counts = sorted(list(set([min(count, input_dim) for count in feature_counts] + [20, 50, 100])))
    feature_counts = [count for count in feature_counts if count <= input_dim]
    
    print(f"\nTesting models with different feature counts: {feature_counts}")
    
    results = []
    for n_features in feature_counts:
        print(f"\nTesting with top {n_features} features...")
        
        # Select top features
        top_features = [feature for feature, _ in feature_importances[:n_features]]
        X_train_selected = X_train[:, [list(df.columns).index(feature) for feature in top_features]]
        X_val_selected = X_val[:, [list(df.columns).index(feature) for feature in top_features]]
        
        # Train model with selected features
        model = create_deep_learning_model_with_economy(X_train_selected.shape[1])
        
        early_stopping = EarlyStopping(
            monitor='val_loss', patience=10, restore_best_weights=True, verbose=0
        )
        
        history = model.fit(
            X_train_selected, y_train,
            epochs=50,
            batch_size=32,
            validation_data=(X_val_selected, y_val),
            callbacks=[early_stopping],
            verbose=0
        )
        
        # Evaluate model
        y_pred_proba = model.predict(X_val_selected)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        accuracy = accuracy_score(y_val, y_pred)
        auc = roc_auc_score(y_val, y_pred_proba)
        
        # Calculate overfitting
        train_acc = history.history['accuracy'][-1]
        val_acc = history.history['val_accuracy'][-1]
        acc_gap = train_acc - val_acc
        
        print(f"  Features: {n_features}, Val Acc: {val_acc:.4f}, Gap: {acc_gap:.4f}, AUC: {auc:.4f}")
        
        results.append({
            'n_features': n_features,
            'train_acc': train_acc,
            'val_acc': val_acc,
            'acc_gap': acc_gap,
            'auc': auc,
            'features': top_features
        })
    
    # Find optimal feature count
    df_results = pd.DataFrame(results)
    
    # Score based on validation accuracy with penalty for overfitting
    df_results['score'] = df_results['val_acc'] - 0.5 * df_results['acc_gap'] + 0.2 * df_results['auc']
    
    best_result = df_results.loc[df_results['score'].idxmax()]
    optimal_feature_count = best_result['n_features']
    optimal_features = best_result['features']
    
    print("\n" + "="*60)
    print(f"OPTIMAL FEATURE SET: {optimal_feature_count} features")
    print("="*60)
    print(f"Validation Accuracy: {best_result['val_acc']:.4f}")
    print(f"Train-Val Accuracy Gap: {best_result['acc_gap']:.4f}")
    print(f"AUC: {best_result['auc']:.4f}")
    
    # Plot results
    plt.figure(figsize=(15, 10))
    
    # Plot feature count vs metrics
    plt.subplot(2, 2, 1)
    plt.plot(df_results['n_features'], df_results['val_acc'], marker='o', label='Validation Accuracy')
    plt.plot(df_results['n_features'], df_results['train_acc'], marker='o', label='Training Accuracy')
    plt.axvline(x=optimal_feature_count, color='red', linestyle='--', label=f'Optimal Count: {optimal_feature_count}')
    plt.title('Accuracy vs Feature Count', fontsize=14)
    plt.xlabel('Number of Features', fontsize=12)
    plt.ylabel('Accuracy', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    # Plot feature count vs overfitting
    plt.subplot(2, 2, 2)
    plt.plot(df_results['n_features'], df_results['acc_gap'], marker='o', color='red')
    plt.axvline(x=optimal_feature_count, color='green', linestyle='--', label=f'Optimal Count: {optimal_feature_count}')
    plt.title('Overfitting vs Feature Count', fontsize=14)
    plt.xlabel('Number of Features', fontsize=12)
    plt.ylabel('Train-Val Accuracy Gap', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    # Plot feature count vs AUC
    plt.subplot(2, 2, 3)
    plt.plot(df_results['n_features'], df_results['auc'], marker='o', color='purple')
    plt.axvline(x=optimal_feature_count, color='green', linestyle='--', label=f'Optimal Count: {optimal_feature_count}')
    plt.title('AUC vs Feature Count', fontsize=14)
    plt.xlabel('Number of Features', fontsize=12)
    plt.ylabel('AUC', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    # Plot feature count vs score
    plt.subplot(2, 2, 4)
    plt.plot(df_results['n_features'], df_results['score'], marker='o', color='blue')
    plt.axvline(x=optimal_feature_count, color='green', linestyle='--', label=f'Optimal Count: {optimal_feature_count}')
    plt.title('Score vs Feature Count', fontsize=14)
    plt.xlabel('Number of Features', fontsize=12)
    plt.ylabel('Combined Score', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig('feature_selection_results.png', dpi=300)
    plt.show()
    
    # Categorize and analyze the selected features
    if optimal_features:
        print("\nAnalyzing selected features by category:")
        
        # Economy features
        economy_features = [f for f in optimal_features if any(term in f.lower() for term in 
                                                        ['eco', 'pistol', 'buy', 'economy'])]
        print(f"\nEconomy features ({len(economy_features)}/{len(economy_features)}): ")
        for feature in economy_features[:10]:  # Show first 10
            print(f"  - {feature}")
        
        # Player features
        player_features = [f for f in optimal_features if any(term in f.lower() for term in 
                                                      ['rating', 'acs', 'kd', 'adr', 'headshot',
                                                       'clutch', 'aces', 'first_blood'])]
        print(f"\nPlayer features ({len(player_features)}/{len(player_features)}): ")
        for feature in player_features[:10]:  # Show first 10
            print(f"  - {feature}")
        
        # Map features
        map_features = [f for f in optimal_features if 'map_' in f.lower()]
        print(f"\nMap features ({len(map_features)}/{len(map_features)}): ")
        for feature in map_features[:10]:  # Show first 10
            print(f"  - {feature}")
        
        # Calculate retention percentage by category
        all_economy_features = [f for f in df.columns if any(term in f.lower() for term in 
                                                  ['eco', 'pistol', 'buy', 'economy'])]
        all_player_features = [f for f in df.columns if any(term in f.lower() for term in 
                                                 ['rating', 'acs', 'kd', 'adr', 'headshot',
                                                  'clutch', 'aces', 'first_blood'])]
        all_map_features = [f for f in df.columns if 'map_' in f.lower()]
        
        economy_retention = len(economy_features) / max(len(all_economy_features), 1) * 100
        player_retention = len(player_features) / max(len(all_player_features), 1) * 100
        map_retention = len(map_features) / max(len(all_map_features), 1) * 100
        
        print("\nFeature retention by category:")
        print(f"  Economy features: {economy_retention:.1f}%")
        print(f"  Player features: {player_retention:.1f}%")
        print(f"  Map features: {map_retention:.1f}%")
        
        # Prepare selected feature dataset
        selected_features_indices = [list(df.columns).index(feature) for feature in optimal_features]
        X_selected = X_arr[:, selected_features_indices]
        
        # Scale selected features
        scaler_selected = StandardScaler()
        X_selected_scaled = scaler_selected.fit_transform(X_selected)
        
        return X_selected_scaled, optimal_features, scaler_selected
    
    return X_scaled, list(df.columns), scaler
Finally, let's integrate everything into a complete pipeline that combines learning curves, feature selection, and regularization optimization:

python
def optimize_model_pipeline(X, y, test_size=0.2, random_state=42):
    """Complete pipeline to optimize model with feature selection and regularization."""
    # Step 1: Perform feature selection
    print("\nStep 1: Performing feature selection to reduce overfitting...")
    X_selected, selected_features, scaler_selected = select_optimal_features(X, y, test_size, random_state)
    
    # Step 2: Diagnose overfitting with learning curves
    print("\nStep 2: Training model with selected features and diagnosing overfitting...")
    
    # Split data
    X_train, X_val, y_train, y_val = train_test_split(
        X_selected, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    # Train model with learning curves
    input_dim = X_train.shape[1]
    model = create_deep_learning_model_with_economy(input_dim)
    
    # Setup learning curve tracking
    training_losses = []
    validation_losses = []
    training_accuracies = []
    validation_accuracies = []
    epochs_completed = 0
    
    class LearningCurvesCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            nonlocal epochs_completed
            epochs_completed = epoch + 1
            
            training_losses.append(logs.get('loss'))
            validation_losses.append(logs.get('val_loss'))
            training_accuracies.append(logs.get('accuracy'))
            validation_accuracies.append(logs.get('val_accuracy'))
    
    learning_curves_callback = LearningCurvesCallback()
    
    # Define other callbacks
    early_stopping = EarlyStopping(
        monitor='val_loss', patience=15, restore_best_weights=True, verbose=1
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1
    )
    
    model_checkpoint = ModelCheckpoint(
        'best_valorant_model_selected_features.h5', 
        save_best_only=True, 
        monitor='val_accuracy'
    )
    
    # Train model
    history = model.fit(
        X_train, y_train,
        epochs=100,
        batch_size=32,
        validation_data=(X_val, y_val),
        callbacks=[early_stopping, reduce_lr, model_checkpoint, learning_curves_callback],
        verbose=1
    )
    
    # Check for overfitting
    train_acc = training_accuracies[-1]
    val_acc = validation_accuracies[-1]
    acc_gap = train_acc - val_acc
    
    if acc_gap > 0.08:  # Moderate to severe overfitting
        print(f"\nOverfitting detected after feature selection. Gap: {acc_gap:.4f}")
        print("\nStep 3: Finding optimal regularization parameters...")
        
        # Find optimal regularization parameters
        regularization_strengths = [0.0001, 0.0005, 0.001, 0.005, 0.01]
        dropout_rates = [0.3, 0.4, 0.5]
        
        # Store results
        reg_results = []
        
        # Try different combinations
        total_combos = len(regularization_strengths) * len(dropout_rates)
        combo_count = 0
        
        for reg_strength in regularization_strengths:
            for dropout_rate in dropout_rates:
                combo_count += 1
                print(f"\nTesting combination {combo_count}/{total_combos}: L2={reg_strength}, Dropout={dropout_rate}")
                
                # Create and train model
                model = create_deep_learning_model_with_economy_and_regularization(
                    input_dim, regularization_strength=reg_strength, dropout_rate=dropout_rate
                )
                
                # Train with early stopping
                early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)
                
                history = model.fit(
                    X_train, y_train,
                    epochs=30,  # Reduced epochs for faster testing
                    batch_size=32,
                    validation_data=(X_val, y_val),
                    callbacks=[early_stop],
                    verbose=0
                )
                
                # Evaluate results
                train_acc = history.history['accuracy'][-1]
                val_acc = history.history['val_accuracy'][-1]
                train_loss = history.history['loss'][-1]
                val_loss = history.history['val_loss'][-1]
                
                acc_gap = train_acc - val_acc
                
                reg_results.append({
                    'reg_strength': reg_strength,
                    'dropout_rate': dropout_rate,
                    'train_acc': train_acc,
                    'val_acc': val_acc,
                    'acc_gap': acc_gap
                })
                
                print(f"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Gap: {acc_gap:.4f}")
        
        # Find best parameters
        df_results = pd.DataFrame(reg_results)
        df_results['score'] = df_results['val_acc'] - 0.5 * df_results['acc_gap']
        best_params = df_results.loc[df_results['score'].idxmax()]
        
        best_reg_strength = best_params['reg_strength']
        best_dropout_rate = best_params['dropout_rate']
        
        print("\nBest regularization parameters found:")
        print(f"L2 Regularization Strength: {best_reg_strength}")
        print(f"Dropout Rate: {best_dropout_rate}")
        
        # Train final model with best parameters
        print("\nStep 4: Training final model with optimal feature set and regularization...")
        final_model = create_deep_learning_model_with_economy_and_regularization(
            input_dim, regularization_strength=best_reg_strength, dropout_rate=best_dropout_rate
        )
        
        final_history = final_model.fit(
            X_train, y_train,
            epochs=100,
            batch_size=32,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping, reduce_lr, model_checkpoint],
            verbose=1
        )
        
        # Save final model
        final_model.save('valorant_model_optimized.h5')
        
        # Save feature list and scaler
        with open('selected_features.pkl', 'wb') as f:
            pickle.dump(selected_features, f)
        
        with open('feature_scaler_optimized.pkl', 'wb') as f:
            pickle.dump(scaler_selected, f)
        
        # Evaluate final model
        y_pred_proba = final_model.predict(X_val)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred)
        recall = recall_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred)
        auc = roc_auc_score(y_val, y_pred_proba)
        
        print("\n" + "="*60)
        print("FINAL OPTIMIZED MODEL EVALUATION")
        print("="*60)
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(f"AUC: {auc:.4f}")
        
        final_train_acc = final_history.history['accuracy'][-1]
        final_val_acc = final_history.history['val_accuracy'][-1]
        final_acc_gap = final_train_acc - final_val_acc
        
        print(f"\nFinal Train-Val Accuracy Gap: {final_acc_gap:.4f}")
        print(f"Initial Gap: {acc_gap:.4f}")
        
        gap_reduction = 100 * (1 - final_acc_gap / acc_gap)
        print(f"Overfitting Reduction: {gap_reduction:.2f}%")
        
        return final_model, scaler_selected, selected_features
    else:
        print(f"\nNo significant overfitting detected after feature selection. Gap: {acc_gap:.4f}")
        print("Using model with selected features without additional regularization.")
        
        # Load the best model from checkpoint
        final_model = load_model('best_valorant_model_selected_features.h5')




Conitinue from #Load the best model from checkpoint

I do not need the beggining part of the code

Edit
Let me continue with the implementation of the model optimization pipeline, starting from the checkpoint loading:

python
        # Load the best model from checkpoint
        final_model = load_model('best_valorant_model_selected_features.h5')
        
        # Save feature list and scaler for future use
        with open('selected_features.pkl', 'wb') as f:
            pickle.dump(selected_features, f)
        
        with open('feature_scaler_optimized.pkl', 'wb') as f:
            pickle.dump(scaler_selected, f)
        
        # Evaluate final model
        y_pred_proba = final_model.predict(X_val)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred)
        recall = recall_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred)
        auc = roc_auc_score(y_val, y_pred_proba)
        
        print("\n" + "="*60)
        print("FEATURE-SELECTED MODEL EVALUATION")
        print("="*60)
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(f"AUC: {auc:.4f}")
        
        # Compare with original full feature set
        print("\nFeature Selection Summary:")
        print(f"Original feature count: {len(X[0]) if isinstance(X, list) else X.shape[1]}")
        print(f"Selected feature count: {len(selected_features)}")
        reduction_pct = (1 - len(selected_features) / (len(X[0]) if isinstance(X, list) else X.shape[1])) * 100
        print(f"Feature reduction: {reduction_pct:.1f}%")
        
        return final_model, scaler_selected, selected_features
Now let's create a function to integrate this pipeline into the predict_match function to ensure it uses the optimized model and features:

python
def predict_match_with_optimized_model(team1_name, team2_name, model=None, scaler=None, feature_names=None, export_data=True, display_details=True):
    """Predict match outcome using the optimized model with selected features and regularization."""
    print(f"Predicting match between {team1_name} and {team2_name} using optimized model...")
    
    # Get team IDs
    team1_id = get_team_id(team1_name)
    team2_id = get_team_id(team2_name)
    
    if not team1_id or not team2_id:
        print("Could not find one or both teams. Please check team names.")
        return None

    # Fetch team details to get team tags
    team1_details, team1_tag = fetch_team_details(team1_id)
    team2_details, team2_tag = fetch_team_details(team2_id)    
    print(f"Team tags: {team1_name} = {team1_tag}, {team2_name} = {team2_tag}")
    
    # Fetch match histories
    team1_history = fetch_team_match_history(team1_id)
    team2_history = fetch_team_match_history(team2_id)
    
    if not team1_history or not team2_history:
        print("Could not fetch match history for one or both teams.")
        return None
    
    # Parse match data
    team1_matches = parse_match_data(team1_history, team1_name)
    team2_matches = parse_match_data(team2_history, team2_name)

    # Store team tags for use in economy data matching
    for match in team1_matches:
        match['team_tag'] = team1_tag
    
    for match in team2_matches:
        match['team_tag'] = team2_tag
    
    # Fetch player stats for both teams
    team1_player_stats = fetch_team_player_stats(team1_id)
    team2_player_stats = fetch_team_player_stats(team2_id)

    # Calculate team stats with economy data
    team1_stats = calculate_team_stats_with_economy(team1_matches, team1_player_stats)
    team2_stats = calculate_team_stats_with_economy(team2_matches, team2_player_stats)
    
    # Store team tags in the stats
    team1_stats['team_tag'] = team1_tag
    team2_stats['team_tag'] = team2_tag
    
    # Extract additional metrics
    team1_map_performance = extract_map_performance(team1_matches)
    team2_map_performance = extract_map_performance(team2_matches)
    
    team1_tournament_performance = extract_tournament_performance(team1_matches)
    team2_tournament_performance = extract_tournament_performance(team2_matches)
    
    team1_performance_trends = analyze_performance_trends(team1_matches)
    team2_performance_trends = analyze_performance_trends(team2_matches)
    
    team1_opponent_quality = analyze_opponent_quality(team1_matches, team1_id)
    team2_opponent_quality = analyze_opponent_quality(team2_matches, team2_id)
    
    # Add derived metrics to team stats
    team1_stats['map_performance'] = team1_map_performance
    team2_stats['map_performance'] = team2_map_performance
    
    team1_stats['tournament_performance'] = team1_tournament_performance
    team2_stats['tournament_performance'] = team2_tournament_performance
    
    team1_stats['performance_trends'] = team1_performance_trends
    team2_stats['performance_trends'] = team2_performance_trends
    
    team1_stats['opponent_quality'] = team1_opponent_quality
    team2_stats['opponent_quality'] = team2_opponent_quality
    
    # Prepare data for model
    all_features = prepare_data_for_model_with_economy(team1_stats, team2_stats)
    
    if not all_features:
        print("Could not prepare features for prediction.")
        return None
    
    # Load optimized model if not provided
    if model is None:
        try:
            # Try to load optimized model first
            if os.path.exists('valorant_model_optimized.h5'):
                model = load_model('valorant_model_optimized.h5')
                
                with open('feature_scaler_optimized.pkl', 'rb') as f:
                    scaler = pickle.load(f)
                
                with open('selected_features.pkl', 'rb') as f:
                    feature_names = pickle.load(f)
                    
                print("Loaded optimized model with selected features.")
            else:
                # Fall back to regular model
                model = load_model('valorant_model.h5')
                
                with open('feature_scaler.pkl', 'rb') as f:
                    scaler = pickle.load(f)
                
                with open('feature_names.pkl', 'rb') as f:
                    feature_names = pickle.load(f)
                
                print("Loaded regular model (optimized model not found).")
        except Exception as e:
            print(f"Error loading model: {e}")
            print("Please train a model first or provide a trained model.")
            return None
    
    # Convert features to DataFrame
    features_df = pd.DataFrame([all_features])
    
    # Select only the features used in the optimized model
    if feature_names:
        # Add missing features with default values
        for feature in feature_names:
            if feature not in features_df.columns:
                features_df[feature] = 0
        
        # Keep only the selected features
        features_df = features_df[feature_names]
    
    # Scale features
    X = scaler.transform(features_df.values)
    
    # Make prediction
    prediction = model.predict(X)[0][0]
    
    # Calculate confidence
    confidence = max(prediction, 1 - prediction)
    
    result = {
        'team1': team1_name,
        'team2': team2_name,
        'team1_win_probability': float(prediction),
        'team2_win_probability': float(1 - prediction),
        'predicted_winner': team1_name if prediction > 0.5 else team2_name,
        'confidence': float(confidence),
        'team1_stats_summary': {
            'matches_played': team1_stats['matches'] if isinstance(team1_stats['matches'], int) else len(team1_stats['matches']),
            'win_rate': team1_stats['win_rate'],
            'recent_form': team1_stats['recent_form'],
            'avg_player_rating': team1_stats.get('avg_player_rating', 0),
            'star_player': team1_stats.get('player_stats', {}).get('star_player_name', ''),
            'star_player_rating': team1_stats.get('star_player_rating', 0),
            'pistol_win_rate': team1_stats.get('pistol_win_rate', 0),
            'eco_win_rate': team1_stats.get('eco_win_rate', 0),
            'full_buy_win_rate': team1_stats.get('full_buy_win_rate', 0),
            'economy_efficiency': team1_stats.get('economy_efficiency', 0)
        },
        'team2_stats_summary': {
            'matches_played': team2_stats['matches'] if isinstance(team2_stats['matches'], int) else len(team2_stats['matches']),
            'win_rate': team2_stats['win_rate'],
            'recent_form': team2_stats['recent_form'],
            'avg_player_rating': team2_stats.get('avg_player_rating', 0),
            'star_player': team2_stats.get('player_stats', {}).get('star_player_name', ''),
            'star_player_rating': team2_stats.get('star_player_rating', 0),
            'pistol_win_rate': team2_stats.get('pistol_win_rate', 0),
            'eco_win_rate': team2_stats.get('eco_win_rate', 0),
            'full_buy_win_rate': team2_stats.get('full_buy_win_rate', 0),
            'economy_efficiency': team2_stats.get('economy_efficiency', 0)
        },
        'model_info': {
            'features_used': len(feature_names) if feature_names else 'all',
            'feature_names': feature_names if feature_names else 'all',
            'model_type': 'optimized' if os.path.exists('valorant_model_optimized.h5') else 'regular'
        }
    }
    
    # Export prediction data if requested
    if export_data:
        export_prediction_data_with_economy(result, team1_stats, team2_stats)
    
    # Display detailed results if requested
    if display_details:
        display_prediction_results_with_economy(result, team1_stats, team2_stats)
        
        # Additionally display model information
        print("\n" + "="*60)
        print("MODEL INFORMATION")
        print("="*60)
        print(f"Model type: {result['model_info']['model_type']}")
        print(f"Features used: {result['model_info']['features_used']}")
        
        if feature_names and len(feature_names) <= 20:
            print("\nSelected features:")
            for feature in feature_names:
                print(f"  - {feature}")
        elif feature_names:
            print(f"\nUsing {len(feature_names)} selected features")
            
            # Count feature types
            economy_features = sum(1 for f in feature_names if any(term in f.lower() for term in 
                                                     ['eco', 'pistol', 'buy', 'economy']))
            player_features = sum(1 for f in feature_names if any(term in f.lower() for term in 
                                                    ['rating', 'acs', 'kd', 'adr', 'headshot',
                                                     'clutch', 'aces', 'first_blood']))
            map_features = sum(1 for f in feature_names if 'map_' in f.lower())
            
            print(f"  Economy features: {economy_features}")
            print(f"  Player features: {player_features}")
            print(f"  Map features: {map_features}")
    
    return result
Now, let's update the main function to include the option to run the complete optimization pipeline:

python
def main():
    """Main function to handle command line arguments and run the program."""
    parser = argparse.ArgumentParser(description="Valorant Match Predictor with Overfitting Detection and Optimization")
    
    # Add command line arguments
    parser.add_argument("--train", action="store_true", help="Train a new model")
    parser.add_argument("--optimize", action="store_true", help="Run complete model optimization pipeline")
    parser.add_argument("--predict", action="store_true", help="Predict a specific match")
    parser.add_argument("--team1", type=str, help="First team name")
    parser.add_argument("--team2", type=str, help="Second team name")
    parser.add_argument("--analyze", action="store_true", help="Analyze all upcoming matches")
    parser.add_argument("--test-teams", nargs='+', help="List of teams to use for testing")
    parser.add_argument("--backtest", action="store_true", help="Perform backtesting")
    parser.add_argument("--cutoff-date", type=str, help="Cutoff date for backtesting (YYYY/MM/DD)")
    parser.add_argument("--bet-amount", type=float, default=100, help="Bet amount for backtesting")
    parser.add_argument("--confidence", type=float, default=0.7, help="Confidence threshold for backtesting")
    parser.add_argument("--players", action="store_true", help="Include player stats in analysis")
    parser.add_argument("--economy", action="store_true", help="Include economy data in analysis")
    parser.add_argument("--learning-curves", action="store_true", help="Generate detailed learning curves")
    parser.add_argument("--verbose", action="store_true", help="Show detailed progress")

    args = parser.parse_args()
    
    if args.train:
        print("Training a new model with player statistics and economy data...")
        
        # [existing team data collection code]
        
        # Build training dataset with economy features
        print("\nBuilding training dataset with player and economy features...")
        X, y = build_training_dataset_with_economy(team_data_collection)
        
        print(f"Built training dataset with {len(X)} samples.")
        
        # Check if we have enough data to train
        if len(X) < 10:
            print("Not enough training data. Please collect more match data.")
            return
        
        if args.learning_curves:
            # Train model with detailed learning curves for overfitting detection
            print("\nTraining model with detailed learning curves for overfitting detection...")
            model, scaler, feature_names, learning_curve_data = train_model_with_learning_curves(X, y)
            
            # Display overfitting diagnosis
            print(f"\nLearning Curve Diagnosis: {learning_curve_data['diagnosis']}")
            print("\nRecommendations:")
            for rec in learning_curve_data['recommendations']:
                print(f"  {rec}")
        elif args.optimize:
            # Run complete model optimization pipeline
            print("\nRunning complete model optimization pipeline...")
            model, scaler, feature_names = optimize_model_pipeline(X, y)
        else:
            # Train regular model
            print("\nTraining standard model...")
            model, scaler, feature_names = train_model(X, y)
        
        print("Model training complete.")
    
    elif args.predict and args.team1 and args.team2:
        # Check if optimized model files exist
        optimized_model_exists = os.path.exists('valorant_model_optimized.h5')
        
        if optimized_model_exists:
            print(f"Predicting match between {args.team1} and {args.team2} using optimized model...")
            prediction = predict_match_with_optimized_model(args.team1, args.team2)
        else:
            print(f"Predicting match between {args.team1} and {args.team2} with standard model...")
            prediction = predict_match_with_economy(args.team1, args.team2)
        
        if prediction:
            visualize_prediction_with_economy(prediction)
        else:
            print(f"Could not generate prediction for {args.team1} vs {args.team2}")
    
    elif args.optimize and not args.train:
        # Run optimization on existing model data
        print("Running optimization on existing model data...")
        try:
            # Load existing training data
            with open('feature_names.pkl', 'rb') as f:
                feature_names = pickle.load(f)
                
            # Collect new data for optimization
            team_data_collection = collect_all_team_data(include_player_stats=True, include_economy=True, verbose=args.verbose)
            
            if team_data_collection:
                X, y = build_training_dataset_with_economy(team_data_collection)
                
                if len(X) > 10:
                    # Run optimization pipeline
                    model, scaler, optimized_features = optimize_model_pipeline(X, y)
                    print("Model optimization complete.")
                else:
                    print("Not enough training data for optimization.")
            else:
                print("Failed to collect team data for optimization.")
        except Exception as e:
            print(f"Error during optimization: {e}")
            print("Please train a model first before optimization.")
            
    elif args.learning_curves and not args.train:
        # Generate learning curves for existing model data
        print("Generating learning curves for existing model data...")
        try:
            # Load existing training data
            with open('feature_names.pkl', 'rb') as f:
                feature_names = pickle.load(f)
                
            # Collect new data for analysis
            team_data_collection = collect_all_team_data(include_player_stats=True, include_economy=True, verbose=args.verbose)
            
            if team_data_collection:
                X, y = build_training_dataset_with_economy(team_data_collection)
                
                if len(X) > 10:
                    # Run learning curve analysis
                    model, scaler, feature_names, learning_curve_data = train_model_with_learning_curves(X, y)
                    
                    # Display overfitting diagnosis
                    print(f"\nLearning Curve Diagnosis: {learning_curve_data['diagnosis']}")
                    print("\nRecommendations:")
                    for rec in learning_curve_data['recommendations']:
                        print(f"  {rec}")
                else:
                    print("Not enough training data for learning curve analysis.")
            else:
                print("Failed to collect team data for learning curve analysis.")
        except Exception as e:
            print(f"Error during learning curve analysis: {e}")
            print("Please train a model first before generating learning curves.")
    
    elif args.analyze:
        # Analyze upcoming matches using the best available model
        if os.path.exists('valorant_model_optimized.h5'):
            print("Analyzing upcoming matches with optimized model...")
            # [update analyze_upcoming_matches function to use optimized model]
        else:
            print("Analyzing upcoming matches with standard model...")
            analyze_upcoming_matches()
            
    elif args.backtest:
        if not args.cutoff_date:
            print("Please specify a cutoff date with --cutoff-date YYYY/MM/DD")
            return
        
        print(f"Performing backtesting with cutoff date: {args.cutoff_date}")
        print(f"Bet amount: ${args.bet_amount}, Confidence threshold: {args.confidence}")
        
        # Determine which model to use for backtesting
        if os.path.exists('valorant_model_optimized.h5'):
            print("Using optimized model for backtesting...")
            # [modify backtest_model to use optimized model]
        else:
            print("Using standard model for backtesting...")
            results = backtest_model(args.cutoff_date, 
                                    bet_amount=args.bet_amount, 
                                    confidence_threshold=args.confidence)
        
        if results:
            print("\nBacktesting Results:")
            print(f"Overall Accuracy: {results['overall_accuracy']:.4f}")
            print(f"High Confidence Accuracy: {results['high_conf_accuracy']:.4f}")
            print(f"ROI: {results['roi']:.4f}")
            print(f"Total profit: ${results['total_profit']:.2f}")
            print(f"Total bets: {results['total_bets']}")
            avg_profit = results['total_profit']/results['total_bets'] if results['total_bets'] > 0 else 0
            print(f"Average profit per bet: ${avg_profit:.2f}")
        else:
            print("Backtesting failed or returned no results.")
    
    else:
        print("Please specify an action: --train, --optimize, --learning-curves, --predict, --analyze, or --backtest")
        print("For predictions, specify --team1 and --team2")
        print("For backtesting, specify --cutoff-date YYYY/MM/DD")
Finally, let's add a function to visualize the learning curves and model performance after optimization:

python
def visualize_optimization_results(original_metrics, optimized_metrics):
    """Visualize the improvement from model optimization."""
    plt.figure(figsize=(15, 10))
    
    # Metrics comparison
    plt.subplot(2, 2, 1)
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']
    x = np.arange(len(metrics))
    width = 0.35
    
    plt.bar(x - width/2, original_metrics, width, label='Original Model')
    plt.bar(x + width/2, optimized_metrics, width, label='Optimized Model')
    
    plt.title('Performance Metrics Comparison', fontsize=14)
    plt.ylabel('Score', fontsize=12)
    plt.xticks(x, metrics)
    plt.grid(True, linestyle='--', alpha=0.6, axis='y')
    plt.legend(fontsize=12)
    
    # Calculate improvement percentages
    improvements = [(opt - orig) / orig * 100 for orig, opt in zip(original_metrics, optimized_metrics)]
    
    # Improvement visualization
    plt.subplot(2, 2, 2)
    colors = ['green' if imp > 0 else 'red' for imp in improvements]
    plt.bar(metrics, improvements, color=colors)
    plt.axhline(y=0, color='gray', linestyle='-')
    plt.title('Percentage Improvement in Metrics', fontsize=14)
    plt.ylabel('Improvement (%)', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6, axis='y')
    
    # Add improvement values
    for i, imp in enumerate(improvements):
        plt.text(i, imp + (1 if imp > 0 else -2), f"{imp:.1f}%", 
                 ha='center', va='bottom' if imp > 0 else 'top', fontsize=10)
    
    # Confusion matrix comparison
    plt.subplot(2, 2, 3)
    # [code to display confusion matrices - would need to be implemented]
    plt.title('Original Model Confusion Matrix', fontsize=14)
    
    plt.subplot(2, 2, 4)
    # [code to display confusion matrices - would need to be implemented]
    plt.title('Optimized Model Confusion Matrix', fontsize=14)
    
    plt.tight_layout()
    plt.savefig('optimization_results.png', dpi=300)
    plt.show()
With these implementations, you now have a comprehensive pipeline for detecting and mitigating overfitting in your Valorant match prediction model. The key components are:

Learning Curve Analysis: To diagnose if and how much your model is overfitting
Feature Selection: To reduce dimensionality and focus on the most informative features
Regularization Optimization: To find the optimal regularization parameters
Model Validation: To ensure the optimized model actually performs better
This approach should significantly improve your model's generalization capability and prediction accuracy on real-world matches.




Aweomse, now can you take me through each of the funcitons you added and if they replace a function or if there are helpers?
eg.

def this_function replaces _that funciton
def this_function2 does not replace any functions

Edit
I'll explain each function that was added to help address overfitting in your Valorant match prediction model, clarifying whether they replace existing functions or serve as helpers:

1. Learning Curve Analysis Functions
train_model_with_learning_curves(X, y, test_size=0.2, random_state=42)
Replaces: Partially replaces train_model()
Purpose: Enhanced version of your existing training function that adds detailed learning curve tracking and overfitting diagnostics
Key enhancements: Tracks training and validation metrics across epochs, diagnoses overfitting severity, and provides visual diagnostics and recommendations
LearningCurvesCallback (class within train_model_with_learning_curves)
Replaces: No existing function
Purpose: Helper class that tracks metrics during training for building learning curves
Usage: Monitors train/validation gaps and provides real-time overfitting diagnosis
2. Feature Selection Functions
select_optimal_features(X, y, test_size=0.2, random_state=42)
Replaces: No existing function
Purpose: New function that identifies the most predictive features and eliminates noise
Key functionality: Tests different feature subsets to find the optimal balance between accuracy and model complexity
3. Regularization Optimization Functions
create_deep_learning_model_with_economy_and_regularization(input_dim, regularization_strength=0.001, dropout_rate=0.4)
Replaces: Extends create_deep_learning_model_with_economy()
Purpose: Modified version of your model architecture with adjustable regularization parameters
Key changes: Adds configurable L2 regularization and dropout rates
find_optimal_regularization(X, y, test_size=0.2, random_state=42)
Replaces: No existing function
Purpose: Helper function that systematically tests different regularization parameters
Usage: Finds the best L2 regularization strength and dropout rate for your specific dataset
4. Integrated Optimization Pipeline
optimize_model_pipeline(X, y, test_size=0.2, random_state=42)
Replaces: No existing function
Purpose: Comprehensive pipeline that combines feature selection, learning curve analysis, and regularization optimization
Key functionality: Performs multi-stage optimization to systematically improve model generalization
train_model_with_overfitting_detection(X, y, test_size=0.2, random_state=42)
Replaces: No existing function
Purpose: Helper function that first diagnoses overfitting with learning curves, then applies appropriate regularization
Usage: Integrates learning curve diagnosis with targeted regularization improvements
5. Optimized Prediction Functions
predict_match_with_optimized_model(team1_name, team2_name, ...)
Replaces: Extends predict_match_with_economy()
Purpose: Enhances the prediction function to use the optimized model and selected features
Key changes: Automatically uses the optimized model if available, provides additional model information
visualize_optimization_results(original_metrics, optimized_metrics)
Replaces: No existing function
Purpose: Helper function that visualizes the improvements from optimization
Usage: Creates comparative visualizations of model performance before and after optimization
6. Updated Main Function
The updated main() function doesn't replace your existing one but extends it with new command-line arguments and functionality:

--optimize: Runs the complete optimization pipeline
--learning-curves: Generates detailed learning curves without full optimization
Enhanced handling of optimized models throughout prediction and backtesting
Implementation Strategy
To implement these improvements in your codebase:

Add the new functions as presented (they don't conflict with existing ones)
Replace the modified functions (like create_deep_learning_model_with_economy_and_regularization) or keep both and choose which to use based on context
Update your main() function to include the new command-line options
Test each component incrementally, starting with learning curves, then feature selection, and finally regularization optimization
These functions provide a systematic approach to detecting and addressing overfitting in your model, which should significantly improve its generalization performance and prediction accuracy.





#!/usr/bin/env python3 print("Starting Deep Learning Valorant Match Predictor...") import requests import json import os import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime import argparse import pickle import time import re from tqdm import tqdm #

pasted

Final feature matrix shape: (40, 278)
Class distribution: [16 16]
Traceback (most recent call last):
  File "/Users/ethan.harker/Desktop/code/Valorant-Machine-Learning/test1.py", line 5216, in <module>
    main()
  File "/Users/ethan.harker/Desktop/code/Valorant-Machine-Learning/test1.py", line 5075, in main
    model, scaler, feature_names, learning_curve_data = train_model_with_learning_curves(X, y)
  File "/Users/ethan.harker/Desktop/code/Valorant-Machine-Learning/test1.py", line 3230, in train_model_with_learning_curves
    model = create_deep_learning_model_with_economy(input_dim)
NameError: name 'create_deep_learning_model_with_economy' is not defined
ethan.harker@Ethans-MacBook-Air Valorant-Machine-Learning %

Edit
I see the issue in your code. The error is that the function create_deep_learning_model_with_economy is not defined anywhere in your script, but it's being called in the train_model_with_learning_curves function. Let me help you address this problem and provide some improvements for your Valorant match prediction model.

Missing Model Creation Function
First, let's define the missing function. You need to add this function to your code:

python
def create_deep_learning_model_with_economy(input_dim):
    """Create an enhanced deep learning model for match prediction with player stats and economy data."""
    # Define inputs
    inputs = Input(shape=(input_dim,))
    
    # First layer - shared feature processing
    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    
    # Second layer - deeper processing
    x = Dense(128, activation='relu', kernel_regularizer=l2(0.0005))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    # Player stats pathway with additional neurons
    x = Dense(96, activation='relu', kernel_regularizer=l2(0.0005))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.25)(x)
    
    # Economy-specific pathway with expanded capacity
    x = Dense(64, activation='relu', kernel_regularizer=l2(0.0005))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.25)(x)
    
    # Combined pathway
    x = Dense(32, activation='relu', kernel_regularizer=l2(0.0005))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    # Output
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Compile model
    model.compile(loss='binary_crossentropy', 
                 optimizer=Adam(learning_rate=0.0005),
                 metrics=['accuracy'])
    
    # Print model summary to see the expanded architecture
    print("\nModel Architecture:")
    model.summary()
    
    return model
Improving Model Performance
Beyond fixing the immediate error, here are some strategic improvements to address overfitting and enhance model performance:

1. Input Data & Feature Engineering
Your model is trying to work with 278 features but only 40 samples. This dimension mismatch is a classic cause of overfitting. Here are recommendations:

python
def reduce_dimensionality(X, y, method='pca', n_components=40):
    """Reduce feature dimensionality to avoid overfitting."""
    print(f"Reducing dimensions from {X.shape[1]} to {n_components} using {method}")
    
    if method == 'pca':
        from sklearn.decomposition import PCA
        reducer = PCA(n_components=n_components, random_state=42)
    elif method == 'feature_selection':
        from sklearn.feature_selection import SelectKBest, f_classif
        reducer = SelectKBest(f_classif, k=n_components)
    else:
        raise ValueError(f"Unknown dimensionality reduction method: {method}")
    
    X_reduced = reducer.fit_transform(X, y)
    print(f"Reduced feature matrix shape: {X_reduced.shape}")
    
    return X_reduced, reducer
2. Sample Generation
With only 40 samples, you need more training data. Add this after your SMOTE implementation:

python
def generate_additional_samples(X_train, y_train, multiplier=2):
    """Generate additional synthetic samples beyond SMOTE."""
    from sklearn.utils import resample
    
    # Only apply if we have fewer than 100 samples
    if X_train.shape[0] < 100:
        print(f"Generating additional samples (multiplier={multiplier})")
        n_samples = X_train.shape[0]
        n_new_samples = n_samples * (multiplier - 1)
        
        # Get indices for each class
        class_0_idx = np.where(y_train == 0)[0]
        class_1_idx = np.where(y_train == 1)[0]
        
        # Generate new samples for each class
        X_new = []
        y_new = []
        
        for class_idx, class_label in [(class_0_idx, 0), (class_1_idx, 1)]:
            X_class = X_train[class_idx]
            
            # Create bootstrap samples
            X_resampled = resample(X_class, 
                                  n_samples=int(n_new_samples/2),
                                  random_state=42)
            
            # Add small Gaussian noise to create new samples
            noise = np.random.normal(0, 0.01, X_resampled.shape)
            X_resampled = X_resampled + noise
            
            X_new.append(X_resampled)
            y_new.extend([class_label] * X_resampled.shape[0])
        
        # Combine original and new samples
        X_augmented = np.vstack([X_train, np.vstack(X_new)])
        y_augmented = np.concatenate([y_train, np.array(y_new)])
        
        print(f"Augmented dataset shape: {X_augmented.shape}")
        return X_augmented, y_augmented
    
    return X_train, y_train
3. Adaptive Regularization
Create a function to identify the optimal regularization strength based on the training/validation split:

python
def find_optimal_regularization(X_train, y_train, X_val, y_val):
    """Find the optimal regularization strength to reduce overfitting."""
    l2_values = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]
    dropout_values = [0.2, 0.3, 0.4, 0.5]
    
    best_val_acc = 0
    best_params = {'l2': 0.001, 'dropout': 0.4}
    
    print("Testing regularization parameters...")
    
    for l2_val in l2_values:
        for dropout_val in dropout_values:
            # Create a small model to test parameters
            input_dim = X_train.shape[1]
            inputs = Input(shape=(input_dim,))
            x = Dense(64, activation='relu', kernel_regularizer=l2(l2_val))(inputs)
            x = BatchNormalization()(x)
            x = Dropout(dropout_val)(x)
            x = Dense(32, activation='relu', kernel_regularizer=l2(l2_val))(x)
            x = BatchNormalization()(x)
            x = Dropout(dropout_val)(x)
            outputs = Dense(1, activation='sigmoid')(x)
            
            model = Model(inputs=inputs, outputs=outputs)
            model.compile(loss='binary_crossentropy', 
                         optimizer=Adam(learning_rate=0.001),
                         metrics=['accuracy'])
            
            # Train quickly
            history = model.fit(
                X_train, y_train,
                epochs=20,
                batch_size=16,
                validation_data=(X_val, y_val),
                verbose=0
            )
            
            # Check validation accuracy
            val_acc = max(history.history['val_accuracy'])
            train_acc = max(history.history['accuracy'])
            acc_gap = train_acc - val_acc
            
            # We want high validation accuracy with small gap
            score = val_acc - (0.5 * acc_gap)
            
            if score > best_val_acc:
                best_val_acc = score
                best_params = {'l2': l2_val, 'dropout': dropout_val}
                print(f"New best params: L2={l2_val}, Dropout={dropout_val}, Val Acc={val_acc:.4f}, Gap={acc_gap:.4f}")
    
    return best_params
4. Ensemble Modeling
Create an ensemble of models to reduce overfitting:

python
def create_model_ensemble(X_train, y_train, X_val, y_val, n_models=5):
    """Create an ensemble of models to improve prediction stability."""
    models = []
    
    for i in range(n_models):
        # Create model with slightly different architecture
        input_dim = X_train.shape[1]
        
        # Vary network architecture slightly
        hidden_units = np.random.choice([32, 64, 96, 128, 256])
        dropout_rate = np.random.uniform(0.2, 0.5)
        l2_reg = np.random.choice([0.0001, 0.0005, 0.001, 0.005, 0.01])
        
        # Create model
        model = create_deep_learning_model_with_economy_and_regularization(
            input_dim, regularization_strength=l2_reg, dropout_rate=dropout_rate
        )
        
        # Train with early stopping
        early_stopping = EarlyStopping(
            monitor='val_loss', patience=10, restore_best_weights=True
        )
        
        model.fit(
            X_train, y_train,
            epochs=50,
            batch_size=16,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping],
            verbose=0
        )
        
        # Evaluate and add to ensemble
        val_acc = model.evaluate(X_val, y_val, verbose=0)[1]
        print(f"Model {i+1}: Val Accuracy = {val_acc:.4f}")
        models.append(model)
    
    return models
5. Training Function with Advanced Techniques
Here's a modified training function that incorporates dimensionality reduction, data augmentation, and optimal regularization:

python
def train_advanced_model(X, y, test_size=0.2, random_state=42):
    """Train the model using advanced techniques to combat overfitting."""
    # Convert to numpy if needed
    if isinstance(X, list):
        X = np.array(X)
    if isinstance(y, list):
        y = np.array(y)
    
    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Reduce dimensionality if needed
    if X_scaled.shape[1] > X_scaled.shape[0] * 0.5:
        target_components = min(X_scaled.shape[0] // 2, 50)  # Rule of thumb
        X_reduced, reducer = reduce_dimensionality(X_scaled, y, n_components=target_components)
    else:
        X_reduced = X_scaled
        reducer = None
    
    # Split into train and validation sets
    X_train, X_val, y_train, y_val = train_test_split(
        X_reduced, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    # Apply SMOTE for class imbalance
    class_counts = np.bincount(y_train)
    if min(class_counts) / max(class_counts) < 0.8:  # Check for imbalance
        try:
            min_samples = min(class_counts)
            k_neighbors = min(5, min_samples-1)
            smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            print(f"Applied SMOTE: {np.bincount(y_train)}")
        except Exception as e:
            print(f"Error applying SMOTE: {e}")
    
    # Generate additional samples for small datasets
    X_train, y_train = generate_additional_samples(X_train, y_train, multiplier=3)
    
    # Find optimal regularization parameters
    best_params = find_optimal_regularization(X_train, y_train, X_val, y_val)
    
    # Train final model with optimal parameters
    input_dim = X_train.shape[1]
    model = create_deep_learning_model_with_economy_and_regularization(
        input_dim, 
        regularization_strength=best_params['l2'],
        dropout_rate=best_params['dropout']
    )
    
    # Define callbacks
    early_stopping = EarlyStopping(
        monitor='val_loss', patience=15, restore_best_weights=True, verbose=1
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1
    )
    
    # Train model
    history = model.fit(
        X_train, y_train,
        epochs=100,
        batch_size=min(32, len(X_train) // 4),  # Adjust batch size for small datasets
        validation_data=(X_val, y_val),
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    
    # Save preprocessing steps for later use
    preprocessing = {
        'scaler': scaler,
        'reducer': reducer,
        'feature_names': list(range(X_reduced.shape[1])) if reducer else None
    }
    
    return model, preprocessing, history
Additional Recommendations
Beyond these code changes, here are more recommendations to improve your model:

Feature Correlation Analysis: Many of your 278 features are likely highly correlated. Run a correlation analysis and remove features with correlation > 0.8.
Cross-Validation: With such a small dataset, use k-fold cross-validation (k=5 or k=10) instead of a single train/test split.
Bayesian Optimization: For hyperparameter tuning, consider Bayesian optimization instead of grid search to find optimal parameters more efficiently.
Gradient Accumulation: For small datasets, implement gradient accumulation to stabilize training.
Feature Importance Analysis: After training, analyze feature importance to understand which metrics (player stats, economy data, etc.) are most predictive.
Learning Rate Warmup: Implement learning rate warmup to help the model find better optima.
Loss Function: Consider using focal loss instead of binary cross-entropy to handle class imbalance better.
Data Quality: Review your data cleaning process to ensure there are no extreme outliers that could throw off the model.
By implementing these changes, you should see a substantial improvement in model performance and reduced overfitting, even with your limited dataset.





What does this tell you about the training? This was just a test dataset with 5 teams. I will do 30 teams next, then 100 teams.


Skipping match against PCIFIC Espor - no data available Skipping match against PCIFIC Espor - no data available Skipping match against PCIFIC Espor - no data available Skipping match against PCIFIC Espor - no data available Skipping match against PCIFIC Espor - no data available Skipping

pasted



recent_5_significant_diff: int64 recent_5_significant_advantage_team1: int64 recent_10_significant_diff: int64 recent_10_significant_advantage_team1: int64 recent_20_significant_diff: int64 recent_20_significant_advantage_team1: int64 momentum_5_vs_10_significant_diff: int64 momentum_5_vs_10_signifi

pasted


detailed_learning_curves.png


#!/usr/bin/env python3 print("Starting Deep Learning Valorant Match Predictor...") import requests import json import os import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime import argparse import pickle import time import re from tqdm import tqdm #


#!/usr/bin/env python3 print("Starting Deep Learning Valorant Match Predictor...") import requests import json import os import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime import argparse import pickle import time import re from tqdm import tqdm #



Pasted content

37.30 KB 758 lines

Formatting may be inconsistent from source

  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available

=================================================
TRAINING DATASET STATISTICS
=================================================
Created 40 training samples with:
  - Samples with economy data: 40
  - Samples with player stats: 40
  - Samples with both: 40
  - Teams with economy data: 5/5
  - Teams with player stats: 5/5

Feature breakdown:
  - Economy features: 52
  - Player features: 64
  - Map features: 1
  - Total features: 278

Sample economy features:
  - pistol_win_rate_diff
  - better_pistol_team1
  - avg_pistol_win_rate
  - eco_win_rate_diff
  - better_eco_team1

Sample player features:
  - player_rating_diff
  - better_player_rating_team1
  - avg_player_rating
  - acs_diff
  - better_acs_team1
Built training dataset with 40 samples.

Training model with detailed learning curves for overfitting detection...

Feature columns and their types:
win_rate_diff: float64
better_win_rate_team1: int64
recent_form_diff: float64
better_recent_form_team1: int64
score_diff_differential: float64
better_score_diff_team1: int64
total_matches: int64
match_count_diff: int64
avg_win_rate: float64
avg_recent_form: float64
wins_diff: int64
losses_diff: int64
win_loss_ratio_diff: float64
avg_score_diff: float64
better_avg_score_team1: int64
avg_score_metric: float64
avg_opponent_score_diff: float64
better_defense_team1: int64
avg_defense_metric: float64
recent_5_diff: float64
better_recent_5_team1: int64
avg_recent_5: float64
recent_10_diff: float64
better_recent_10_team1: int64
avg_recent_10: float64
recent_20_diff: float64
better_recent_20_team1: int64
avg_recent_20: float64
momentum_5_vs_10_diff: float64
better_momentum_5_vs_10_team1: int64
avg_momentum_5_vs_10: float64
momentum_10_vs_20_diff: float64
better_momentum_10_vs_20_team1: int64
avg_momentum_10_vs_20: float64
weighted_win_rate_diff: float64
better_weighted_win_rate_team1: int64
avg_weighted_win_rate: float64
player_rating_diff: float64
better_player_rating_team1: int64
avg_player_rating: float64
acs_diff: float64
better_acs_team1: int64
avg_acs: float64
kd_diff: float64
better_kd_team1: int64
avg_kd: float64
kast_diff: float64
better_kast_team1: int64
avg_kast: float64
adr_diff: float64
better_adr_team1: int64
avg_adr: float64
headshot_diff: float64
better_headshot_team1: int64
avg_headshot: float64
star_player_diff: float64
better_star_player_team1: int64
avg_star_player: float64
consistency_diff: float64
better_consistency_team1: int64
avg_consistency: float64
fk_fd_diff: float64
better_fk_fd_team1: int64
avg_fk_fd: float64
player_count_diff: int64
player_count_ratio: float64
avg_player_count: float64
pistol_win_rate_diff: float64
better_pistol_team1: int64
avg_pistol_win_rate: float64
eco_win_rate_diff: float64
better_eco_team1: int64
avg_eco_win_rate: float64
semi_eco_win_rate_diff: float64
better_semi_eco_team1: int64
avg_semi_eco_win_rate: float64
semi_buy_win_rate_diff: float64
better_semi_buy_team1: int64
avg_semi_buy_win_rate: float64
full_buy_win_rate_diff: float64
better_full_buy_team1: int64
avg_full_buy_win_rate: float64
low_economy_win_rate_diff: float64
better_low_economy_team1: int64
avg_low_economy_win_rate: float64
high_economy_win_rate_diff: float64
better_high_economy_team1: int64
avg_high_economy_win_rate: float64
economy_efficiency_diff: float64
better_economy_efficiency_team1: int64
avg_economy_efficiency: float64
pistol_confidence_diff: float64
better_pistol_confidence_team1: int64
avg_pistol_confidence: float64
pistol_sample_diff: int64
better_pistol_sample_team1: int64
avg_pistol_sample: float64
first_bloods_diff: float64
better_first_bloods_team1: int64
avg_first_bloods: float64
clutches_diff: float64
better_clutches_team1: int64
avg_clutches: float64
aces_diff: float64
better_aces_team1: int64
avg_aces: float64
entry_kills_diff: float64
better_entry_kills_team1: int64
avg_entry_kills: float64
first_kill_diff_differential: float64
better_first_kill_diff_team1: int64
avg_first_kill_diff: float64
headshot_percentage_diff: float64
better_headshot_percentage_team1: int64
avg_headshot_percentage: float64
kast_adv_diff: float64
better_kast_adv_team1: int64
avg_kast_adv: float64
adr_adv_diff: float64
better_adr_adv_team1: int64
avg_adr_adv: float64
agent_overlap: int64
common_map_count: int64
opponent_rank_diff: int64
better_opponent_quality_team1: int64
avg_opponent_rank: float64
opponent_rating_diff: int64
better_opponent_rating_team1: int64
avg_opponent_rating: float64
top_10_win_rate_diff: int64
better_top_10_team1: int64
avg_top_10_win_rate: float64
bottom_50_win_rate_diff: int64
better_bottom_50_team1: int64
avg_bottom_50_win_rate: float64
upset_factor_diff: int64
better_upset_team1: int64
avg_upset_factor: float64
upset_vulnerability_diff: int64
less_vulnerable_team1: int64
avg_upset_vulnerability: float64
team_rank_diff: int64
better_ranked_team1: int64
avg_team_rank: float64
rank_gap: int64
team_rating_quality_diff: float64
better_rated_quality_team1: int64
avg_team_rating_quality: float64
rating_gap: float64
mid_tier_tourney_diff: float64
better_mid_tier_team1: float64
avg_mid_tier_win_rate: float64
overall_tourney_diff: float64
better_tourney_team1: int64
avg_tourney_win_rate: float64
h2h_win_rate: float64
h2h_matches: int64
h2h_score_diff: float64
h2h_advantage_team1: int64
h2h_significant: int64
h2h_confidence: float64
rating_x_win_rate: float64
pistol_x_eco: float64
pistol_x_full_buy: float64
star_x_consistency: float64
h2h_x_form: float64
headshot_x_kd: float64
win_rate_x_opp_quality: float64
first_blood_x_win_rate: float64
clutch_x_consistency: float64
team1_better_count: int64
team1_better_ratio: float64
win_rate_significant_diff: int64
win_rate_significant_advantage_team1: int64
recent_form_significant_diff: int64
recent_form_significant_advantage_team1: int64
match_count_significant_diff: int64
match_count_significant_advantage_team1: int64
wins_significant_diff: int64
wins_significant_advantage_team1: int64
losses_significant_diff: int64
losses_significant_advantage_team1: int64
win_loss_ratio_significant_diff: int64
win_loss_ratio_significant_advantage_team1: int64
avg_score_significant_diff: int64
avg_score_significant_advantage_team1: int64
avg_opponent_score_significant_diff: int64
avg_opponent_score_significant_advantage_team1: int64
recent_5_significant_diff: int64
recent_5_significant_advantage_team1: int64
recent_10_significant_diff: int64
recent_10_significant_advantage_team1: int64
recent_20_significant_diff: int64
recent_20_significant_advantage_team1: int64
momentum_5_vs_10_significant_diff: int64
momentum_5_vs_10_significant_advantage_team1: int64
momentum_10_vs_20_significant_diff: int64
momentum_10_vs_20_significant_advantage_team1: int64
weighted_win_rate_significant_diff: int64
weighted_win_rate_significant_advantage_team1: int64
player_rating_significant_diff: int64
player_rating_significant_advantage_team1: int64
acs_significant_diff: int64
acs_significant_advantage_team1: int64
kd_significant_diff: int64
kd_significant_advantage_team1: int64
kast_significant_diff: int64
kast_significant_advantage_team1: int64
adr_significant_diff: int64
adr_significant_advantage_team1: int64
headshot_significant_diff: int64
headshot_significant_advantage_team1: int64
star_player_significant_diff: int64
star_player_significant_advantage_team1: int64
consistency_significant_diff: int64
consistency_significant_advantage_team1: int64
fk_fd_significant_diff: int64
fk_fd_significant_advantage_team1: int64
player_count_significant_diff: int64
player_count_significant_advantage_team1: int64
pistol_win_rate_significant_diff: int64
pistol_win_rate_significant_advantage_team1: int64
eco_win_rate_significant_diff: int64
eco_win_rate_significant_advantage_team1: int64
semi_eco_win_rate_significant_diff: int64
semi_eco_win_rate_significant_advantage_team1: int64
semi_buy_win_rate_significant_diff: int64
semi_buy_win_rate_significant_advantage_team1: int64
full_buy_win_rate_significant_diff: int64
full_buy_win_rate_significant_advantage_team1: int64
low_economy_win_rate_significant_diff: int64
low_economy_win_rate_significant_advantage_team1: int64
high_economy_win_rate_significant_diff: int64
high_economy_win_rate_significant_advantage_team1: int64
economy_efficiency_significant_diff: int64
economy_efficiency_significant_advantage_team1: int64
pistol_confidence_significant_diff: int64
pistol_confidence_significant_advantage_team1: int64
pistol_sample_significant_diff: int64
pistol_sample_significant_advantage_team1: int64
first_bloods_significant_diff: int64
first_bloods_significant_advantage_team1: int64
clutches_significant_diff: int64
clutches_significant_advantage_team1: int64
aces_significant_diff: int64
aces_significant_advantage_team1: int64
entry_kills_significant_diff: int64
entry_kills_significant_advantage_team1: int64
avg_first_kill_significant_diff: int64
avg_first_kill_significant_advantage_team1: int64
headshot_percentage_significant_diff: int64
headshot_percentage_significant_advantage_team1: int64
kast_adv_significant_diff: int64
kast_adv_significant_advantage_team1: int64
adr_adv_significant_diff: int64
adr_adv_significant_advantage_team1: int64
opponent_rank_significant_diff: int64
opponent_rank_significant_advantage_team1: int64
opponent_rating_significant_diff: int64
opponent_rating_significant_advantage_team1: int64
top_10_win_rate_significant_diff: int64
top_10_win_rate_significant_advantage_team1: int64
bottom_50_win_rate_significant_diff: int64
bottom_50_win_rate_significant_advantage_team1: int64
upset_factor_significant_diff: int64
upset_factor_significant_advantage_team1: int64
upset_vulnerability_significant_diff: int64
upset_vulnerability_significant_advantage_team1: int64
team_rank_significant_diff: int64
team_rank_significant_advantage_team1: int64
team_rating_quality_significant_diff: int64
team_rating_quality_significant_advantage_team1: int64
mid_tier_tourney_significant_diff: float64
mid_tier_tourney_significant_advantage_team1: float64
overall_tourney_significant_diff: int64
overall_tourney_significant_advantage_team1: int64
h2h_score_significant_diff: int64
h2h_score_significant_advantage_team1: int64

Final feature matrix shape: (40, 278)
Class distribution: [16 16]

Model Architecture:
Model: "functional"

 Layer (type)         Output Shape    Param # 

 input_layer          (None, 278)           0 
 (InputLayer)                                 

 dense (Dense)        (None, 256)      71,424 

 batch_normalization  (None, 256)       1,024 
 (BatchNormalizatio                          

 dropout (Dropout)    (None, 256)           0 

 dense_1 (Dense)      (None, 128)      32,896 

 batch_normalizatio  (None, 128)         512 
 (BatchNormalizatio                          

 dropout_1 (Dropout)  (None, 128)           0 

 dense_2 (Dense)      (None, 96)       12,384 

 batch_normalizatio  (None, 96)          384 
 (BatchNormalizatio                          

 dropout_2 (Dropout)  (None, 96)            0 

 dense_3 (Dense)      (None, 64)        6,208 

 batch_normalizatio  (None, 64)          256 
 (BatchNormalizatio                          

 dropout_3 (Dropout)  (None, 64)            0 

 dense_4 (Dense)      (None, 32)        2,080 

 batch_normalizatio  (None, 32)          128 
 (BatchNormalizatio                          

 dropout_4 (Dropout)  (None, 32)            0 

 dense_5 (Dense)      (None, 1)            33 

 Total params: 127,329 (497.38 KB)
 Trainable params: 126,177 (492.88 KB)
 Non-trainable params: 1,152 (4.50 KB)
Epoch 1/100
1/1  0s 8s/step - accuracy: 0.4062 - loss: 1.5894WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 

Epoch 1 Overfitting Status: Good Fit
Train-Val Accuracy Gap: -0.0938, Loss Gap: 0.4087
1/1  9s 9s/step - accuracy: 0.4062 - loss: 1.5894 - val_accuracy: 0.5000 - val_loss: 1.1807 - learning_rate: 5.0000e-04
Epoch 2/100
1/1  0s 52ms/step - accuracy: 0.4062 - loss: 1.5855
Epoch 2 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0312, Loss Gap: 0.4064
1/1  0s 350ms/step - accuracy: 0.4062 - loss: 1.5855 - val_accuracy: 0.3750 - val_loss: 1.1790 - learning_rate: 5.0000e-04
Epoch 3/100
1/1  0s 74ms/step - accuracy: 0.6250 - loss: 1.2191
Epoch 3 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: 0.0428
1/1  0s 350ms/step - accuracy: 0.6250 - loss: 1.2191 - val_accuracy: 0.3750 - val_loss: 1.1762 - learning_rate: 5.0000e-04
Epoch 4/100
1/1  0s 57ms/step - accuracy: 0.5000 - loss: 1.5661
Epoch 4 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.1250, Loss Gap: 0.3919
1/1  0s 384ms/step - accuracy: 0.5000 - loss: 1.5661 - val_accuracy: 0.3750 - val_loss: 1.1742 - learning_rate: 5.0000e-04
Epoch 5/100
1/1  0s 53ms/step - accuracy: 0.6875 - loss: 1.1591
Epoch 5 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.3125, Loss Gap: -0.0104
1/1  0s 379ms/step - accuracy: 0.6875 - loss: 1.1591 - val_accuracy: 0.3750 - val_loss: 1.1695 - learning_rate: 5.0000e-04
Epoch 6/100
1/1  0s 54ms/step - accuracy: 0.5312 - loss: 1.3373
Epoch 6 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0312, Loss Gap: 0.1723
1/1  0s 497ms/step - accuracy: 0.5312 - loss: 1.3373 - val_accuracy: 0.5000 - val_loss: 1.1651 - learning_rate: 5.0000e-04
Epoch 7/100
1/1  0s 40ms/step - accuracy: 0.5312 - loss: 1.1432
Epoch 7 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0312, Loss Gap: -0.0173
1/1  1s 509ms/step - accuracy: 0.5312 - loss: 1.1432 - val_accuracy: 0.5000 - val_loss: 1.1605 - learning_rate: 5.0000e-04
Epoch 8/100
1/1  0s 48ms/step - accuracy: 0.5938 - loss: 1.1520
Epoch 8 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.0938, Loss Gap: -0.0035
1/1  0s 332ms/step - accuracy: 0.5938 - loss: 1.1520 - val_accuracy: 0.5000 - val_loss: 1.1555 - learning_rate: 5.0000e-04
Epoch 9/100
1/1  0s 43ms/step - accuracy: 0.5000 - loss: 1.3099
Epoch 9 Overfitting Status: Good Fit
Train-Val Accuracy Gap: 0.0000, Loss Gap: 0.1584
1/1  0s 365ms/step - accuracy: 0.5000 - loss: 1.3099 - val_accuracy: 0.5000 - val_loss: 1.1516 - learning_rate: 5.0000e-04
Epoch 10/100
1/1  0s 59ms/step - accuracy: 0.6875 - loss: 1.0504
Epoch 10 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.0977
1/1  0s 479ms/step - accuracy: 0.6875 - loss: 1.0504 - val_accuracy: 0.5000 - val_loss: 1.1481 - learning_rate: 5.0000e-04
Epoch 11/100
1/1  0s 40ms/step - accuracy: 0.5938 - loss: 1.2408
Epoch 11 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.0938, Loss Gap: 0.0974
1/1  0s 341ms/step - accuracy: 0.5938 - loss: 1.2408 - val_accuracy: 0.5000 - val_loss: 1.1434 - learning_rate: 5.0000e-04
Epoch 12/100
1/1  0s 61ms/step - accuracy: 0.6875 - loss: 1.0089
Epoch 12 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.1299
1/1  0s 437ms/step - accuracy: 0.6875 - loss: 1.0089 - val_accuracy: 0.5000 - val_loss: 1.1389 - learning_rate: 5.0000e-04
Epoch 13/100
1/1  0s 45ms/step - accuracy: 0.5625 - loss: 1.1582
Epoch 13 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0625, Loss Gap: 0.0245
1/1  0s 286ms/step - accuracy: 0.5625 - loss: 1.1582 - val_accuracy: 0.5000 - val_loss: 1.1337 - learning_rate: 5.0000e-04
Epoch 14/100
1/1  0s 42ms/step - accuracy: 0.5938 - loss: 1.1326
Epoch 14 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.0938, Loss Gap: 0.0043
1/1  0s 309ms/step - accuracy: 0.5938 - loss: 1.1326 - val_accuracy: 0.5000 - val_loss: 1.1283 - learning_rate: 5.0000e-04
Epoch 15/100
1/1  0s 59ms/step - accuracy: 0.6875 - loss: 1.0172
Epoch 15 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.1059
1/1  0s 398ms/step - accuracy: 0.6875 - loss: 1.0172 - val_accuracy: 0.5000 - val_loss: 1.1231 - learning_rate: 5.0000e-04
Epoch 16/100
1/1  0s 62ms/step - accuracy: 0.7500 - loss: 0.9138
Epoch 16 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2041
1/1  1s 763ms/step - accuracy: 0.7500 - loss: 0.9138 - val_accuracy: 0.5000 - val_loss: 1.1179 - learning_rate: 5.0000e-04
Epoch 17/100
1/1  0s 72ms/step - accuracy: 0.6250 - loss: 1.0291
Epoch 17 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.1250, Loss Gap: -0.0838
1/1  1s 583ms/step - accuracy: 0.6250 - loss: 1.0291 - val_accuracy: 0.5000 - val_loss: 1.1129 - learning_rate: 5.0000e-04
Epoch 18/100
1/1  0s 55ms/step - accuracy: 0.7500 - loss: 0.9252
Epoch 18 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.1813
1/1  0s 315ms/step - accuracy: 0.7500 - loss: 0.9252 - val_accuracy: 0.5000 - val_loss: 1.1066 - learning_rate: 5.0000e-04
Epoch 19/100
1/1  0s 49ms/step - accuracy: 0.6250 - loss: 1.1217
Epoch 19 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.1250, Loss Gap: 0.0212
1/1  0s 370ms/step - accuracy: 0.6250 - loss: 1.1217 - val_accuracy: 0.5000 - val_loss: 1.1005 - learning_rate: 5.0000e-04
Epoch 20/100
1/1  0s 47ms/step - accuracy: 0.7188 - loss: 1.0576WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 

Epoch 20 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.0938, Loss Gap: -0.0377
1/1  0s 464ms/step - accuracy: 0.7188 - loss: 1.0576 - val_accuracy: 0.6250 - val_loss: 1.0953 - learning_rate: 5.0000e-04
Epoch 21/100
1/1  0s 48ms/step - accuracy: 0.6250 - loss: 1.1420
Epoch 21 Overfitting Status: Good Fit
Train-Val Accuracy Gap: 0.0000, Loss Gap: 0.0518
1/1  0s 261ms/step - accuracy: 0.6250 - loss: 1.1420 - val_accuracy: 0.6250 - val_loss: 1.0902 - learning_rate: 5.0000e-04
Epoch 22/100
1/1  0s 40ms/step - accuracy: 0.7812 - loss: 0.9062
Epoch 22 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1793
1/1  0s 306ms/step - accuracy: 0.7812 - loss: 0.9062 - val_accuracy: 0.6250 - val_loss: 1.0855 - learning_rate: 5.0000e-04
Epoch 23/100
1/1  0s 54ms/step - accuracy: 0.6875 - loss: 1.0635
Epoch 23 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0625, Loss Gap: -0.0174
1/1  0s 367ms/step - accuracy: 0.6875 - loss: 1.0635 - val_accuracy: 0.6250 - val_loss: 1.0809 - learning_rate: 5.0000e-04
Epoch 24/100
1/1  0s 67ms/step - accuracy: 0.7812 - loss: 0.9562
Epoch 24 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1201
1/1  0s 419ms/step - accuracy: 0.7812 - loss: 0.9562 - val_accuracy: 0.6250 - val_loss: 1.0764 - learning_rate: 5.0000e-04
Epoch 25/100
1/1  0s 44ms/step - accuracy: 0.6562 - loss: 1.0177
Epoch 25 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0312, Loss Gap: -0.0538
1/1  0s 303ms/step - accuracy: 0.6562 - loss: 1.0177 - val_accuracy: 0.6250 - val_loss: 1.0715 - learning_rate: 5.0000e-04
Epoch 26/100
1/1  0s 56ms/step - accuracy: 0.6875 - loss: 1.0352
Epoch 26 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0625, Loss Gap: -0.0313
1/1  0s 313ms/step - accuracy: 0.6875 - loss: 1.0352 - val_accuracy: 0.6250 - val_loss: 1.0666 - learning_rate: 5.0000e-04
Epoch 27/100
1/1  0s 61ms/step - accuracy: 0.7812 - loss: 0.9134
Epoch 27 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1492
1/1  0s 294ms/step - accuracy: 0.7812 - loss: 0.9134 - val_accuracy: 0.6250 - val_loss: 1.0627 - learning_rate: 5.0000e-04
Epoch 28/100
1/1  0s 41ms/step - accuracy: 0.7500 - loss: 0.9370
Epoch 28 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.1250, Loss Gap: -0.1219
1/1  0s 263ms/step - accuracy: 0.7500 - loss: 0.9370 - val_accuracy: 0.6250 - val_loss: 1.0588 - learning_rate: 5.0000e-04
Epoch 29/100
1/1  0s 38ms/step - accuracy: 0.7812 - loss: 0.9453
Epoch 29 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1097
1/1  0s 255ms/step - accuracy: 0.7812 - loss: 0.9453 - val_accuracy: 0.6250 - val_loss: 1.0550 - learning_rate: 5.0000e-04
Epoch 30/100
1/1  0s 41ms/step - accuracy: 0.6875 - loss: 1.0243
Epoch 30 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0625, Loss Gap: -0.0281
1/1  0s 253ms/step - accuracy: 0.6875 - loss: 1.0243 - val_accuracy: 0.6250 - val_loss: 1.0524 - learning_rate: 5.0000e-04
Epoch 31/100
1/1  0s 38ms/step - accuracy: 0.7812 - loss: 0.9662
Epoch 31 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.0839
1/1  0s 245ms/step - accuracy: 0.7812 - loss: 0.9662 - val_accuracy: 0.6250 - val_loss: 1.0501 - learning_rate: 5.0000e-04
Epoch 32/100
1/1  0s 40ms/step - accuracy: 0.8750 - loss: 0.7705
Epoch 32 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2773
1/1  0s 255ms/step - accuracy: 0.8750 - loss: 0.7705 - val_accuracy: 0.6250 - val_loss: 1.0477 - learning_rate: 5.0000e-04
Epoch 33/100
1/1  0s 40ms/step - accuracy: 0.7812 - loss: 0.8972
Epoch 33 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1482
1/1  0s 253ms/step - accuracy: 0.7812 - loss: 0.8972 - val_accuracy: 0.6250 - val_loss: 1.0454 - learning_rate: 5.0000e-04
Epoch 34/100
1/1  0s 40ms/step - accuracy: 0.8750 - loss: 0.7443
Epoch 34 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2984
1/1  0s 258ms/step - accuracy: 0.8750 - loss: 0.7443 - val_accuracy: 0.6250 - val_loss: 1.0427 - learning_rate: 5.0000e-04
Epoch 35/100
1/1  0s 45ms/step - accuracy: 0.8750 - loss: 0.7363
Epoch 35 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.3035
1/1  0s 320ms/step - accuracy: 0.8750 - loss: 0.7363 - val_accuracy: 0.6250 - val_loss: 1.0398 - learning_rate: 5.0000e-04
Epoch 36/100
1/1  0s 49ms/step - accuracy: 0.8438 - loss: 0.8262
Epoch 36 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2107
1/1  0s 305ms/step - accuracy: 0.8438 - loss: 0.8262 - val_accuracy: 0.6250 - val_loss: 1.0369 - learning_rate: 5.0000e-04
Epoch 37/100
1/1  0s 39ms/step - accuracy: 0.9062 - loss: 0.7903
Epoch 37 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2812, Loss Gap: -0.2439
1/1  0s 247ms/step - accuracy: 0.9062 - loss: 0.7903 - val_accuracy: 0.6250 - val_loss: 1.0341 - learning_rate: 5.0000e-04
Epoch 38/100
1/1  0s 47ms/step - accuracy: 0.7812 - loss: 0.9203
Epoch 38 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1113
1/1  0s 303ms/step - accuracy: 0.7812 - loss: 0.9203 - val_accuracy: 0.6250 - val_loss: 1.0316 - learning_rate: 5.0000e-04
Epoch 39/100
1/1  0s 41ms/step - accuracy: 0.8438 - loss: 0.7524
Epoch 39 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2770
1/1  0s 263ms/step - accuracy: 0.8438 - loss: 0.7524 - val_accuracy: 0.6250 - val_loss: 1.0294 - learning_rate: 5.0000e-04
Epoch 40/100
1/1  0s 41ms/step - accuracy: 0.7188 - loss: 0.9347
Epoch 40 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.0938, Loss Gap: -0.0922
1/1  0s 275ms/step - accuracy: 0.7188 - loss: 0.9347 - val_accuracy: 0.6250 - val_loss: 1.0269 - learning_rate: 5.0000e-04
Epoch 41/100
1/1  0s 52ms/step - accuracy: 0.8438 - loss: 0.7439
Epoch 41 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2808
1/1  0s 377ms/step - accuracy: 0.8438 - loss: 0.7439 - val_accuracy: 0.6250 - val_loss: 1.0247 - learning_rate: 5.0000e-04
Epoch 42/100
1/1  0s 47ms/step - accuracy: 0.7812 - loss: 0.8901
Epoch 42 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1327
1/1  0s 275ms/step - accuracy: 0.7812 - loss: 0.8901 - val_accuracy: 0.6250 - val_loss: 1.0228 - learning_rate: 5.0000e-04
Epoch 43/100
1/1  0s 40ms/step - accuracy: 0.8438 - loss: 0.7803
Epoch 43 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2404
1/1  0s 291ms/step - accuracy: 0.8438 - loss: 0.7803 - val_accuracy: 0.6250 - val_loss: 1.0207 - learning_rate: 5.0000e-04
Epoch 44/100
1/1  0s 49ms/step - accuracy: 0.8125 - loss: 0.9203
Epoch 44 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.0989
1/1  0s 357ms/step - accuracy: 0.8125 - loss: 0.9203 - val_accuracy: 0.6250 - val_loss: 1.0192 - learning_rate: 5.0000e-04
Epoch 45/100
1/1  0s 50ms/step - accuracy: 0.8438 - loss: 0.8052
Epoch 45 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2125
1/1  0s 369ms/step - accuracy: 0.8438 - loss: 0.8052 - val_accuracy: 0.6250 - val_loss: 1.0177 - learning_rate: 5.0000e-04
Epoch 46/100
1/1  0s 55ms/step - accuracy: 0.9062 - loss: 0.6801
Epoch 46 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2812, Loss Gap: -0.3361
1/1  0s 333ms/step - accuracy: 0.9062 - loss: 0.6801 - val_accuracy: 0.6250 - val_loss: 1.0162 - learning_rate: 5.0000e-04
Epoch 47/100
1/1  0s 52ms/step - accuracy: 0.8125 - loss: 1.0161
Epoch 47 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: 0.0009
1/1  0s 377ms/step - accuracy: 0.8125 - loss: 1.0161 - val_accuracy: 0.6250 - val_loss: 1.0152 - learning_rate: 5.0000e-04
Epoch 48/100
1/1  0s 44ms/step - accuracy: 0.9062 - loss: 0.7229
Epoch 48 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2812, Loss Gap: -0.2912
1/1  0s 306ms/step - accuracy: 0.9062 - loss: 0.7229 - val_accuracy: 0.6250 - val_loss: 1.0141 - learning_rate: 5.0000e-04
Epoch 49/100
1/1  0s 53ms/step - accuracy: 0.8750 - loss: 0.7442
Epoch 49 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2686
1/1  0s 391ms/step - accuracy: 0.8750 - loss: 0.7442 - val_accuracy: 0.6250 - val_loss: 1.0129 - learning_rate: 5.0000e-04
Epoch 50/100
1/1  0s 49ms/step - accuracy: 0.8125 - loss: 0.7415
Epoch 50 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.2704
1/1  0s 337ms/step - accuracy: 0.8125 - loss: 0.7415 - val_accuracy: 0.6250 - val_loss: 1.0119 - learning_rate: 5.0000e-04
Epoch 51/100
1/1  0s 51ms/step - accuracy: 0.9062 - loss: 0.6894
Epoch 51 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2812, Loss Gap: -0.3215
1/1  0s 373ms/step - accuracy: 0.9062 - loss: 0.6894 - val_accuracy: 0.6250 - val_loss: 1.0109 - learning_rate: 5.0000e-04
Epoch 52/100
1/1  0s 49ms/step - accuracy: 0.9062 - loss: 0.6651
Epoch 52 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2812, Loss Gap: -0.3445
1/1  0s 318ms/step - accuracy: 0.9062 - loss: 0.6651 - val_accuracy: 0.6250 - val_loss: 1.0096 - learning_rate: 5.0000e-04
Epoch 53/100
1/1  0s 54ms/step - accuracy: 0.7812 - loss: 0.7710
Epoch 53 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.2374
1/1  0s 397ms/step - accuracy: 0.7812 - loss: 0.7710 - val_accuracy: 0.6250 - val_loss: 1.0084 - learning_rate: 5.0000e-04
Epoch 54/100
1/1  0s 44ms/step - accuracy: 0.8750 - loss: 0.7300
Epoch 54 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2776
1/1  0s 265ms/step - accuracy: 0.8750 - loss: 0.7300 - val_accuracy: 0.6250 - val_loss: 1.0077 - learning_rate: 5.0000e-04
Epoch 55/100
1/1  0s 46ms/step - accuracy: 0.8125 - loss: 0.8208
Epoch 55 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.1860
1/1  0s 295ms/step - accuracy: 0.8125 - loss: 0.8208 - val_accuracy: 0.6250 - val_loss: 1.0068 - learning_rate: 5.0000e-04
Epoch 56/100
1/1  0s 41ms/step - accuracy: 0.7812 - loss: 0.8204
Epoch 56 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1856
1/1  0s 318ms/step - accuracy: 0.7812 - loss: 0.8204 - val_accuracy: 0.6250 - val_loss: 1.0060 - learning_rate: 5.0000e-04
Epoch 57/100
1/1  0s 54ms/step - accuracy: 0.8438 - loss: 0.7143
Epoch 57 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2910
1/1  0s 360ms/step - accuracy: 0.8438 - loss: 0.7143 - val_accuracy: 0.6250 - val_loss: 1.0053 - learning_rate: 5.0000e-04
Epoch 58/100
1/1  0s 39ms/step - accuracy: 0.8750 - loss: 0.7329
Epoch 58 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2719
1/1  0s 250ms/step - accuracy: 0.8750 - loss: 0.7329 - val_accuracy: 0.6250 - val_loss: 1.0048 - learning_rate: 5.0000e-04
Epoch 59/100
1/1  0s 42ms/step - accuracy: 0.9062 - loss: 0.6761
Epoch 59 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2812, Loss Gap: -0.3281
1/1  0s 255ms/step - accuracy: 0.9062 - loss: 0.6761 - val_accuracy: 0.6250 - val_loss: 1.0041 - learning_rate: 5.0000e-04
Epoch 60/100
1/1  0s 40ms/step - accuracy: 0.8750 - loss: 0.6431
Epoch 60 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.3605
1/1  0s 266ms/step - accuracy: 0.8750 - loss: 0.6431 - val_accuracy: 0.6250 - val_loss: 1.0036 - learning_rate: 5.0000e-04
Epoch 61/100
1/1  0s 50ms/step - accuracy: 0.8438 - loss: 0.6892
Epoch 61 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.3139
1/1  0s 399ms/step - accuracy: 0.8438 - loss: 0.6892 - val_accuracy: 0.6250 - val_loss: 1.0031 - learning_rate: 5.0000e-04
Epoch 62/100
1/1  0s 52ms/step - accuracy: 0.8750 - loss: 0.7260
Epoch 62 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2766
1/1  0s 370ms/step - accuracy: 0.8750 - loss: 0.7260 - val_accuracy: 0.6250 - val_loss: 1.0026 - learning_rate: 5.0000e-04
Epoch 63/100
1/1  0s 55ms/step - accuracy: 0.8438 - loss: 0.7495
Epoch 63 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2526
1/1  0s 382ms/step - accuracy: 0.8438 - loss: 0.7495 - val_accuracy: 0.6250 - val_loss: 1.0021 - learning_rate: 5.0000e-04
Epoch 64/100
1/1  0s 60ms/step - accuracy: 0.8125 - loss: 0.7971
Epoch 64 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.2046
1/1  0s 383ms/step - accuracy: 0.8125 - loss: 0.7971 - val_accuracy: 0.6250 - val_loss: 1.0017 - learning_rate: 5.0000e-04
Epoch 65/100
1/1  0s 61ms/step - accuracy: 0.8750 - loss: 0.7393
Epoch 65 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2615
1/1  0s 398ms/step - accuracy: 0.8750 - loss: 0.7393 - val_accuracy: 0.6250 - val_loss: 1.0008 - learning_rate: 5.0000e-04
Epoch 66/100
1/1  0s 63ms/step - accuracy: 0.8438 - loss: 0.7597
Epoch 66 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2404
1/1  0s 378ms/step - accuracy: 0.8438 - loss: 0.7597 - val_accuracy: 0.6250 - val_loss: 1.0000 - learning_rate: 5.0000e-04
Epoch 67/100
1/1  0s 58ms/step - accuracy: 0.8750 - loss: 0.7520
Epoch 67 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2473
1/1  0s 401ms/step - accuracy: 0.8750 - loss: 0.7520 - val_accuracy: 0.6250 - val_loss: 0.9993 - learning_rate: 5.0000e-04
Epoch 68/100
1/1  0s 205ms/step - accuracy: 0.8750 - loss: 0.8101
Epoch 68 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.1886
1/1  1s 553ms/step - accuracy: 0.8750 - loss: 0.8101 - val_accuracy: 0.6250 - val_loss: 0.9988 - learning_rate: 5.0000e-04
Epoch 69/100
1/1  0s 42ms/step - accuracy: 0.8438 - loss: 0.7390
Epoch 69 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2594
1/1  0s 339ms/step - accuracy: 0.8438 - loss: 0.7390 - val_accuracy: 0.6250 - val_loss: 0.9984 - learning_rate: 5.0000e-04
Epoch 70/100
1/1  0s 39ms/step - accuracy: 0.7812 - loss: 0.8582



#!/usr/bin/env python3 print("Starting Deep Learning Valorant Match Predictor...") import requests import json import os import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime import argparse import pickle import time import re from tqdm import tqdm #

pasted

you are a professional data scientist with machine laerning experience that is going to help me with my machine learning project.

Here is my script. as a professional data scientist, what are the suggestions you have to increase accuracy, validation accuracy, etc.

Edit
I've reviewed your Valorant match prediction script, and as a professional data scientist, I can offer several suggestions to improve model accuracy and validation performance. Your code already implements many good practices, but here are some areas for enhancement:

1. Data Processing Improvements
Feature Engineering
Temporal Features: Add more time-based features such as team momentum (recent performance trends weighted by recency), or seasonal effects if applicable
Feature Interactions: You're already creating some interaction terms, but consider more systematic interactions between key predictors
Non-linear Transformations: Apply transformations (log, square root, etc.) to features with skewed distributions
Anomaly Detection: Implement techniques to identify and handle outlier matches
Data Quality
Missing Value Strategy: Implement more sophisticated imputation methods (MICE, KNN imputation) rather than simple zero-filling
Data Leakage Prevention: Ensure strict temporal ordering when preparing training data to avoid including future information
Feature Selection: Use more rigorous feature selection techniques like recursive feature elimination (RFE) with cross-validation
2. Model Architecture Improvements
Neural Network Design
Architecture Search: Test different layer configurations systematically (number of layers, neurons per layer)
Residual Connections: Add residual connections between layers to help with gradient flow
Ensemble Approaches: Create an ensemble of models (e.g., combine neural networks with gradient boosting)
Hyperparameter Tuning: Use Bayesian optimization for hyperparameter tuning instead of manual selection
Training Process
Learning Rate Scheduling: Implement a more sophisticated learning rate schedule (cosine annealing, one-cycle policy)
Regularization: Test different combinations of L1/L2 penalties, dropout rates, and batch normalization
Balanced Batching: Ensure training batches have balanced class representation
Gradient Accumulation: For more stable updates, especially with imbalanced data
3. Validation Improvements
Cross-Validation Strategy
Time Series Cross-Validation: Implement temporal cross-validation instead of random splits
Group-Based CV: Ensure teams don't leak between training and validation sets
Multi-Metric Evaluation: Add calibration metrics (Brier score, log loss) in addition to accuracy
Monitoring & Diagnostics
Learning Curves: Implement detailed learning curves to detect overfitting earlier
Feature Importance Stability: Track feature importance across CV folds to ensure stability
Prediction Confidence Analysis: Analyze prediction confidence vs. accuracy relationship
4. Specific Technical Modifications
python
# Add a calibration layer to improve probability estimates
def create_deep_learning_model_with_economy(input_dim):
    # [existing model code]
    
    # Add temperature scaling for better calibration
    temperature = tf.Variable(1.0, trainable=True)
    outputs = Dense(1, activation='sigmoid')(x)
    calibrated_outputs = outputs / temperature
    
    # Create model with calibration
    model = Model(inputs=inputs, outputs=calibrated_outputs)
    
    # [rest of existing code]
python
# Implement a more robust SMOTE approach with combined resampling
def train_model(X, y, test_size=0.2, random_state=42):
    # [existing code]
    
    # More sophisticated handling of class imbalance
    if np.min(class_counts) / np.sum(class_counts) < 0.4:
        from imblearn.combine import SMOTETomek
        try:
            resampler = SMOTETomek(random_state=random_state)
            X_train, y_train = resampler.fit_resample(X_train, y_train)
            print(f"After SMOTETomek resampling: X_train shape: {X_train.shape}")
        except Exception as e:
            print(f"Error applying SMOTETomek: {e}")
    
    # [rest of existing code]
python
# Add focal loss to handle class imbalance better
def focal_loss(gamma=2.0, alpha=0.25):
    def focal_loss_fixed(y_true, y_pred):
        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))
        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))
        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))
    return focal_loss_fixed

# Then in your model compilation:
model.compile(loss=focal_loss(gamma=2.0, alpha=0.25), 
             optimizer=Adam(learning_rate=0.0005),
             metrics=['accuracy', AUC(name='auc')])
5. Advanced Techniques to Consider
Transfer Learning: Train base model on all matches, then fine-tune on specific tournaments/regions
Uncertainty Estimation: Implement Monte Carlo dropout or deep ensembles for uncertainty quantification
Adversarial Training: Add slight perturbations to input features during training for better robustness
Meta-Learning: Train models to predict which prediction models will perform best under different circumstances
6. Evaluation & Monitoring Improvements
Confusion Matrix Analysis: Analyze confusion matrices for specific team matchups
Performance by Context: Track model performance across different contexts (tournaments, regions, online vs. LAN)
Betting Simulation: Implement Kelly criterion or other sophisticated betting strategies in backtesting
Model Explainability: Add SHAP or LIME analysis to better understand individual predictions
These suggestions range from simple modifications to more advanced techniques. I recommend implementing them incrementally, measuring the impact on validation metrics after each change. The most effective approach will depend on your specific dataset characteristics and the main performance bottlenecks of your current implementation.




which of these improvements will give me likely the biggest percentage of improvements

Edit
Based on my review of your Valorant match prediction code, here are the improvements most likely to give you the biggest percentage gains in model performance, listed in order of expected impact:

Feature Engineering & Selection
Creating more powerful interaction terms between key predictors (particularly between player stats and economy features)
Implementing proper feature selection to remove noise and focus on the most predictive variables
This alone could yield a 5-10% improvement as your model seems to have many features that might be introducing noise
Improved Cross-Validation Strategy
Implementing temporal cross-validation instead of random splits
Ensuring no team data leaks between training and validation
This could give you a 3-7% more realistic assessment of model performance and improve generalization
Better Handling of Class Imbalance
Implementing SMOTETomek or other advanced resampling techniques
Adding a focal loss function to emphasize difficult examples
This could improve performance by 3-8%, especially for predicting upset victories
Model Calibration
Adding a calibration layer or using techniques like Platt scaling
Implementing temperature scaling to improve probability estimates
This could give you a 2-6% improvement in prediction confidence reliability
Learning Rate Scheduling
Implementing a more sophisticated learning rate schedule
Using techniques like cosine annealing with restarts
This could improve convergence by 2-5% and help escape local minima
These five improvements, when implemented systematically, are likely to give you the biggest return on investment. The single most impactful improvement would probably be better feature engineering and selection, as neural networks are very sensitive to the quality of input features, especially when dealing with relatively small datasets (which is common in esports prediction tasks).




I am having a problem with overfitting, so can we start with Learning Curves: Implement detailed learning curves to detect overfitting earlier

Edit
I'll focus on implementing learning curves to help you detect and address overfitting earlier in your Valorant match prediction model. Learning curves are an excellent diagnostic tool for understanding model performance during training.

Here's how we can implement detailed learning curves to monitor overfitting:

python
def train_model_with_learning_curves(X, y, test_size=0.2, random_state=42):
    """Train the deep learning model with detailed learning curves for early overfitting detection."""
    # Check if we have data
    if not X or len(X) == 0:
        print("Error: No training data available")
        return None, None, None, None
        
    # Convert feature dictionary to DataFrame and then to numpy array
    df = pd.DataFrame(X)
    
    # Fill NA values with 0
    df = df.fillna(0)
    
    # Print column info for debugging
    print("\nFeature columns and their types:")
    for col in df.columns:
        print(f"{col}: {df[col].dtype}")
        # Print a few examples if the type is object (non-numeric)
        if df[col].dtype == 'object':
            print(f"  Examples: {df[col].head(3).tolist()}")
            # Convert objects to numeric if possible, otherwise drop
            try:
                df[col] = df[col].astype(float)
                print(f"  Converted {col} to float")
            except (ValueError, TypeError):
                print(f"  Dropping column {col} due to non-numeric values")
                df = df.drop(columns=[col])
    
    # Check if DataFrame is empty after cleaning
    if df.empty:
        print("Error: Empty feature dataframe after cleaning")
        return None, None, None, None
    
    # Convert to numpy array
    X_arr = df.values
    y_arr = np.array(y)
    
    print(f"\nFinal feature matrix shape: {X_arr.shape}")
    
    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_arr)
    
    # Split data into training and validation sets
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y_arr, test_size=test_size, random_state=random_state, stratify=y_arr
    )
    
    # Further split training data to create a smaller training subset for learning curves
    X_train_subset, _, y_train_subset, _ = train_test_split(
        X_train, y_train, train_size=0.5, random_state=random_state, stratify=y_train
    )
    
    # Check for class imbalance
    class_counts = np.bincount(y_train)
    print(f"Class distribution: {class_counts}")
    
    # Handle class imbalance if necessary
    if np.min(class_counts) / np.sum(class_counts) < 0.4:  # If imbalanced
        print("Detected class imbalance, applying SMOTE...")
        try:
            if np.min(class_counts) < 5:
                print("Not enough samples in minority class for SMOTE. Using original data.")
            else:
                min_samples = np.min(class_counts)
                k_neighbors = min(5, min_samples-1)
                print(f"Using k_neighbors={k_neighbors} for SMOTE (min_samples={min_samples})")
                
                smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
                X_train, y_train = smote.fit_resample(X_train, y_train)
                X_train_subset, y_train_subset = smote.fit_resample(X_train_subset, y_train_subset)
                print(f"After SMOTE: X_train shape: {X_train.shape}, X_train_subset shape: {X_train_subset.shape}")
        except Exception as e:
            print(f"Error applying SMOTE: {e}")
            print("Continuing with original data.")
    
    # Create and train model
    input_dim = X_train.shape[1]
    model = create_deep_learning_model_with_economy(input_dim)
    
    # Define callbacks
    early_stopping = EarlyStopping(
        monitor='val_loss', patience=15, restore_best_weights=True, verbose=1
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1
    )
    
    model_checkpoint = ModelCheckpoint(
        'best_valorant_model.h5', 
        save_best_only=True, 
        monitor='val_accuracy'
    )
    
    # Lists to store metrics for learning curves
    training_losses = []
    validation_losses = []
    training_accuracies = []
    validation_accuracies = []
    subset_training_losses = []
    subset_training_accuracies = []
    epochs_completed = 0
    
    class LearningCurvesCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            nonlocal epochs_completed
            epochs_completed = epoch + 1
            
            # Store main metrics
            training_losses.append(logs.get('loss'))
            validation_losses.append(logs.get('val_loss'))
            training_accuracies.append(logs.get('accuracy'))
            validation_accuracies.append(logs.get('val_accuracy'))
            
            # Evaluate on training subset (to diagnose bias/variance)
            subset_metrics = self.model.evaluate(X_train_subset, y_train_subset, verbose=0)
            subset_training_losses.append(subset_metrics[0])
            subset_training_accuracies.append(subset_metrics[1])
            
            # Print overfitting diagnostic
            train_val_loss_gap = logs.get('loss') - logs.get('val_loss')
            train_val_acc_gap = logs.get('accuracy') - logs.get('val_accuracy')
            
            overfitting_status = "SEVERE OVERFITTING" if train_val_acc_gap > 0.15 else \
                               "Moderate Overfitting" if train_val_acc_gap > 0.08 else \
                               "Slight Overfitting" if train_val_acc_gap > 0.03 else \
                               "Good Fit"
            
            print(f"\nEpoch {epoch+1} Overfitting Status: {overfitting_status}")
            print(f"Train-Val Accuracy Gap: {train_val_acc_gap:.4f}, Loss Gap: {train_val_loss_gap:.4f}")
    
    learning_curves_callback = LearningCurvesCallback()
    
    # Train model
    history = model.fit(
        X_train, y_train,
        epochs=100,
        batch_size=32,
        validation_data=(X_val, y_val),
        callbacks=[early_stopping, reduce_lr, model_checkpoint, learning_curves_callback],
        verbose=1
    )
    
    # Plot detailed learning curves
    plt.figure(figsize=(20, 10))
    
    # Loss curves - Main plot
    plt.subplot(2, 2, 1)
    plt.plot(range(1, epochs_completed + 1), training_losses, label='Training Loss', marker='o')
    plt.plot(range(1, epochs_completed + 1), validation_losses, label='Validation Loss', marker='o')
    plt.title('Model Loss', fontsize=14)
    plt.ylabel('Loss', fontsize=12)
    plt.xlabel('Epoch', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    # Accuracy curves - Main plot
    plt.subplot(2, 2, 2)
    plt.plot(range(1, epochs_completed + 1), training_accuracies, label='Training Accuracy', marker='o')
    plt.plot(range(1, epochs_completed + 1), validation_accuracies, label='Validation Accuracy', marker='o')
    plt.title('Model Accuracy', fontsize=14)
    plt.ylabel('Accuracy', fontsize=12)
    plt.xlabel('Epoch', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    # Training Loss comparison (full vs subset)
    plt.subplot(2, 2, 3)
    plt.plot(range(1, epochs_completed + 1), training_losses, label='Full Training Set Loss', marker='o')
    plt.plot(range(1, epochs_completed + 1), subset_training_losses, label='Training Subset Loss', marker='o')
    plt.title('Training Loss: Full Set vs Subset', fontsize=14)
    plt.ylabel('Loss', fontsize=12)
    plt.xlabel('Epoch', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    # Plot accuracy gaps (to visualize overfitting)
    plt.subplot(2, 2, 4)
    acc_gaps = [train - val for train, val in zip(training_accuracies, validation_accuracies)]
    plt.plot(range(1, epochs_completed + 1), acc_gaps, label='Train-Validation Accuracy Gap', marker='o', color='red')
    plt.axhline(y=0.05, color='green', linestyle='--', label='Acceptable Gap Threshold')
    plt.axhline(y=0.10, color='orange', linestyle='--', label='Moderate Overfitting Threshold')
    plt.axhline(y=0.15, color='red', linestyle='--', label='Severe Overfitting Threshold')
    plt.title('Overfitting Analysis', fontsize=14)
    plt.ylabel('Accuracy Gap', fontsize=12)
    plt.xlabel('Epoch', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig('detailed_learning_curves.png', dpi=300)
    plt.show()
    
    # Create overfitting diagnosis report
    final_train_acc = training_accuracies[-1]
    final_val_acc = validation_accuracies[-1]
    final_acc_gap = final_train_acc - final_val_acc
    
    max_val_acc = max(validation_accuracies)
    max_val_acc_epoch = validation_accuracies.index(max_val_acc) + 1
    
    optimal_stopping_epoch = early_stopping.best_epoch + 1 if hasattr(early_stopping, 'best_epoch') else max_val_acc_epoch
    
    print("\n" + "="*60)
    print("OVERFITTING DIAGNOSIS REPORT")
    print("="*60)
    
    print(f"Final Training Accuracy: {final_train_acc:.4f}")
    print(f"Final Validation Accuracy: {final_val_acc:.4f}")
    print(f"Final Accuracy Gap: {final_acc_gap:.4f}")
    
    print(f"\nHighest Validation Accuracy: {max_val_acc:.4f} (Epoch {max_val_acc_epoch})")
    print(f"Optimal Stopping Epoch: {optimal_stopping_epoch}")
    
    if final_acc_gap > 0.15:
        overfitting_diagnosis = "SEVERE OVERFITTING"
        recommendations = [
            "1. Increase regularization (L1, L2, or both)",
            "2. Increase dropout rate",
            "3. Reduce model complexity (fewer layers/neurons)",
            "4. Apply early stopping at epoch " + str(optimal_stopping_epoch),
            "5. Collect more training data if possible",
            "6. Try feature selection to reduce dimensionality"
        ]
    elif final_acc_gap > 0.08:
        overfitting_diagnosis = "MODERATE OVERFITTING"
        recommendations = [
            "1. Slightly increase regularization",
            "2. Slightly increase dropout rate",
            "3. Apply early stopping at epoch " + str(optimal_stopping_epoch),
            "4. Consider adding more diverse data augmentation"
        ]
    elif final_acc_gap > 0.03:
        overfitting_diagnosis = "SLIGHT OVERFITTING"
        recommendations = [
            "1. Apply early stopping at epoch " + str(optimal_stopping_epoch),
            "2. Consider adding small amounts of regularization",
            "3. Feature selection might help"
        ]
    else:
        overfitting_diagnosis = "GOOD FIT"
        recommendations = [
            "1. Model is well-balanced",
            "2. Consider training longer if validation accuracy is still improving",
            "3. Try increasing model capacity slightly to improve performance"
        ]
    
    print(f"\nDiagnosis: {overfitting_diagnosis}")
    print("\nRecommendations:")
    for rec in recommendations:
        print(rec)
    
    print("\nAdvanced Analysis:")
    if max(acc_gaps) > 0.15 and max(validation_accuracies) < 0.65:
        print("- Model shows signs of SEVERE overfitting but low overall performance")
        print("- This suggests the model is memorizing noise in a difficult dataset")
        print("- Consider better feature engineering before increasing regularization")
    elif max_val_acc < 0.6:
        print("- Model isn't achieving high validation accuracy")
        print("- This suggests underfitting or poor feature quality")
        print("- Consider better feature engineering or a more complex model")
    
    # Analyze learning rate effects
    if hasattr(history.history, 'lr'):
        lr_changes = [i for i, (lr1, lr2) in enumerate(zip(history.history['lr'][:-1], history.history['lr'][1:])) if lr1 != lr2]
        if lr_changes:
            print("\nLearning Rate Effect Analysis:")
            for epoch in lr_changes:
                print(f"- Learning rate changed at epoch {epoch+1}")
                print(f"  Before change: Val Acc = {validation_accuracies[epoch]:.4f}, Val Loss = {validation_losses[epoch]:.4f}")
                print(f"  After change: Val Acc = {validation_accuracies[epoch+1]:.4f}, Val Loss = {validation_losses[epoch+1]:.4f}")
    
    # Save learning curve data for future analysis
    learning_curve_data = {
        'epochs': list(range(1, epochs_completed + 1)),
        'training_loss': training_losses,
        'validation_loss': validation_losses,
        'training_accuracy': training_accuracies,
        'validation_accuracy': validation_accuracies,
        'subset_training_loss': subset_training_losses,
        'subset_training_accuracy': subset_training_accuracies,
        'accuracy_gap': acc_gaps,
        'diagnosis': overfitting_diagnosis,
        'recommendations': recommendations
    }
    
    with open('learning_curve_data.pkl', 'wb') as f:
        pickle.dump(learning_curve_data, f)
    
    # Evaluate on test set
    y_pred_proba = model.predict(X_val)
    y_pred = (y_pred_proba > 0.5).astype(int).flatten()
    
    # Calculate metrics
    accuracy = accuracy_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)
    auc = roc_auc_score(y_val, y_pred_proba)
    
    print(f"\nModel Evaluation on Validation Set:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"AUC: {auc:.4f}")
    print("="*60)
    
    # Save model artifacts
    model.save('valorant_model.h5')
    
    # Save scaler for future use
    with open('feature_scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    
    # Save feature names
    with open('feature_names.pkl', 'wb') as f:
        pickle.dump(list(df.columns), f)
    
    return model, scaler, list(df.columns), learning_curve_data
Now, to implement regularization improvements based on the learning curves diagnosis, let's modify your model architecture:

python
def create_deep_learning_model_with_economy_and_regularization(input_dim, regularization_strength=0.001, dropout_rate=0.4):
    """Create an enhanced deep learning model with adjustable regularization parameters."""
    # Define inputs
    inputs = Input(shape=(input_dim,))
    
    # First layer - shared feature processing
    x = Dense(256, activation='relu', 
              kernel_regularizer=l2(regularization_strength),
              kernel_initializer='he_normal')(inputs)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate)(x)
    
    # Second layer - deeper processing
    x = Dense(128, activation='relu', 
              kernel_regularizer=l2(regularization_strength/2),
              kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate * 0.75)(x)
    
    # Player stats pathway with additional neurons
    x = Dense(96, activation='relu', 
              kernel_regularizer=l2(regularization_strength/4),
              kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate * 0.6)(x)
    
    # Economy-specific pathway with expanded capacity
    x = Dense(64, activation='relu', 
              kernel_regularizer=l2(regularization_strength/8),
              kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate * 0.5)(x)
    
    # Combined pathway
    x = Dense(32, activation='relu', 
              kernel_regularizer=l2(regularization_strength/16),
              kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate * 0.4)(x)
    
    # Output
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Compile model
    model.compile(loss='binary_crossentropy', 
                 optimizer=Adam(learning_rate=0.0005),
                 metrics=['accuracy'])
    
    # Print model summary to see the expanded architecture
    print("\nModel Architecture with Regularization:")
    model.summary()
    
    return model
To systematically test different regularization levels based on the learning curves, here's a function to find the optimal regularization parameters:

python
def find_optimal_regularization(X, y, test_size=0.2, random_state=42):
    """Find optimal regularization parameters to combat overfitting."""
    print("\n" + "="*60)
    print("FINDING OPTIMAL REGULARIZATION PARAMETERS")
    print("="*60)
    
    # Setup data same as in train_model_with_learning_curves
    df = pd.DataFrame(X).fillna(0)
    
    # Handle non-numeric columns
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                df[col] = df[col].astype(float)
            except (ValueError, TypeError):
                df = df.drop(columns=[col])
    
    if df.empty:
        print("Error: Empty feature dataframe after cleaning")
        return None
    
    # Convert to numpy array and scale
    X_arr = df.values
    y_arr = np.array(y)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_arr)
    
    # Split data
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y_arr, test_size=test_size, random_state=random_state, stratify=y_arr
    )
    
    # Handle class imbalance if needed
    class_counts = np.bincount(y_train)
    if np.min(class_counts) / np.sum(class_counts) < 0.4:
        try:
            if np.min(class_counts) >= 5:
                min_samples = np.min(class_counts)
                k_neighbors = min(5, min_samples-1)
                smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
                X_train, y_train = smote.fit_resample(X_train, y_train)
        except Exception as e:
            print(f"Error applying SMOTE: {e}")
    
    # Define parameter grid
    regularization_strengths = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]
    dropout_rates = [0.2, 0.3, 0.4, 0.5, 0.6]
    
    # Store results
    results = []
    
    # Setup early stopping
    early_stopping = EarlyStopping(
        monitor='val_loss', patience=10, restore_best_weights=True, verbose=0
    )
    
    # Try different combinations
    input_dim = X_train.shape[1]
    total_combos = len(regularization_strengths) * len(dropout_rates)
    combo_count = 0
    
    for reg_strength in regularization_strengths:
        for dropout_rate in dropout_rates:
            combo_count += 1
            print(f"\nTesting combination {combo_count}/{total_combos}: L2={reg_strength}, Dropout={dropout_rate}")
            
            # Create and train model
            model = create_deep_learning_model_with_economy_and_regularization(
                input_dim, regularization_strength=reg_strength, dropout_rate=dropout_rate
            )
            
            history = model.fit(
                X_train, y_train,
                epochs=30,  # Reduced epochs for faster testing
                batch_size=32,
                validation_data=(X_val, y_val),
                callbacks=[early_stopping],
                verbose=0
            )
            
            # Evaluate results
            train_acc = history.history['accuracy'][-1]
            val_acc = history.history['val_accuracy'][-1]
            train_loss = history.history['loss'][-1]
            val_loss = history.history['val_loss'][-1]
            
            acc_gap = train_acc - val_acc
            epochs_trained = len(history.history['accuracy'])
            
            results.append({
                'reg_strength': reg_strength,
                'dropout_rate': dropout_rate,
                'train_acc': train_acc,
                'val_acc': val_acc,
                'train_loss': train_loss,
                'val_loss': val_loss,
                'acc_gap': acc_gap,
                'epochs_trained': epochs_trained
            })
            
            print(f"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Gap: {acc_gap:.4f}, Epochs: {epochs_trained}")
    
    # Find best parameters
    # Sort by validation accuracy, with a penalty for large gaps
    df_results = pd.DataFrame(results)
    df_results['score'] = df_results['val_acc'] - 0.5 * df_results['acc_gap']
    df_results = df_results.sort_values('score', ascending=False)
    
    best_params = df_results.iloc[0]
    print("\nBest regularization parameters found:")
    print(f"L2 Regularization Strength: {best_params['reg_strength']}")
    print(f"Dropout Rate: {best_params['dropout_rate']}")
    print(f"Validation Accuracy: {best_params['val_acc']:.4f}")
    print(f"Train-Val Accuracy Gap: {best_params['acc_gap']:.4f}")
    
    # Visualize results
    plt.figure(figsize=(15, 10))
    
    # Plot the effect of regularization strength
    plt.subplot(2, 2, 1)
    reg_effect = df_results.groupby('reg_strength')[['val_acc', 'acc_gap']].mean()
    reg_effect.plot(kind='line', marker='o', ax=plt.gca())
    plt.title('Effect of L2 Regularization', fontsize=14)
    plt.xlabel('L2 Strength', fontsize=12)
    plt.ylabel('Metric Value', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    
    # Plot the effect of dropout rate
    plt.subplot(2, 2, 2)
    dropout_effect = df_results.groupby('dropout_rate')[['val_acc', 'acc_gap']].mean()
    dropout_effect.plot(kind='line', marker='o', ax=plt.gca())
    plt.title('Effect of Dropout Rate', fontsize=14)
    plt.xlabel('Dropout Rate', fontsize=12)
    plt.ylabel('Metric Value', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    
    # Plot validation accuracy heatmap
    plt.subplot(2, 2, 3)
    pivot_val_acc = pd.pivot_table(
        df_results, values='val_acc', index='dropout_rate', columns='reg_strength'
    )
    sns.heatmap(pivot_val_acc, annot=True, cmap='viridis', fmt='.3f')
    plt.title('Validation Accuracy by Regularization Parameters', fontsize=14)
    plt.xlabel('L2 Strength', fontsize=12)
    plt.ylabel('Dropout Rate', fontsize=12)
    
    # Plot accuracy gap heatmap
    plt.subplot(2, 2, 4)
    pivot_acc_gap = pd.pivot_table(
        df_results, values='acc_gap', index='dropout_rate', columns='reg_strength'
    )
    sns.heatmap(pivot_acc_gap, annot=True, cmap='coolwarm_r', fmt='.3f')
    plt.title('Accuracy Gap by Regularization Parameters', fontsize=14)
    plt.xlabel('L2 Strength', fontsize=12)
    plt.ylabel('Dropout Rate', fontsize=12)
    
    plt.tight_layout()
    plt.savefig('regularization_parameter_search.png', dpi=300)
    plt.show()
    
    # Return best parameters
    return best_params['reg_strength'], best_params['dropout_rate']
And here's an updated main function that uses these new functions to diagnose and fix overfitting:

python
def train_model_with_overfitting_detection(X, y, test_size=0.2, random_state=42):
    """Complete pipeline to train model with overfitting detection and mitigation."""
    # Step 1: Train with learning curves to diagnose overfitting
    print("\nStep 1: Training model with learning curves to diagnose overfitting...")
    _, _, _, learning_curve_data = train_model_with_learning_curves(X, y, test_size, random_state)
    
    # Check if severe overfitting was detected
    if learning_curve_data['diagnosis'] in ["SEVERE OVERFITTING", "MODERATE OVERFITTING"]:
        print(f"\nOverfitting detected: {learning_curve_data['diagnosis']}")
        
        # Step 2: Find optimal regularization parameters
        print("\nStep 2: Finding optimal regularization parameters...")
        best_reg_strength, best_dropout_rate = find_optimal_regularization(X, y, test_size, random_state)
        
        # Step 3: Retrain with optimal regularization
        print("\nStep 3: Retraining model with optimal regularization...")
        
        # Convert and prepare data
        df = pd.DataFrame(X).fillna(0)
        
        # Handle non-numeric columns
        for col in df.columns:
            if df[col].dtype == 'object':
                try:
                    df[col] = df[col].astype(float)
                except (ValueError, TypeError):
                    df = df.drop(columns=[col])
        
        X_arr = df.values
        y_arr = np.array(y)
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_arr)
        
        # Split data
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y_arr, test_size=test_size, random_state=random_state, stratify=y_arr
        )
        
        # Handle class imbalance if needed
        class_counts = np.bincount(y_train)
        if np.min(class_counts) / np.sum(class_counts) < 0.4:
            try:
                if np.min(class_counts) >= 5:
                    min_samples = np.min(class_counts)
                    k_neighbors = min(5, min_samples-1)
                    smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
                    X_train, y_train = smote.fit_resample(X_train, y_train)
            except Exception as e:
                print(f"Error applying SMOTE: {e}")
        
        # Train final model with optimal regularization
        input_dim = X_train.shape[1]
        model = create_deep_learning_model_with_economy_and_regularization(
            input_dim, regularization_strength=best_reg_strength, dropout_rate=best_dropout_rate
        )
        
        early_stopping = EarlyStopping(
            monitor='val_loss', patience=15, restore_best_weights=True, verbose=1
        )
        
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1
        )
        
        model_checkpoint = ModelCheckpoint(
            'best_valorant_model_final.h5', 
            save_best_only=True, 
            monitor='val_accuracy'
        )
        
        history = model.fit(
            X_train, y_train,
            epochs=100,
            batch_size=32,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping, reduce_lr, model_checkpoint],
            verbose=1
        )
        
        # Save model and artifacts
        model.save('valorant_model_regularized.h5')
        
        with open('feature_scaler.pkl', 'wb') as f:
            pickle.dump(scaler, f)
        
        with open('feature_names.pkl', 'wb') as f:
            pickle.dump(list(df.columns), f)
        
        # Evaluate final model
        y_pred_proba = model.predict(X_val)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred)
        recall = recall_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred)
        auc = r




Continue

Edit
Let me continue with the implementation of learning curves and overfitting detection for your Valorant match prediction model:

python
def train_model_with_overfitting_detection(X, y, test_size=0.2, random_state=42):
    """Complete pipeline to train model with overfitting detection and mitigation."""
    # Step 1: Train with learning curves to diagnose overfitting
    print("\nStep 1: Training model with learning curves to diagnose overfitting...")
    _, _, _, learning_curve_data = train_model_with_learning_curves(X, y, test_size, random_state)
    
    # Check if severe overfitting was detected
    if learning_curve_data['diagnosis'] in ["SEVERE OVERFITTING", "MODERATE OVERFITTING"]:
        print(f"\nOverfitting detected: {learning_curve_data['diagnosis']}")
        
        # Step 2: Find optimal regularization parameters
        print("\nStep 2: Finding optimal regularization parameters...")
        best_reg_strength, best_dropout_rate = find_optimal_regularization(X, y, test_size, random_state)
        
        # Step 3: Retrain with optimal regularization
        print("\nStep 3: Retraining model with optimal regularization...")
        
        # Convert and prepare data
        df = pd.DataFrame(X).fillna(0)
        
        # Handle non-numeric columns
        for col in df.columns:
            if df[col].dtype == 'object':
                try:
                    df[col] = df[col].astype(float)
                except (ValueError, TypeError):
                    df = df.drop(columns=[col])
        
        X_arr = df.values
        y_arr = np.array(y)
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_arr)
        
        # Split data
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y_arr, test_size=test_size, random_state=random_state, stratify=y_arr
        )
        
        # Handle class imbalance if needed
        class_counts = np.bincount(y_train)
        if np.min(class_counts) / np.sum(class_counts) < 0.4:
            try:
                if np.min(class_counts) >= 5:
                    min_samples = np.min(class_counts)
                    k_neighbors = min(5, min_samples-1)
                    smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
                    X_train, y_train = smote.fit_resample(X_train, y_train)
            except Exception as e:
                print(f"Error applying SMOTE: {e}")
        
        # Train final model with optimal regularization
        input_dim = X_train.shape[1]
        model = create_deep_learning_model_with_economy_and_regularization(
            input_dim, regularization_strength=best_reg_strength, dropout_rate=best_dropout_rate
        )
        
        early_stopping = EarlyStopping(
            monitor='val_loss', patience=15, restore_best_weights=True, verbose=1
        )
        
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1
        )
        
        model_checkpoint = ModelCheckpoint(
            'best_valorant_model_final.h5', 
            save_best_only=True, 
            monitor='val_accuracy'
        )
        
        history = model.fit(
            X_train, y_train,
            epochs=100,
            batch_size=32,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping, reduce_lr, model_checkpoint],
            verbose=1
        )
        
        # Save model and artifacts
        model.save('valorant_model_regularized.h5')
        
        with open('feature_scaler.pkl', 'wb') as f:
            pickle.dump(scaler, f)
        
        with open('feature_names.pkl', 'wb') as f:
            pickle.dump(list(df.columns), f)
        
        # Evaluate final model
        y_pred_proba = model.predict(X_val)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred)
        recall = recall_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred)
        auc = roc_auc_score(y_val, y_pred_proba)
        
        print("\n" + "="*60)
        print("FINAL MODEL EVALUATION (WITH REGULARIZATION)")
        print("="*60)
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(f"AUC: {auc:.4f}")
        
        # Compare with initial model
        train_acc = history.history['accuracy'][-1]
        val_acc = history.history['val_accuracy'][-1]
        final_acc_gap = train_acc - val_acc
        
        print(f"\nFinal Train-Val Accuracy Gap: {final_acc_gap:.4f}")
        print(f"Initial Accuracy Gap: {learning_curve_data['accuracy_gap'][-1]:.4f}")
        
        reduction = 100 * (1 - final_acc_gap / learning_curve_data['accuracy_gap'][-1])
        print(f"Overfitting Reduction: {reduction:.2f}%")
        
        # Plot final learning curves comparison
        plt.figure(figsize=(15, 10))
        
        # Compare original vs regularized validation accuracy
        plt.subplot(2, 2, 1)
        plt.plot(learning_curve_data['epochs'], learning_curve_data['validation_accuracy'], 
                 label='Original Model', marker='o')
        plt.plot(range(1, len(history.history['val_accuracy']) + 1), history.history['val_accuracy'], 
                 label='Regularized Model', marker='o')
        plt.title('Validation Accuracy Comparison', fontsize=14)
        plt.ylabel('Accuracy', fontsize=12)
        plt.xlabel('Epoch', fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.legend(fontsize=12)
        
        # Compare original vs regularized accuracy gaps
        plt.subplot(2, 2, 2)
        original_gaps = learning_curve_data['accuracy_gap']
        regularized_gaps = [train_acc - val_acc for train_acc, val_acc in 
                           zip(history.history['accuracy'], history.history['val_accuracy'])]
        
        plt.plot(learning_curve_data['epochs'], original_gaps, 
                 label='Original Model', marker='o')
        plt.plot(range(1, len(regularized_gaps) + 1), regularized_gaps, 
                 label='Regularized Model', marker='o')
        plt.axhline(y=0.05, color='green', linestyle='--', label='Acceptable Gap Threshold')
        plt.title('Overfitting Comparison', fontsize=14)
        plt.ylabel('Train-Val Accuracy Gap', fontsize=12)
        plt.xlabel('Epoch', fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.legend(fontsize=12)
        
        # Plot validation loss comparison
        plt.subplot(2, 2, 3)
        plt.plot(learning_curve_data['epochs'], learning_curve_data['validation_loss'], 
                 label='Original Model', marker='o')
        plt.plot(range(1, len(history.history['val_loss']) + 1), history.history['val_loss'], 
                 label='Regularized Model', marker='o')
        plt.title('Validation Loss Comparison', fontsize=14)
        plt.ylabel('Loss', fontsize=12)
        plt.xlabel('Epoch', fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.legend(fontsize=12)
        
        # Plot final metrics comparison
        plt.subplot(2, 2, 4)
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']
        original_metrics = [
            accuracy_score(y_val, (model.predict(X_val) > 0.5).astype(int).flatten()),
            precision_score(y_val, (model.predict(X_val) > 0.5).astype(int).flatten()),
            recall_score(y_val, (model.predict(X_val) > 0.5).astype(int).flatten()),
            f1_score(y_val, (model.predict(X_val) > 0.5).astype(int).flatten()),
            roc_auc_score(y_val, model.predict(X_val))
        ]
        
        regularized_metrics = [accuracy, precision, recall, f1, auc]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        plt.bar(x - width/2, original_metrics, width, label='Original Model')
        plt.bar(x + width/2, regularized_metrics, width, label='Regularized Model')
        
        plt.title('Performance Metrics Comparison', fontsize=14)
        plt.ylabel('Score', fontsize=12)
        plt.xticks(x, metrics)
        plt.grid(True, linestyle='--', alpha=0.6, axis='y')
        plt.legend(fontsize=12)
        
        plt.tight_layout()
        plt.savefig('regularization_comparison.png', dpi=300)
        plt.show()
        
        return model, scaler, list(df.columns)
    else:
        print(f"\nNo severe overfitting detected. Diagnosis: {learning_curve_data['diagnosis']}")
        print("Using original model without additional regularization.")
        
        # Load the original model
        model = load_model('valorant_model.h5')
        
        with open('feature_scaler.pkl', 'rb') as f:
            scaler = pickle.load(f)
        
        with open('feature_names.pkl', 'rb') as f:
            feature_names = pickle.load(f)
        
        return model, scaler, feature_names
Now, let's also add a feature selection component to reduce overfitting by focusing on the most informative features:

python
def select_optimal_features(X, y, test_size=0.2, random_state=42):
    """Perform feature selection to reduce overfitting and improve model performance."""
    print("\n" + "="*60)
    print("PERFORMING FEATURE SELECTION")
    print("="*60)
    
    # Convert data to DataFrame
    df = pd.DataFrame(X)
    
    # Fill missing values and handle non-numeric columns
    df = df.fillna(0)
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                df[col] = df[col].astype(float)
            except (ValueError, TypeError):
                df = df.drop(columns=[col])
    
    X_arr = df.values
    y_arr = np.array(y)
    
    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_arr)
    
    # Split data
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y_arr, test_size=test_size, random_state=random_state, stratify=y_arr
    )
    
    # Create baseline model
    input_dim = X_train.shape[1]
    baseline_model = create_deep_learning_model_with_economy(input_dim)
    
    # Evaluate importance of each feature using permutation importance
    from sklearn.inspection import permutation_importance
    from sklearn.ensemble import RandomForestClassifier
    
    print("Training a Random Forest model for feature importance analysis...")
    rf = RandomForestClassifier(n_estimators=100, random_state=random_state)
    rf.fit(X_train, y_train)
    
    # Compute feature importances
    importances = rf.feature_importances_
    feature_importances = list(zip(df.columns, importances))
    feature_importances.sort(key=lambda x: x[1], reverse=True)
    
    # Print top 20 important features
    print("\nTop 20 most important features:")
    for feature, importance in feature_importances[:20]:
        print(f"{feature}: {importance:.4f}")
    
    # Try different feature count thresholds
    feature_counts = [int(input_dim * ratio) for ratio in [0.25, 0.5, 0.75, 1.0]]
    feature_counts = sorted(list(set([min(count, input_dim) for count in feature_counts] + [20, 50, 100])))
    feature_counts = [count for count in feature_counts if count <= input_dim]
    
    print(f"\nTesting models with different feature counts: {feature_counts}")
    
    results = []
    for n_features in feature_counts:
        print(f"\nTesting with top {n_features} features...")
        
        # Select top features
        top_features = [feature for feature, _ in feature_importances[:n_features]]
        X_train_selected = X_train[:, [list(df.columns).index(feature) for feature in top_features]]
        X_val_selected = X_val[:, [list(df.columns).index(feature) for feature in top_features]]
        
        # Train model with selected features
        model = create_deep_learning_model_with_economy(X_train_selected.shape[1])
        
        early_stopping = EarlyStopping(
            monitor='val_loss', patience=10, restore_best_weights=True, verbose=0
        )
        
        history = model.fit(
            X_train_selected, y_train,
            epochs=50,
            batch_size=32,
            validation_data=(X_val_selected, y_val),
            callbacks=[early_stopping],
            verbose=0
        )
        
        # Evaluate model
        y_pred_proba = model.predict(X_val_selected)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        accuracy = accuracy_score(y_val, y_pred)
        auc = roc_auc_score(y_val, y_pred_proba)
        
        # Calculate overfitting
        train_acc = history.history['accuracy'][-1]
        val_acc = history.history['val_accuracy'][-1]
        acc_gap = train_acc - val_acc
        
        print(f"  Features: {n_features}, Val Acc: {val_acc:.4f}, Gap: {acc_gap:.4f}, AUC: {auc:.4f}")
        
        results.append({
            'n_features': n_features,
            'train_acc': train_acc,
            'val_acc': val_acc,
            'acc_gap': acc_gap,
            'auc': auc,
            'features': top_features
        })
    
    # Find optimal feature count
    df_results = pd.DataFrame(results)
    
    # Score based on validation accuracy with penalty for overfitting
    df_results['score'] = df_results['val_acc'] - 0.5 * df_results['acc_gap'] + 0.2 * df_results['auc']
    
    best_result = df_results.loc[df_results['score'].idxmax()]
    optimal_feature_count = best_result['n_features']
    optimal_features = best_result['features']
    
    print("\n" + "="*60)
    print(f"OPTIMAL FEATURE SET: {optimal_feature_count} features")
    print("="*60)
    print(f"Validation Accuracy: {best_result['val_acc']:.4f}")
    print(f"Train-Val Accuracy Gap: {best_result['acc_gap']:.4f}")
    print(f"AUC: {best_result['auc']:.4f}")
    
    # Plot results
    plt.figure(figsize=(15, 10))
    
    # Plot feature count vs metrics
    plt.subplot(2, 2, 1)
    plt.plot(df_results['n_features'], df_results['val_acc'], marker='o', label='Validation Accuracy')
    plt.plot(df_results['n_features'], df_results['train_acc'], marker='o', label='Training Accuracy')
    plt.axvline(x=optimal_feature_count, color='red', linestyle='--', label=f'Optimal Count: {optimal_feature_count}')
    plt.title('Accuracy vs Feature Count', fontsize=14)
    plt.xlabel('Number of Features', fontsize=12)
    plt.ylabel('Accuracy', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    # Plot feature count vs overfitting
    plt.subplot(2, 2, 2)
    plt.plot(df_results['n_features'], df_results['acc_gap'], marker='o', color='red')
    plt.axvline(x=optimal_feature_count, color='green', linestyle='--', label=f'Optimal Count: {optimal_feature_count}')
    plt.title('Overfitting vs Feature Count', fontsize=14)
    plt.xlabel('Number of Features', fontsize=12)
    plt.ylabel('Train-Val Accuracy Gap', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    # Plot feature count vs AUC
    plt.subplot(2, 2, 3)
    plt.plot(df_results['n_features'], df_results['auc'], marker='o', color='purple')
    plt.axvline(x=optimal_feature_count, color='green', linestyle='--', label=f'Optimal Count: {optimal_feature_count}')
    plt.title('AUC vs Feature Count', fontsize=14)
    plt.xlabel('Number of Features', fontsize=12)
    plt.ylabel('AUC', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    # Plot feature count vs score
    plt.subplot(2, 2, 4)
    plt.plot(df_results['n_features'], df_results['score'], marker='o', color='blue')
    plt.axvline(x=optimal_feature_count, color='green', linestyle='--', label=f'Optimal Count: {optimal_feature_count}')
    plt.title('Score vs Feature Count', fontsize=14)
    plt.xlabel('Number of Features', fontsize=12)
    plt.ylabel('Combined Score', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig('feature_selection_results.png', dpi=300)
    plt.show()
    
    # Categorize and analyze the selected features
    if optimal_features:
        print("\nAnalyzing selected features by category:")
        
        # Economy features
        economy_features = [f for f in optimal_features if any(term in f.lower() for term in 
                                                        ['eco', 'pistol', 'buy', 'economy'])]
        print(f"\nEconomy features ({len(economy_features)}/{len(economy_features)}): ")
        for feature in economy_features[:10]:  # Show first 10
            print(f"  - {feature}")
        
        # Player features
        player_features = [f for f in optimal_features if any(term in f.lower() for term in 
                                                      ['rating', 'acs', 'kd', 'adr', 'headshot',
                                                       'clutch', 'aces', 'first_blood'])]
        print(f"\nPlayer features ({len(player_features)}/{len(player_features)}): ")
        for feature in player_features[:10]:  # Show first 10
            print(f"  - {feature}")
        
        # Map features
        map_features = [f for f in optimal_features if 'map_' in f.lower()]
        print(f"\nMap features ({len(map_features)}/{len(map_features)}): ")
        for feature in map_features[:10]:  # Show first 10
            print(f"  - {feature}")
        
        # Calculate retention percentage by category
        all_economy_features = [f for f in df.columns if any(term in f.lower() for term in 
                                                  ['eco', 'pistol', 'buy', 'economy'])]
        all_player_features = [f for f in df.columns if any(term in f.lower() for term in 
                                                 ['rating', 'acs', 'kd', 'adr', 'headshot',
                                                  'clutch', 'aces', 'first_blood'])]
        all_map_features = [f for f in df.columns if 'map_' in f.lower()]
        
        economy_retention = len(economy_features) / max(len(all_economy_features), 1) * 100
        player_retention = len(player_features) / max(len(all_player_features), 1) * 100
        map_retention = len(map_features) / max(len(all_map_features), 1) * 100
        
        print("\nFeature retention by category:")
        print(f"  Economy features: {economy_retention:.1f}%")
        print(f"  Player features: {player_retention:.1f}%")
        print(f"  Map features: {map_retention:.1f}%")
        
        # Prepare selected feature dataset
        selected_features_indices = [list(df.columns).index(feature) for feature in optimal_features]
        X_selected = X_arr[:, selected_features_indices]
        
        # Scale selected features
        scaler_selected = StandardScaler()
        X_selected_scaled = scaler_selected.fit_transform(X_selected)
        
        return X_selected_scaled, optimal_features, scaler_selected
    
    return X_scaled, list(df.columns), scaler
Finally, let's integrate everything into a complete pipeline that combines learning curves, feature selection, and regularization optimization:

python
def optimize_model_pipeline(X, y, test_size=0.2, random_state=42):
    """Complete pipeline to optimize model with feature selection and regularization."""
    # Step 1: Perform feature selection
    print("\nStep 1: Performing feature selection to reduce overfitting...")
    X_selected, selected_features, scaler_selected = select_optimal_features(X, y, test_size, random_state)
    
    # Step 2: Diagnose overfitting with learning curves
    print("\nStep 2: Training model with selected features and diagnosing overfitting...")
    
    # Split data
    X_train, X_val, y_train, y_val = train_test_split(
        X_selected, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    # Train model with learning curves
    input_dim = X_train.shape[1]
    model = create_deep_learning_model_with_economy(input_dim)
    
    # Setup learning curve tracking
    training_losses = []
    validation_losses = []
    training_accuracies = []
    validation_accuracies = []
    epochs_completed = 0
    
    class LearningCurvesCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            nonlocal epochs_completed
            epochs_completed = epoch + 1
            
            training_losses.append(logs.get('loss'))
            validation_losses.append(logs.get('val_loss'))
            training_accuracies.append(logs.get('accuracy'))
            validation_accuracies.append(logs.get('val_accuracy'))
    
    learning_curves_callback = LearningCurvesCallback()
    
    # Define other callbacks
    early_stopping = EarlyStopping(
        monitor='val_loss', patience=15, restore_best_weights=True, verbose=1
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1
    )
    
    model_checkpoint = ModelCheckpoint(
        'best_valorant_model_selected_features.h5', 
        save_best_only=True, 
        monitor='val_accuracy'
    )
    
    # Train model
    history = model.fit(
        X_train, y_train,
        epochs=100,
        batch_size=32,
        validation_data=(X_val, y_val),
        callbacks=[early_stopping, reduce_lr, model_checkpoint, learning_curves_callback],
        verbose=1
    )
    
    # Check for overfitting
    train_acc = training_accuracies[-1]
    val_acc = validation_accuracies[-1]
    acc_gap = train_acc - val_acc
    
    if acc_gap > 0.08:  # Moderate to severe overfitting
        print(f"\nOverfitting detected after feature selection. Gap: {acc_gap:.4f}")
        print("\nStep 3: Finding optimal regularization parameters...")
        
        # Find optimal regularization parameters
        regularization_strengths = [0.0001, 0.0005, 0.001, 0.005, 0.01]
        dropout_rates = [0.3, 0.4, 0.5]
        
        # Store results
        reg_results = []
        
        # Try different combinations
        total_combos = len(regularization_strengths) * len(dropout_rates)
        combo_count = 0
        
        for reg_strength in regularization_strengths:
            for dropout_rate in dropout_rates:
                combo_count += 1
                print(f"\nTesting combination {combo_count}/{total_combos}: L2={reg_strength}, Dropout={dropout_rate}")
                
                # Create and train model
                model = create_deep_learning_model_with_economy_and_regularization(
                    input_dim, regularization_strength=reg_strength, dropout_rate=dropout_rate
                )
                
                # Train with early stopping
                early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)
                
                history = model.fit(
                    X_train, y_train,
                    epochs=30,  # Reduced epochs for faster testing
                    batch_size=32,
                    validation_data=(X_val, y_val),
                    callbacks=[early_stop],
                    verbose=0
                )
                
                # Evaluate results
                train_acc = history.history['accuracy'][-1]
                val_acc = history.history['val_accuracy'][-1]
                train_loss = history.history['loss'][-1]
                val_loss = history.history['val_loss'][-1]
                
                acc_gap = train_acc - val_acc
                
                reg_results.append({
                    'reg_strength': reg_strength,
                    'dropout_rate': dropout_rate,
                    'train_acc': train_acc,
                    'val_acc': val_acc,
                    'acc_gap': acc_gap
                })
                
                print(f"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Gap: {acc_gap:.4f}")
        
        # Find best parameters
        df_results = pd.DataFrame(reg_results)
        df_results['score'] = df_results['val_acc'] - 0.5 * df_results['acc_gap']
        best_params = df_results.loc[df_results['score'].idxmax()]
        
        best_reg_strength = best_params['reg_strength']
        best_dropout_rate = best_params['dropout_rate']
        
        print("\nBest regularization parameters found:")
        print(f"L2 Regularization Strength: {best_reg_strength}")
        print(f"Dropout Rate: {best_dropout_rate}")
        
        # Train final model with best parameters
        print("\nStep 4: Training final model with optimal feature set and regularization...")
        final_model = create_deep_learning_model_with_economy_and_regularization(
            input_dim, regularization_strength=best_reg_strength, dropout_rate=best_dropout_rate
        )
        
        final_history = final_model.fit(
            X_train, y_train,
            epochs=100,
            batch_size=32,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping, reduce_lr, model_checkpoint],
            verbose=1
        )
        
        # Save final model
        final_model.save('valorant_model_optimized.h5')
        
        # Save feature list and scaler
        with open('selected_features.pkl', 'wb') as f:
            pickle.dump(selected_features, f)
        
        with open('feature_scaler_optimized.pkl', 'wb') as f:
            pickle.dump(scaler_selected, f)
        
        # Evaluate final model
        y_pred_proba = final_model.predict(X_val)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred)
        recall = recall_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred)
        auc = roc_auc_score(y_val, y_pred_proba)
        
        print("\n" + "="*60)
        print("FINAL OPTIMIZED MODEL EVALUATION")
        print("="*60)
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(f"AUC: {auc:.4f}")
        
        final_train_acc = final_history.history['accuracy'][-1]
        final_val_acc = final_history.history['val_accuracy'][-1]
        final_acc_gap = final_train_acc - final_val_acc
        
        print(f"\nFinal Train-Val Accuracy Gap: {final_acc_gap:.4f}")
        print(f"Initial Gap: {acc_gap:.4f}")
        
        gap_reduction = 100 * (1 - final_acc_gap / acc_gap)
        print(f"Overfitting Reduction: {gap_reduction:.2f}%")
        
        return final_model, scaler_selected, selected_features
    else:
        print(f"\nNo significant overfitting detected after feature selection. Gap: {acc_gap:.4f}")
        print("Using model with selected features without additional regularization.")
        
        # Load the best model from checkpoint
        final_model = load_model('best_valorant_model_selected_features.h5')




Conitinue from #Load the best model from checkpoint

I do not need the beggining part of the code

Edit
Let me continue with the implementation of the model optimization pipeline, starting from the checkpoint loading:

python
        # Load the best model from checkpoint
        final_model = load_model('best_valorant_model_selected_features.h5')
        
        # Save feature list and scaler for future use
        with open('selected_features.pkl', 'wb') as f:
            pickle.dump(selected_features, f)
        
        with open('feature_scaler_optimized.pkl', 'wb') as f:
            pickle.dump(scaler_selected, f)
        
        # Evaluate final model
        y_pred_proba = final_model.predict(X_val)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred)
        recall = recall_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred)
        auc = roc_auc_score(y_val, y_pred_proba)
        
        print("\n" + "="*60)
        print("FEATURE-SELECTED MODEL EVALUATION")
        print("="*60)
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(f"AUC: {auc:.4f}")
        
        # Compare with original full feature set
        print("\nFeature Selection Summary:")
        print(f"Original feature count: {len(X[0]) if isinstance(X, list) else X.shape[1]}")
        print(f"Selected feature count: {len(selected_features)}")
        reduction_pct = (1 - len(selected_features) / (len(X[0]) if isinstance(X, list) else X.shape[1])) * 100
        print(f"Feature reduction: {reduction_pct:.1f}%")
        
        return final_model, scaler_selected, selected_features
Now let's create a function to integrate this pipeline into the predict_match function to ensure it uses the optimized model and features:

python
def predict_match_with_optimized_model(team1_name, team2_name, model=None, scaler=None, feature_names=None, export_data=True, display_details=True):
    """Predict match outcome using the optimized model with selected features and regularization."""
    print(f"Predicting match between {team1_name} and {team2_name} using optimized model...")
    
    # Get team IDs
    team1_id = get_team_id(team1_name)
    team2_id = get_team_id(team2_name)
    
    if not team1_id or not team2_id:
        print("Could not find one or both teams. Please check team names.")
        return None

    # Fetch team details to get team tags
    team1_details, team1_tag = fetch_team_details(team1_id)
    team2_details, team2_tag = fetch_team_details(team2_id)    
    print(f"Team tags: {team1_name} = {team1_tag}, {team2_name} = {team2_tag}")
    
    # Fetch match histories
    team1_history = fetch_team_match_history(team1_id)
    team2_history = fetch_team_match_history(team2_id)
    
    if not team1_history or not team2_history:
        print("Could not fetch match history for one or both teams.")
        return None
    
    # Parse match data
    team1_matches = parse_match_data(team1_history, team1_name)
    team2_matches = parse_match_data(team2_history, team2_name)

    # Store team tags for use in economy data matching
    for match in team1_matches:
        match['team_tag'] = team1_tag
    
    for match in team2_matches:
        match['team_tag'] = team2_tag
    
    # Fetch player stats for both teams
    team1_player_stats = fetch_team_player_stats(team1_id)
    team2_player_stats = fetch_team_player_stats(team2_id)

    # Calculate team stats with economy data
    team1_stats = calculate_team_stats_with_economy(team1_matches, team1_player_stats)
    team2_stats = calculate_team_stats_with_economy(team2_matches, team2_player_stats)
    
    # Store team tags in the stats
    team1_stats['team_tag'] = team1_tag
    team2_stats['team_tag'] = team2_tag
    
    # Extract additional metrics
    team1_map_performance = extract_map_performance(team1_matches)
    team2_map_performance = extract_map_performance(team2_matches)
    
    team1_tournament_performance = extract_tournament_performance(team1_matches)
    team2_tournament_performance = extract_tournament_performance(team2_matches)
    
    team1_performance_trends = analyze_performance_trends(team1_matches)
    team2_performance_trends = analyze_performance_trends(team2_matches)
    
    team1_opponent_quality = analyze_opponent_quality(team1_matches, team1_id)
    team2_opponent_quality = analyze_opponent_quality(team2_matches, team2_id)
    
    # Add derived metrics to team stats
    team1_stats['map_performance'] = team1_map_performance
    team2_stats['map_performance'] = team2_map_performance
    
    team1_stats['tournament_performance'] = team1_tournament_performance
    team2_stats['tournament_performance'] = team2_tournament_performance
    
    team1_stats['performance_trends'] = team1_performance_trends
    team2_stats['performance_trends'] = team2_performance_trends
    
    team1_stats['opponent_quality'] = team1_opponent_quality
    team2_stats['opponent_quality'] = team2_opponent_quality
    
    # Prepare data for model
    all_features = prepare_data_for_model_with_economy(team1_stats, team2_stats)
    
    if not all_features:
        print("Could not prepare features for prediction.")
        return None
    
    # Load optimized model if not provided
    if model is None:
        try:
            # Try to load optimized model first
            if os.path.exists('valorant_model_optimized.h5'):
                model = load_model('valorant_model_optimized.h5')
                
                with open('feature_scaler_optimized.pkl', 'rb') as f:
                    scaler = pickle.load(f)
                
                with open('selected_features.pkl', 'rb') as f:
                    feature_names = pickle.load(f)
                    
                print("Loaded optimized model with selected features.")
            else:
                # Fall back to regular model
                model = load_model('valorant_model.h5')
                
                with open('feature_scaler.pkl', 'rb') as f:
                    scaler = pickle.load(f)
                
                with open('feature_names.pkl', 'rb') as f:
                    feature_names = pickle.load(f)
                
                print("Loaded regular model (optimized model not found).")
        except Exception as e:
            print(f"Error loading model: {e}")
            print("Please train a model first or provide a trained model.")
            return None
    
    # Convert features to DataFrame
    features_df = pd.DataFrame([all_features])
    
    # Select only the features used in the optimized model
    if feature_names:
        # Add missing features with default values
        for feature in feature_names:
            if feature not in features_df.columns:
                features_df[feature] = 0
        
        # Keep only the selected features
        features_df = features_df[feature_names]
    
    # Scale features
    X = scaler.transform(features_df.values)
    
    # Make prediction
    prediction = model.predict(X)[0][0]
    
    # Calculate confidence
    confidence = max(prediction, 1 - prediction)
    
    result = {
        'team1': team1_name,
        'team2': team2_name,
        'team1_win_probability': float(prediction),
        'team2_win_probability': float(1 - prediction),
        'predicted_winner': team1_name if prediction > 0.5 else team2_name,
        'confidence': float(confidence),
        'team1_stats_summary': {
            'matches_played': team1_stats['matches'] if isinstance(team1_stats['matches'], int) else len(team1_stats['matches']),
            'win_rate': team1_stats['win_rate'],
            'recent_form': team1_stats['recent_form'],
            'avg_player_rating': team1_stats.get('avg_player_rating', 0),
            'star_player': team1_stats.get('player_stats', {}).get('star_player_name', ''),
            'star_player_rating': team1_stats.get('star_player_rating', 0),
            'pistol_win_rate': team1_stats.get('pistol_win_rate', 0),
            'eco_win_rate': team1_stats.get('eco_win_rate', 0),
            'full_buy_win_rate': team1_stats.get('full_buy_win_rate', 0),
            'economy_efficiency': team1_stats.get('economy_efficiency', 0)
        },
        'team2_stats_summary': {
            'matches_played': team2_stats['matches'] if isinstance(team2_stats['matches'], int) else len(team2_stats['matches']),
            'win_rate': team2_stats['win_rate'],
            'recent_form': team2_stats['recent_form'],
            'avg_player_rating': team2_stats.get('avg_player_rating', 0),
            'star_player': team2_stats.get('player_stats', {}).get('star_player_name', ''),
            'star_player_rating': team2_stats.get('star_player_rating', 0),
            'pistol_win_rate': team2_stats.get('pistol_win_rate', 0),
            'eco_win_rate': team2_stats.get('eco_win_rate', 0),
            'full_buy_win_rate': team2_stats.get('full_buy_win_rate', 0),
            'economy_efficiency': team2_stats.get('economy_efficiency', 0)
        },
        'model_info': {
            'features_used': len(feature_names) if feature_names else 'all',
            'feature_names': feature_names if feature_names else 'all',
            'model_type': 'optimized' if os.path.exists('valorant_model_optimized.h5') else 'regular'
        }
    }
    
    # Export prediction data if requested
    if export_data:
        export_prediction_data_with_economy(result, team1_stats, team2_stats)
    
    # Display detailed results if requested
    if display_details:
        display_prediction_results_with_economy(result, team1_stats, team2_stats)
        
        # Additionally display model information
        print("\n" + "="*60)
        print("MODEL INFORMATION")
        print("="*60)
        print(f"Model type: {result['model_info']['model_type']}")
        print(f"Features used: {result['model_info']['features_used']}")
        
        if feature_names and len(feature_names) <= 20:
            print("\nSelected features:")
            for feature in feature_names:
                print(f"  - {feature}")
        elif feature_names:
            print(f"\nUsing {len(feature_names)} selected features")
            
            # Count feature types
            economy_features = sum(1 for f in feature_names if any(term in f.lower() for term in 
                                                     ['eco', 'pistol', 'buy', 'economy']))
            player_features = sum(1 for f in feature_names if any(term in f.lower() for term in 
                                                    ['rating', 'acs', 'kd', 'adr', 'headshot',
                                                     'clutch', 'aces', 'first_blood']))
            map_features = sum(1 for f in feature_names if 'map_' in f.lower())
            
            print(f"  Economy features: {economy_features}")
            print(f"  Player features: {player_features}")
            print(f"  Map features: {map_features}")
    
    return result
Now, let's update the main function to include the option to run the complete optimization pipeline:

python
def main():
    """Main function to handle command line arguments and run the program."""
    parser = argparse.ArgumentParser(description="Valorant Match Predictor with Overfitting Detection and Optimization")
    
    # Add command line arguments
    parser.add_argument("--train", action="store_true", help="Train a new model")
    parser.add_argument("--optimize", action="store_true", help="Run complete model optimization pipeline")
    parser.add_argument("--predict", action="store_true", help="Predict a specific match")
    parser.add_argument("--team1", type=str, help="First team name")
    parser.add_argument("--team2", type=str, help="Second team name")
    parser.add_argument("--analyze", action="store_true", help="Analyze all upcoming matches")
    parser.add_argument("--test-teams", nargs='+', help="List of teams to use for testing")
    parser.add_argument("--backtest", action="store_true", help="Perform backtesting")
    parser.add_argument("--cutoff-date", type=str, help="Cutoff date for backtesting (YYYY/MM/DD)")
    parser.add_argument("--bet-amount", type=float, default=100, help="Bet amount for backtesting")
    parser.add_argument("--confidence", type=float, default=0.7, help="Confidence threshold for backtesting")
    parser.add_argument("--players", action="store_true", help="Include player stats in analysis")
    parser.add_argument("--economy", action="store_true", help="Include economy data in analysis")
    parser.add_argument("--learning-curves", action="store_true", help="Generate detailed learning curves")
    parser.add_argument("--verbose", action="store_true", help="Show detailed progress")

    args = parser.parse_args()
    
    if args.train:
        print("Training a new model with player statistics and economy data...")
        
        # [existing team data collection code]
        
        # Build training dataset with economy features
        print("\nBuilding training dataset with player and economy features...")
        X, y = build_training_dataset_with_economy(team_data_collection)
        
        print(f"Built training dataset with {len(X)} samples.")
        
        # Check if we have enough data to train
        if len(X) < 10:
            print("Not enough training data. Please collect more match data.")
            return
        
        if args.learning_curves:
            # Train model with detailed learning curves for overfitting detection
            print("\nTraining model with detailed learning curves for overfitting detection...")
            model, scaler, feature_names, learning_curve_data = train_model_with_learning_curves(X, y)
            
            # Display overfitting diagnosis
            print(f"\nLearning Curve Diagnosis: {learning_curve_data['diagnosis']}")
            print("\nRecommendations:")
            for rec in learning_curve_data['recommendations']:
                print(f"  {rec}")
        elif args.optimize:
            # Run complete model optimization pipeline
            print("\nRunning complete model optimization pipeline...")
            model, scaler, feature_names = optimize_model_pipeline(X, y)
        else:
            # Train regular model
            print("\nTraining standard model...")
            model, scaler, feature_names = train_model(X, y)
        
        print("Model training complete.")
    
    elif args.predict and args.team1 and args.team2:
        # Check if optimized model files exist
        optimized_model_exists = os.path.exists('valorant_model_optimized.h5')
        
        if optimized_model_exists:
            print(f"Predicting match between {args.team1} and {args.team2} using optimized model...")
            prediction = predict_match_with_optimized_model(args.team1, args.team2)
        else:
            print(f"Predicting match between {args.team1} and {args.team2} with standard model...")
            prediction = predict_match_with_economy(args.team1, args.team2)
        
        if prediction:
            visualize_prediction_with_economy(prediction)
        else:
            print(f"Could not generate prediction for {args.team1} vs {args.team2}")
    
    elif args.optimize and not args.train:
        # Run optimization on existing model data
        print("Running optimization on existing model data...")
        try:
            # Load existing training data
            with open('feature_names.pkl', 'rb') as f:
                feature_names = pickle.load(f)
                
            # Collect new data for optimization
            team_data_collection = collect_all_team_data(include_player_stats=True, include_economy=True, verbose=args.verbose)
            
            if team_data_collection:
                X, y = build_training_dataset_with_economy(team_data_collection)
                
                if len(X) > 10:
                    # Run optimization pipeline
                    model, scaler, optimized_features = optimize_model_pipeline(X, y)
                    print("Model optimization complete.")
                else:
                    print("Not enough training data for optimization.")
            else:
                print("Failed to collect team data for optimization.")
        except Exception as e:
            print(f"Error during optimization: {e}")
            print("Please train a model first before optimization.")
            
    elif args.learning_curves and not args.train:
        # Generate learning curves for existing model data
        print("Generating learning curves for existing model data...")
        try:
            # Load existing training data
            with open('feature_names.pkl', 'rb') as f:
                feature_names = pickle.load(f)
                
            # Collect new data for analysis
            team_data_collection = collect_all_team_data(include_player_stats=True, include_economy=True, verbose=args.verbose)
            
            if team_data_collection:
                X, y = build_training_dataset_with_economy(team_data_collection)
                
                if len(X) > 10:
                    # Run learning curve analysis
                    model, scaler, feature_names, learning_curve_data = train_model_with_learning_curves(X, y)
                    
                    # Display overfitting diagnosis
                    print(f"\nLearning Curve Diagnosis: {learning_curve_data['diagnosis']}")
                    print("\nRecommendations:")
                    for rec in learning_curve_data['recommendations']:
                        print(f"  {rec}")
                else:
                    print("Not enough training data for learning curve analysis.")
            else:
                print("Failed to collect team data for learning curve analysis.")
        except Exception as e:
            print(f"Error during learning curve analysis: {e}")
            print("Please train a model first before generating learning curves.")
    
    elif args.analyze:
        # Analyze upcoming matches using the best available model
        if os.path.exists('valorant_model_optimized.h5'):
            print("Analyzing upcoming matches with optimized model...")
            # [update analyze_upcoming_matches function to use optimized model]
        else:
            print("Analyzing upcoming matches with standard model...")
            analyze_upcoming_matches()
            
    elif args.backtest:
        if not args.cutoff_date:
            print("Please specify a cutoff date with --cutoff-date YYYY/MM/DD")
            return
        
        print(f"Performing backtesting with cutoff date: {args.cutoff_date}")
        print(f"Bet amount: ${args.bet_amount}, Confidence threshold: {args.confidence}")
        
        # Determine which model to use for backtesting
        if os.path.exists('valorant_model_optimized.h5'):
            print("Using optimized model for backtesting...")
            # [modify backtest_model to use optimized model]
        else:
            print("Using standard model for backtesting...")
            results = backtest_model(args.cutoff_date, 
                                    bet_amount=args.bet_amount, 
                                    confidence_threshold=args.confidence)
        
        if results:
            print("\nBacktesting Results:")
            print(f"Overall Accuracy: {results['overall_accuracy']:.4f}")
            print(f"High Confidence Accuracy: {results['high_conf_accuracy']:.4f}")
            print(f"ROI: {results['roi']:.4f}")
            print(f"Total profit: ${results['total_profit']:.2f}")
            print(f"Total bets: {results['total_bets']}")
            avg_profit = results['total_profit']/results['total_bets'] if results['total_bets'] > 0 else 0
            print(f"Average profit per bet: ${avg_profit:.2f}")
        else:
            print("Backtesting failed or returned no results.")
    
    else:
        print("Please specify an action: --train, --optimize, --learning-curves, --predict, --analyze, or --backtest")
        print("For predictions, specify --team1 and --team2")
        print("For backtesting, specify --cutoff-date YYYY/MM/DD")
Finally, let's add a function to visualize the learning curves and model performance after optimization:

python
def visualize_optimization_results(original_metrics, optimized_metrics):
    """Visualize the improvement from model optimization."""
    plt.figure(figsize=(15, 10))
    
    # Metrics comparison
    plt.subplot(2, 2, 1)
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']
    x = np.arange(len(metrics))
    width = 0.35
    
    plt.bar(x - width/2, original_metrics, width, label='Original Model')
    plt.bar(x + width/2, optimized_metrics, width, label='Optimized Model')
    
    plt.title('Performance Metrics Comparison', fontsize=14)
    plt.ylabel('Score', fontsize=12)
    plt.xticks(x, metrics)
    plt.grid(True, linestyle='--', alpha=0.6, axis='y')
    plt.legend(fontsize=12)
    
    # Calculate improvement percentages
    improvements = [(opt - orig) / orig * 100 for orig, opt in zip(original_metrics, optimized_metrics)]
    
    # Improvement visualization
    plt.subplot(2, 2, 2)
    colors = ['green' if imp > 0 else 'red' for imp in improvements]
    plt.bar(metrics, improvements, color=colors)
    plt.axhline(y=0, color='gray', linestyle='-')
    plt.title('Percentage Improvement in Metrics', fontsize=14)
    plt.ylabel('Improvement (%)', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6, axis='y')
    
    # Add improvement values
    for i, imp in enumerate(improvements):
        plt.text(i, imp + (1 if imp > 0 else -2), f"{imp:.1f}%", 
                 ha='center', va='bottom' if imp > 0 else 'top', fontsize=10)
    
    # Confusion matrix comparison
    plt.subplot(2, 2, 3)
    # [code to display confusion matrices - would need to be implemented]
    plt.title('Original Model Confusion Matrix', fontsize=14)
    
    plt.subplot(2, 2, 4)
    # [code to display confusion matrices - would need to be implemented]
    plt.title('Optimized Model Confusion Matrix', fontsize=14)
    
    plt.tight_layout()
    plt.savefig('optimization_results.png', dpi=300)
    plt.show()
With these implementations, you now have a comprehensive pipeline for detecting and mitigating overfitting in your Valorant match prediction model. The key components are:

Learning Curve Analysis: To diagnose if and how much your model is overfitting
Feature Selection: To reduce dimensionality and focus on the most informative features
Regularization Optimization: To find the optimal regularization parameters
Model Validation: To ensure the optimized model actually performs better
This approach should significantly improve your model's generalization capability and prediction accuracy on real-world matches.




Aweomse, now can you take me through each of the funcitons you added and if they replace a function or if there are helpers?
eg.

def this_function replaces _that funciton
def this_function2 does not replace any functions

Edit
I'll explain each function that was added to help address overfitting in your Valorant match prediction model, clarifying whether they replace existing functions or serve as helpers:

1. Learning Curve Analysis Functions
train_model_with_learning_curves(X, y, test_size=0.2, random_state=42)
Replaces: Partially replaces train_model()
Purpose: Enhanced version of your existing training function that adds detailed learning curve tracking and overfitting diagnostics
Key enhancements: Tracks training and validation metrics across epochs, diagnoses overfitting severity, and provides visual diagnostics and recommendations
LearningCurvesCallback (class within train_model_with_learning_curves)
Replaces: No existing function
Purpose: Helper class that tracks metrics during training for building learning curves
Usage: Monitors train/validation gaps and provides real-time overfitting diagnosis
2. Feature Selection Functions
select_optimal_features(X, y, test_size=0.2, random_state=42)
Replaces: No existing function
Purpose: New function that identifies the most predictive features and eliminates noise
Key functionality: Tests different feature subsets to find the optimal balance between accuracy and model complexity
3. Regularization Optimization Functions
create_deep_learning_model_with_economy_and_regularization(input_dim, regularization_strength=0.001, dropout_rate=0.4)
Replaces: Extends create_deep_learning_model_with_economy()
Purpose: Modified version of your model architecture with adjustable regularization parameters
Key changes: Adds configurable L2 regularization and dropout rates
find_optimal_regularization(X, y, test_size=0.2, random_state=42)
Replaces: No existing function
Purpose: Helper function that systematically tests different regularization parameters
Usage: Finds the best L2 regularization strength and dropout rate for your specific dataset
4. Integrated Optimization Pipeline
optimize_model_pipeline(X, y, test_size=0.2, random_state=42)
Replaces: No existing function
Purpose: Comprehensive pipeline that combines feature selection, learning curve analysis, and regularization optimization
Key functionality: Performs multi-stage optimization to systematically improve model generalization
train_model_with_overfitting_detection(X, y, test_size=0.2, random_state=42)
Replaces: No existing function
Purpose: Helper function that first diagnoses overfitting with learning curves, then applies appropriate regularization
Usage: Integrates learning curve diagnosis with targeted regularization improvements
5. Optimized Prediction Functions
predict_match_with_optimized_model(team1_name, team2_name, ...)
Replaces: Extends predict_match_with_economy()
Purpose: Enhances the prediction function to use the optimized model and selected features
Key changes: Automatically uses the optimized model if available, provides additional model information
visualize_optimization_results(original_metrics, optimized_metrics)
Replaces: No existing function
Purpose: Helper function that visualizes the improvements from optimization
Usage: Creates comparative visualizations of model performance before and after optimization
6. Updated Main Function
The updated main() function doesn't replace your existing one but extends it with new command-line arguments and functionality:

--optimize: Runs the complete optimization pipeline
--learning-curves: Generates detailed learning curves without full optimization
Enhanced handling of optimized models throughout prediction and backtesting
Implementation Strategy
To implement these improvements in your codebase:

Add the new functions as presented (they don't conflict with existing ones)
Replace the modified functions (like create_deep_learning_model_with_economy_and_regularization) or keep both and choose which to use based on context
Update your main() function to include the new command-line options
Test each component incrementally, starting with learning curves, then feature selection, and finally regularization optimization
These functions provide a systematic approach to detecting and addressing overfitting in your model, which should significantly improve its generalization performance and prediction accuracy.





#!/usr/bin/env python3 print("Starting Deep Learning Valorant Match Predictor...") import requests import json import os import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime import argparse import pickle import time import re from tqdm import tqdm #

pasted

Final feature matrix shape: (40, 278)
Class distribution: [16 16]
Traceback (most recent call last):
  File "/Users/ethan.harker/Desktop/code/Valorant-Machine-Learning/test1.py", line 5216, in <module>
    main()
  File "/Users/ethan.harker/Desktop/code/Valorant-Machine-Learning/test1.py", line 5075, in main
    model, scaler, feature_names, learning_curve_data = train_model_with_learning_curves(X, y)
  File "/Users/ethan.harker/Desktop/code/Valorant-Machine-Learning/test1.py", line 3230, in train_model_with_learning_curves
    model = create_deep_learning_model_with_economy(input_dim)
NameError: name 'create_deep_learning_model_with_economy' is not defined
ethan.harker@Ethans-MacBook-Air Valorant-Machine-Learning %

Edit
I see the issue in your code. The error is that the function create_deep_learning_model_with_economy is not defined anywhere in your script, but it's being called in the train_model_with_learning_curves function. Let me help you address this problem and provide some improvements for your Valorant match prediction model.

Missing Model Creation Function
First, let's define the missing function. You need to add this function to your code:

python
def create_deep_learning_model_with_economy(input_dim):
    """Create an enhanced deep learning model for match prediction with player stats and economy data."""
    # Define inputs
    inputs = Input(shape=(input_dim,))
    
    # First layer - shared feature processing
    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    
    # Second layer - deeper processing
    x = Dense(128, activation='relu', kernel_regularizer=l2(0.0005))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    # Player stats pathway with additional neurons
    x = Dense(96, activation='relu', kernel_regularizer=l2(0.0005))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.25)(x)
    
    # Economy-specific pathway with expanded capacity
    x = Dense(64, activation='relu', kernel_regularizer=l2(0.0005))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.25)(x)
    
    # Combined pathway
    x = Dense(32, activation='relu', kernel_regularizer=l2(0.0005))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    # Output
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Compile model
    model.compile(loss='binary_crossentropy', 
                 optimizer=Adam(learning_rate=0.0005),
                 metrics=['accuracy'])
    
    # Print model summary to see the expanded architecture
    print("\nModel Architecture:")
    model.summary()
    
    return model
Improving Model Performance
Beyond fixing the immediate error, here are some strategic improvements to address overfitting and enhance model performance:

1. Input Data & Feature Engineering
Your model is trying to work with 278 features but only 40 samples. This dimension mismatch is a classic cause of overfitting. Here are recommendations:

python
def reduce_dimensionality(X, y, method='pca', n_components=40):
    """Reduce feature dimensionality to avoid overfitting."""
    print(f"Reducing dimensions from {X.shape[1]} to {n_components} using {method}")
    
    if method == 'pca':
        from sklearn.decomposition import PCA
        reducer = PCA(n_components=n_components, random_state=42)
    elif method == 'feature_selection':
        from sklearn.feature_selection import SelectKBest, f_classif
        reducer = SelectKBest(f_classif, k=n_components)
    else:
        raise ValueError(f"Unknown dimensionality reduction method: {method}")
    
    X_reduced = reducer.fit_transform(X, y)
    print(f"Reduced feature matrix shape: {X_reduced.shape}")
    
    return X_reduced, reducer
2. Sample Generation
With only 40 samples, you need more training data. Add this after your SMOTE implementation:

python
def generate_additional_samples(X_train, y_train, multiplier=2):
    """Generate additional synthetic samples beyond SMOTE."""
    from sklearn.utils import resample
    
    # Only apply if we have fewer than 100 samples
    if X_train.shape[0] < 100:
        print(f"Generating additional samples (multiplier={multiplier})")
        n_samples = X_train.shape[0]
        n_new_samples = n_samples * (multiplier - 1)
        
        # Get indices for each class
        class_0_idx = np.where(y_train == 0)[0]
        class_1_idx = np.where(y_train == 1)[0]
        
        # Generate new samples for each class
        X_new = []
        y_new = []
        
        for class_idx, class_label in [(class_0_idx, 0), (class_1_idx, 1)]:
            X_class = X_train[class_idx]
            
            # Create bootstrap samples
            X_resampled = resample(X_class, 
                                  n_samples=int(n_new_samples/2),
                                  random_state=42)
            
            # Add small Gaussian noise to create new samples
            noise = np.random.normal(0, 0.01, X_resampled.shape)
            X_resampled = X_resampled + noise
            
            X_new.append(X_resampled)
            y_new.extend([class_label] * X_resampled.shape[0])
        
        # Combine original and new samples
        X_augmented = np.vstack([X_train, np.vstack(X_new)])
        y_augmented = np.concatenate([y_train, np.array(y_new)])
        
        print(f"Augmented dataset shape: {X_augmented.shape}")
        return X_augmented, y_augmented
    
    return X_train, y_train
3. Adaptive Regularization
Create a function to identify the optimal regularization strength based on the training/validation split:

python
def find_optimal_regularization(X_train, y_train, X_val, y_val):
    """Find the optimal regularization strength to reduce overfitting."""
    l2_values = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]
    dropout_values = [0.2, 0.3, 0.4, 0.5]
    
    best_val_acc = 0
    best_params = {'l2': 0.001, 'dropout': 0.4}
    
    print("Testing regularization parameters...")
    
    for l2_val in l2_values:
        for dropout_val in dropout_values:
            # Create a small model to test parameters
            input_dim = X_train.shape[1]
            inputs = Input(shape=(input_dim,))
            x = Dense(64, activation='relu', kernel_regularizer=l2(l2_val))(inputs)
            x = BatchNormalization()(x)
            x = Dropout(dropout_val)(x)
            x = Dense(32, activation='relu', kernel_regularizer=l2(l2_val))(x)
            x = BatchNormalization()(x)
            x = Dropout(dropout_val)(x)
            outputs = Dense(1, activation='sigmoid')(x)
            
            model = Model(inputs=inputs, outputs=outputs)
            model.compile(loss='binary_crossentropy', 
                         optimizer=Adam(learning_rate=0.001),
                         metrics=['accuracy'])
            
            # Train quickly
            history = model.fit(
                X_train, y_train,
                epochs=20,
                batch_size=16,
                validation_data=(X_val, y_val),
                verbose=0
            )
            
            # Check validation accuracy
            val_acc = max(history.history['val_accuracy'])
            train_acc = max(history.history['accuracy'])
            acc_gap = train_acc - val_acc
            
            # We want high validation accuracy with small gap
            score = val_acc - (0.5 * acc_gap)
            
            if score > best_val_acc:
                best_val_acc = score
                best_params = {'l2': l2_val, 'dropout': dropout_val}
                print(f"New best params: L2={l2_val}, Dropout={dropout_val}, Val Acc={val_acc:.4f}, Gap={acc_gap:.4f}")
    
    return best_params
4. Ensemble Modeling
Create an ensemble of models to reduce overfitting:

python
def create_model_ensemble(X_train, y_train, X_val, y_val, n_models=5):
    """Create an ensemble of models to improve prediction stability."""
    models = []
    
    for i in range(n_models):
        # Create model with slightly different architecture
        input_dim = X_train.shape[1]
        
        # Vary network architecture slightly
        hidden_units = np.random.choice([32, 64, 96, 128, 256])
        dropout_rate = np.random.uniform(0.2, 0.5)
        l2_reg = np.random.choice([0.0001, 0.0005, 0.001, 0.005, 0.01])
        
        # Create model
        model = create_deep_learning_model_with_economy_and_regularization(
            input_dim, regularization_strength=l2_reg, dropout_rate=dropout_rate
        )
        
        # Train with early stopping
        early_stopping = EarlyStopping(
            monitor='val_loss', patience=10, restore_best_weights=True
        )
        
        model.fit(
            X_train, y_train,
            epochs=50,
            batch_size=16,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping],
            verbose=0
        )
        
        # Evaluate and add to ensemble
        val_acc = model.evaluate(X_val, y_val, verbose=0)[1]
        print(f"Model {i+1}: Val Accuracy = {val_acc:.4f}")
        models.append(model)
    
    return models
5. Training Function with Advanced Techniques
Here's a modified training function that incorporates dimensionality reduction, data augmentation, and optimal regularization:

python
def train_advanced_model(X, y, test_size=0.2, random_state=42):
    """Train the model using advanced techniques to combat overfitting."""
    # Convert to numpy if needed
    if isinstance(X, list):
        X = np.array(X)
    if isinstance(y, list):
        y = np.array(y)
    
    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Reduce dimensionality if needed
    if X_scaled.shape[1] > X_scaled.shape[0] * 0.5:
        target_components = min(X_scaled.shape[0] // 2, 50)  # Rule of thumb
        X_reduced, reducer = reduce_dimensionality(X_scaled, y, n_components=target_components)
    else:
        X_reduced = X_scaled
        reducer = None
    
    # Split into train and validation sets
    X_train, X_val, y_train, y_val = train_test_split(
        X_reduced, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    # Apply SMOTE for class imbalance
    class_counts = np.bincount(y_train)
    if min(class_counts) / max(class_counts) < 0.8:  # Check for imbalance
        try:
            min_samples = min(class_counts)
            k_neighbors = min(5, min_samples-1)
            smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            print(f"Applied SMOTE: {np.bincount(y_train)}")
        except Exception as e:
            print(f"Error applying SMOTE: {e}")
    
    # Generate additional samples for small datasets
    X_train, y_train = generate_additional_samples(X_train, y_train, multiplier=3)
    
    # Find optimal regularization parameters
    best_params = find_optimal_regularization(X_train, y_train, X_val, y_val)
    
    # Train final model with optimal parameters
    input_dim = X_train.shape[1]
    model = create_deep_learning_model_with_economy_and_regularization(
        input_dim, 
        regularization_strength=best_params['l2'],
        dropout_rate=best_params['dropout']
    )
    
    # Define callbacks
    early_stopping = EarlyStopping(
        monitor='val_loss', patience=15, restore_best_weights=True, verbose=1
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1
    )
    
    # Train model
    history = model.fit(
        X_train, y_train,
        epochs=100,
        batch_size=min(32, len(X_train) // 4),  # Adjust batch size for small datasets
        validation_data=(X_val, y_val),
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    
    # Save preprocessing steps for later use
    preprocessing = {
        'scaler': scaler,
        'reducer': reducer,
        'feature_names': list(range(X_reduced.shape[1])) if reducer else None
    }
    
    return model, preprocessing, history
Additional Recommendations
Beyond these code changes, here are more recommendations to improve your model:

Feature Correlation Analysis: Many of your 278 features are likely highly correlated. Run a correlation analysis and remove features with correlation > 0.8.
Cross-Validation: With such a small dataset, use k-fold cross-validation (k=5 or k=10) instead of a single train/test split.
Bayesian Optimization: For hyperparameter tuning, consider Bayesian optimization instead of grid search to find optimal parameters more efficiently.
Gradient Accumulation: For small datasets, implement gradient accumulation to stabilize training.
Feature Importance Analysis: After training, analyze feature importance to understand which metrics (player stats, economy data, etc.) are most predictive.
Learning Rate Warmup: Implement learning rate warmup to help the model find better optima.
Loss Function: Consider using focal loss instead of binary cross-entropy to handle class imbalance better.
Data Quality: Review your data cleaning process to ensure there are no extreme outliers that could throw off the model.
By implementing these changes, you should see a substantial improvement in model performance and reduced overfitting, even with your limited dataset.





What does this tell you about the training? This was just a test dataset with 5 teams. I will do 30 teams next, then 100 teams.


Skipping match against PCIFIC Espor - no data available Skipping match against PCIFIC Espor - no data available Skipping match against PCIFIC Espor - no data available Skipping match against PCIFIC Espor - no data available Skipping match against PCIFIC Espor - no data available Skipping

pasted



recent_5_significant_diff: int64 recent_5_significant_advantage_team1: int64 recent_10_significant_diff: int64 recent_10_significant_advantage_team1: int64 recent_20_significant_diff: int64 recent_20_significant_advantage_team1: int64 momentum_5_vs_10_significant_diff: int64 momentum_5_vs_10_signifi

pasted


detailed_learning_curves.png


#!/usr/bin/env python3 print("Starting Deep Learning Valorant Match Predictor...") import requests import json import os import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime import argparse import pickle import time import re from tqdm import tqdm #


#!/usr/bin/env python3 print("Starting Deep Learning Valorant Match Predictor...") import requests import json import os import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import datetime import argparse import pickle import time import re from tqdm import tqdm #



Pasted content

37.30 KB 758 lines

Formatting may be inconsistent from source

  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available
  Skipping match against PCIFIC Espor - no data available

=================================================
TRAINING DATASET STATISTICS
=================================================
Created 40 training samples with:
  - Samples with economy data: 40
  - Samples with player stats: 40
  - Samples with both: 40
  - Teams with economy data: 5/5
  - Teams with player stats: 5/5

Feature breakdown:
  - Economy features: 52
  - Player features: 64
  - Map features: 1
  - Total features: 278

Sample economy features:
  - pistol_win_rate_diff
  - better_pistol_team1
  - avg_pistol_win_rate
  - eco_win_rate_diff
  - better_eco_team1

Sample player features:
  - player_rating_diff
  - better_player_rating_team1
  - avg_player_rating
  - acs_diff
  - better_acs_team1
Built training dataset with 40 samples.

Training model with detailed learning curves for overfitting detection...

Feature columns and their types:
win_rate_diff: float64
better_win_rate_team1: int64
recent_form_diff: float64
better_recent_form_team1: int64
score_diff_differential: float64
better_score_diff_team1: int64
total_matches: int64
match_count_diff: int64
avg_win_rate: float64
avg_recent_form: float64
wins_diff: int64
losses_diff: int64
win_loss_ratio_diff: float64
avg_score_diff: float64
better_avg_score_team1: int64
avg_score_metric: float64
avg_opponent_score_diff: float64
better_defense_team1: int64
avg_defense_metric: float64
recent_5_diff: float64
better_recent_5_team1: int64
avg_recent_5: float64
recent_10_diff: float64
better_recent_10_team1: int64
avg_recent_10: float64
recent_20_diff: float64
better_recent_20_team1: int64
avg_recent_20: float64
momentum_5_vs_10_diff: float64
better_momentum_5_vs_10_team1: int64
avg_momentum_5_vs_10: float64
momentum_10_vs_20_diff: float64
better_momentum_10_vs_20_team1: int64
avg_momentum_10_vs_20: float64
weighted_win_rate_diff: float64
better_weighted_win_rate_team1: int64
avg_weighted_win_rate: float64
player_rating_diff: float64
better_player_rating_team1: int64
avg_player_rating: float64
acs_diff: float64
better_acs_team1: int64
avg_acs: float64
kd_diff: float64
better_kd_team1: int64
avg_kd: float64
kast_diff: float64
better_kast_team1: int64
avg_kast: float64
adr_diff: float64
better_adr_team1: int64
avg_adr: float64
headshot_diff: float64
better_headshot_team1: int64
avg_headshot: float64
star_player_diff: float64
better_star_player_team1: int64
avg_star_player: float64
consistency_diff: float64
better_consistency_team1: int64
avg_consistency: float64
fk_fd_diff: float64
better_fk_fd_team1: int64
avg_fk_fd: float64
player_count_diff: int64
player_count_ratio: float64
avg_player_count: float64
pistol_win_rate_diff: float64
better_pistol_team1: int64
avg_pistol_win_rate: float64
eco_win_rate_diff: float64
better_eco_team1: int64
avg_eco_win_rate: float64
semi_eco_win_rate_diff: float64
better_semi_eco_team1: int64
avg_semi_eco_win_rate: float64
semi_buy_win_rate_diff: float64
better_semi_buy_team1: int64
avg_semi_buy_win_rate: float64
full_buy_win_rate_diff: float64
better_full_buy_team1: int64
avg_full_buy_win_rate: float64
low_economy_win_rate_diff: float64
better_low_economy_team1: int64
avg_low_economy_win_rate: float64
high_economy_win_rate_diff: float64
better_high_economy_team1: int64
avg_high_economy_win_rate: float64
economy_efficiency_diff: float64
better_economy_efficiency_team1: int64
avg_economy_efficiency: float64
pistol_confidence_diff: float64
better_pistol_confidence_team1: int64
avg_pistol_confidence: float64
pistol_sample_diff: int64
better_pistol_sample_team1: int64
avg_pistol_sample: float64
first_bloods_diff: float64
better_first_bloods_team1: int64
avg_first_bloods: float64
clutches_diff: float64
better_clutches_team1: int64
avg_clutches: float64
aces_diff: float64
better_aces_team1: int64
avg_aces: float64
entry_kills_diff: float64
better_entry_kills_team1: int64
avg_entry_kills: float64
first_kill_diff_differential: float64
better_first_kill_diff_team1: int64
avg_first_kill_diff: float64
headshot_percentage_diff: float64
better_headshot_percentage_team1: int64
avg_headshot_percentage: float64
kast_adv_diff: float64
better_kast_adv_team1: int64
avg_kast_adv: float64
adr_adv_diff: float64
better_adr_adv_team1: int64
avg_adr_adv: float64
agent_overlap: int64
common_map_count: int64
opponent_rank_diff: int64
better_opponent_quality_team1: int64
avg_opponent_rank: float64
opponent_rating_diff: int64
better_opponent_rating_team1: int64
avg_opponent_rating: float64
top_10_win_rate_diff: int64
better_top_10_team1: int64
avg_top_10_win_rate: float64
bottom_50_win_rate_diff: int64
better_bottom_50_team1: int64
avg_bottom_50_win_rate: float64
upset_factor_diff: int64
better_upset_team1: int64
avg_upset_factor: float64
upset_vulnerability_diff: int64
less_vulnerable_team1: int64
avg_upset_vulnerability: float64
team_rank_diff: int64
better_ranked_team1: int64
avg_team_rank: float64
rank_gap: int64
team_rating_quality_diff: float64
better_rated_quality_team1: int64
avg_team_rating_quality: float64
rating_gap: float64
mid_tier_tourney_diff: float64
better_mid_tier_team1: float64
avg_mid_tier_win_rate: float64
overall_tourney_diff: float64
better_tourney_team1: int64
avg_tourney_win_rate: float64
h2h_win_rate: float64
h2h_matches: int64
h2h_score_diff: float64
h2h_advantage_team1: int64
h2h_significant: int64
h2h_confidence: float64
rating_x_win_rate: float64
pistol_x_eco: float64
pistol_x_full_buy: float64
star_x_consistency: float64
h2h_x_form: float64
headshot_x_kd: float64
win_rate_x_opp_quality: float64
first_blood_x_win_rate: float64
clutch_x_consistency: float64
team1_better_count: int64
team1_better_ratio: float64
win_rate_significant_diff: int64
win_rate_significant_advantage_team1: int64
recent_form_significant_diff: int64
recent_form_significant_advantage_team1: int64
match_count_significant_diff: int64
match_count_significant_advantage_team1: int64
wins_significant_diff: int64
wins_significant_advantage_team1: int64
losses_significant_diff: int64
losses_significant_advantage_team1: int64
win_loss_ratio_significant_diff: int64
win_loss_ratio_significant_advantage_team1: int64
avg_score_significant_diff: int64
avg_score_significant_advantage_team1: int64
avg_opponent_score_significant_diff: int64
avg_opponent_score_significant_advantage_team1: int64
recent_5_significant_diff: int64
recent_5_significant_advantage_team1: int64
recent_10_significant_diff: int64
recent_10_significant_advantage_team1: int64
recent_20_significant_diff: int64
recent_20_significant_advantage_team1: int64
momentum_5_vs_10_significant_diff: int64
momentum_5_vs_10_significant_advantage_team1: int64
momentum_10_vs_20_significant_diff: int64
momentum_10_vs_20_significant_advantage_team1: int64
weighted_win_rate_significant_diff: int64
weighted_win_rate_significant_advantage_team1: int64
player_rating_significant_diff: int64
player_rating_significant_advantage_team1: int64
acs_significant_diff: int64
acs_significant_advantage_team1: int64
kd_significant_diff: int64
kd_significant_advantage_team1: int64
kast_significant_diff: int64
kast_significant_advantage_team1: int64
adr_significant_diff: int64
adr_significant_advantage_team1: int64
headshot_significant_diff: int64
headshot_significant_advantage_team1: int64
star_player_significant_diff: int64
star_player_significant_advantage_team1: int64
consistency_significant_diff: int64
consistency_significant_advantage_team1: int64
fk_fd_significant_diff: int64
fk_fd_significant_advantage_team1: int64
player_count_significant_diff: int64
player_count_significant_advantage_team1: int64
pistol_win_rate_significant_diff: int64
pistol_win_rate_significant_advantage_team1: int64
eco_win_rate_significant_diff: int64
eco_win_rate_significant_advantage_team1: int64
semi_eco_win_rate_significant_diff: int64
semi_eco_win_rate_significant_advantage_team1: int64
semi_buy_win_rate_significant_diff: int64
semi_buy_win_rate_significant_advantage_team1: int64
full_buy_win_rate_significant_diff: int64
full_buy_win_rate_significant_advantage_team1: int64
low_economy_win_rate_significant_diff: int64
low_economy_win_rate_significant_advantage_team1: int64
high_economy_win_rate_significant_diff: int64
high_economy_win_rate_significant_advantage_team1: int64
economy_efficiency_significant_diff: int64
economy_efficiency_significant_advantage_team1: int64
pistol_confidence_significant_diff: int64
pistol_confidence_significant_advantage_team1: int64
pistol_sample_significant_diff: int64
pistol_sample_significant_advantage_team1: int64
first_bloods_significant_diff: int64
first_bloods_significant_advantage_team1: int64
clutches_significant_diff: int64
clutches_significant_advantage_team1: int64
aces_significant_diff: int64
aces_significant_advantage_team1: int64
entry_kills_significant_diff: int64
entry_kills_significant_advantage_team1: int64
avg_first_kill_significant_diff: int64
avg_first_kill_significant_advantage_team1: int64
headshot_percentage_significant_diff: int64
headshot_percentage_significant_advantage_team1: int64
kast_adv_significant_diff: int64
kast_adv_significant_advantage_team1: int64
adr_adv_significant_diff: int64
adr_adv_significant_advantage_team1: int64
opponent_rank_significant_diff: int64
opponent_rank_significant_advantage_team1: int64
opponent_rating_significant_diff: int64
opponent_rating_significant_advantage_team1: int64
top_10_win_rate_significant_diff: int64
top_10_win_rate_significant_advantage_team1: int64
bottom_50_win_rate_significant_diff: int64
bottom_50_win_rate_significant_advantage_team1: int64
upset_factor_significant_diff: int64
upset_factor_significant_advantage_team1: int64
upset_vulnerability_significant_diff: int64
upset_vulnerability_significant_advantage_team1: int64
team_rank_significant_diff: int64
team_rank_significant_advantage_team1: int64
team_rating_quality_significant_diff: int64
team_rating_quality_significant_advantage_team1: int64
mid_tier_tourney_significant_diff: float64
mid_tier_tourney_significant_advantage_team1: float64
overall_tourney_significant_diff: int64
overall_tourney_significant_advantage_team1: int64
h2h_score_significant_diff: int64
h2h_score_significant_advantage_team1: int64

Final feature matrix shape: (40, 278)
Class distribution: [16 16]

Model Architecture:
Model: "functional"

 Layer (type)         Output Shape    Param # 

 input_layer          (None, 278)           0 
 (InputLayer)                                 

 dense (Dense)        (None, 256)      71,424 

 batch_normalization  (None, 256)       1,024 
 (BatchNormalizatio                          

 dropout (Dropout)    (None, 256)           0 

 dense_1 (Dense)      (None, 128)      32,896 

 batch_normalizatio  (None, 128)         512 
 (BatchNormalizatio                          

 dropout_1 (Dropout)  (None, 128)           0 

 dense_2 (Dense)      (None, 96)       12,384 

 batch_normalizatio  (None, 96)          384 
 (BatchNormalizatio                          

 dropout_2 (Dropout)  (None, 96)            0 

 dense_3 (Dense)      (None, 64)        6,208 

 batch_normalizatio  (None, 64)          256 
 (BatchNormalizatio                          

 dropout_3 (Dropout)  (None, 64)            0 

 dense_4 (Dense)      (None, 32)        2,080 

 batch_normalizatio  (None, 32)          128 
 (BatchNormalizatio                          

 dropout_4 (Dropout)  (None, 32)            0 

 dense_5 (Dense)      (None, 1)            33 

 Total params: 127,329 (497.38 KB)
 Trainable params: 126,177 (492.88 KB)
 Non-trainable params: 1,152 (4.50 KB)
Epoch 1/100
1/1  0s 8s/step - accuracy: 0.4062 - loss: 1.5894WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 

Epoch 1 Overfitting Status: Good Fit
Train-Val Accuracy Gap: -0.0938, Loss Gap: 0.4087
1/1  9s 9s/step - accuracy: 0.4062 - loss: 1.5894 - val_accuracy: 0.5000 - val_loss: 1.1807 - learning_rate: 5.0000e-04
Epoch 2/100
1/1  0s 52ms/step - accuracy: 0.4062 - loss: 1.5855
Epoch 2 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0312, Loss Gap: 0.4064
1/1  0s 350ms/step - accuracy: 0.4062 - loss: 1.5855 - val_accuracy: 0.3750 - val_loss: 1.1790 - learning_rate: 5.0000e-04
Epoch 3/100
1/1  0s 74ms/step - accuracy: 0.6250 - loss: 1.2191
Epoch 3 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: 0.0428
1/1  0s 350ms/step - accuracy: 0.6250 - loss: 1.2191 - val_accuracy: 0.3750 - val_loss: 1.1762 - learning_rate: 5.0000e-04
Epoch 4/100
1/1  0s 57ms/step - accuracy: 0.5000 - loss: 1.5661
Epoch 4 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.1250, Loss Gap: 0.3919
1/1  0s 384ms/step - accuracy: 0.5000 - loss: 1.5661 - val_accuracy: 0.3750 - val_loss: 1.1742 - learning_rate: 5.0000e-04
Epoch 5/100
1/1  0s 53ms/step - accuracy: 0.6875 - loss: 1.1591
Epoch 5 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.3125, Loss Gap: -0.0104
1/1  0s 379ms/step - accuracy: 0.6875 - loss: 1.1591 - val_accuracy: 0.3750 - val_loss: 1.1695 - learning_rate: 5.0000e-04
Epoch 6/100
1/1  0s 54ms/step - accuracy: 0.5312 - loss: 1.3373
Epoch 6 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0312, Loss Gap: 0.1723
1/1  0s 497ms/step - accuracy: 0.5312 - loss: 1.3373 - val_accuracy: 0.5000 - val_loss: 1.1651 - learning_rate: 5.0000e-04
Epoch 7/100
1/1  0s 40ms/step - accuracy: 0.5312 - loss: 1.1432
Epoch 7 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0312, Loss Gap: -0.0173
1/1  1s 509ms/step - accuracy: 0.5312 - loss: 1.1432 - val_accuracy: 0.5000 - val_loss: 1.1605 - learning_rate: 5.0000e-04
Epoch 8/100
1/1  0s 48ms/step - accuracy: 0.5938 - loss: 1.1520
Epoch 8 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.0938, Loss Gap: -0.0035
1/1  0s 332ms/step - accuracy: 0.5938 - loss: 1.1520 - val_accuracy: 0.5000 - val_loss: 1.1555 - learning_rate: 5.0000e-04
Epoch 9/100
1/1  0s 43ms/step - accuracy: 0.5000 - loss: 1.3099
Epoch 9 Overfitting Status: Good Fit
Train-Val Accuracy Gap: 0.0000, Loss Gap: 0.1584
1/1  0s 365ms/step - accuracy: 0.5000 - loss: 1.3099 - val_accuracy: 0.5000 - val_loss: 1.1516 - learning_rate: 5.0000e-04
Epoch 10/100
1/1  0s 59ms/step - accuracy: 0.6875 - loss: 1.0504
Epoch 10 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.0977
1/1  0s 479ms/step - accuracy: 0.6875 - loss: 1.0504 - val_accuracy: 0.5000 - val_loss: 1.1481 - learning_rate: 5.0000e-04
Epoch 11/100
1/1  0s 40ms/step - accuracy: 0.5938 - loss: 1.2408
Epoch 11 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.0938, Loss Gap: 0.0974
1/1  0s 341ms/step - accuracy: 0.5938 - loss: 1.2408 - val_accuracy: 0.5000 - val_loss: 1.1434 - learning_rate: 5.0000e-04
Epoch 12/100
1/1  0s 61ms/step - accuracy: 0.6875 - loss: 1.0089
Epoch 12 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.1299
1/1  0s 437ms/step - accuracy: 0.6875 - loss: 1.0089 - val_accuracy: 0.5000 - val_loss: 1.1389 - learning_rate: 5.0000e-04
Epoch 13/100
1/1  0s 45ms/step - accuracy: 0.5625 - loss: 1.1582
Epoch 13 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0625, Loss Gap: 0.0245
1/1  0s 286ms/step - accuracy: 0.5625 - loss: 1.1582 - val_accuracy: 0.5000 - val_loss: 1.1337 - learning_rate: 5.0000e-04
Epoch 14/100
1/1  0s 42ms/step - accuracy: 0.5938 - loss: 1.1326
Epoch 14 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.0938, Loss Gap: 0.0043
1/1  0s 309ms/step - accuracy: 0.5938 - loss: 1.1326 - val_accuracy: 0.5000 - val_loss: 1.1283 - learning_rate: 5.0000e-04
Epoch 15/100
1/1  0s 59ms/step - accuracy: 0.6875 - loss: 1.0172
Epoch 15 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.1059
1/1  0s 398ms/step - accuracy: 0.6875 - loss: 1.0172 - val_accuracy: 0.5000 - val_loss: 1.1231 - learning_rate: 5.0000e-04
Epoch 16/100
1/1  0s 62ms/step - accuracy: 0.7500 - loss: 0.9138
Epoch 16 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2041
1/1  1s 763ms/step - accuracy: 0.7500 - loss: 0.9138 - val_accuracy: 0.5000 - val_loss: 1.1179 - learning_rate: 5.0000e-04
Epoch 17/100
1/1  0s 72ms/step - accuracy: 0.6250 - loss: 1.0291
Epoch 17 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.1250, Loss Gap: -0.0838
1/1  1s 583ms/step - accuracy: 0.6250 - loss: 1.0291 - val_accuracy: 0.5000 - val_loss: 1.1129 - learning_rate: 5.0000e-04
Epoch 18/100
1/1  0s 55ms/step - accuracy: 0.7500 - loss: 0.9252
Epoch 18 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.1813
1/1  0s 315ms/step - accuracy: 0.7500 - loss: 0.9252 - val_accuracy: 0.5000 - val_loss: 1.1066 - learning_rate: 5.0000e-04
Epoch 19/100
1/1  0s 49ms/step - accuracy: 0.6250 - loss: 1.1217
Epoch 19 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.1250, Loss Gap: 0.0212
1/1  0s 370ms/step - accuracy: 0.6250 - loss: 1.1217 - val_accuracy: 0.5000 - val_loss: 1.1005 - learning_rate: 5.0000e-04
Epoch 20/100
1/1  0s 47ms/step - accuracy: 0.7188 - loss: 1.0576WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 

Epoch 20 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.0938, Loss Gap: -0.0377
1/1  0s 464ms/step - accuracy: 0.7188 - loss: 1.0576 - val_accuracy: 0.6250 - val_loss: 1.0953 - learning_rate: 5.0000e-04
Epoch 21/100
1/1  0s 48ms/step - accuracy: 0.6250 - loss: 1.1420
Epoch 21 Overfitting Status: Good Fit
Train-Val Accuracy Gap: 0.0000, Loss Gap: 0.0518
1/1  0s 261ms/step - accuracy: 0.6250 - loss: 1.1420 - val_accuracy: 0.6250 - val_loss: 1.0902 - learning_rate: 5.0000e-04
Epoch 22/100
1/1  0s 40ms/step - accuracy: 0.7812 - loss: 0.9062
Epoch 22 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1793
1/1  0s 306ms/step - accuracy: 0.7812 - loss: 0.9062 - val_accuracy: 0.6250 - val_loss: 1.0855 - learning_rate: 5.0000e-04
Epoch 23/100
1/1  0s 54ms/step - accuracy: 0.6875 - loss: 1.0635
Epoch 23 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0625, Loss Gap: -0.0174
1/1  0s 367ms/step - accuracy: 0.6875 - loss: 1.0635 - val_accuracy: 0.6250 - val_loss: 1.0809 - learning_rate: 5.0000e-04
Epoch 24/100
1/1  0s 67ms/step - accuracy: 0.7812 - loss: 0.9562
Epoch 24 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1201
1/1  0s 419ms/step - accuracy: 0.7812 - loss: 0.9562 - val_accuracy: 0.6250 - val_loss: 1.0764 - learning_rate: 5.0000e-04
Epoch 25/100
1/1  0s 44ms/step - accuracy: 0.6562 - loss: 1.0177
Epoch 25 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0312, Loss Gap: -0.0538
1/1  0s 303ms/step - accuracy: 0.6562 - loss: 1.0177 - val_accuracy: 0.6250 - val_loss: 1.0715 - learning_rate: 5.0000e-04
Epoch 26/100
1/1  0s 56ms/step - accuracy: 0.6875 - loss: 1.0352
Epoch 26 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0625, Loss Gap: -0.0313
1/1  0s 313ms/step - accuracy: 0.6875 - loss: 1.0352 - val_accuracy: 0.6250 - val_loss: 1.0666 - learning_rate: 5.0000e-04
Epoch 27/100
1/1  0s 61ms/step - accuracy: 0.7812 - loss: 0.9134
Epoch 27 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1492
1/1  0s 294ms/step - accuracy: 0.7812 - loss: 0.9134 - val_accuracy: 0.6250 - val_loss: 1.0627 - learning_rate: 5.0000e-04
Epoch 28/100
1/1  0s 41ms/step - accuracy: 0.7500 - loss: 0.9370
Epoch 28 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.1250, Loss Gap: -0.1219
1/1  0s 263ms/step - accuracy: 0.7500 - loss: 0.9370 - val_accuracy: 0.6250 - val_loss: 1.0588 - learning_rate: 5.0000e-04
Epoch 29/100
1/1  0s 38ms/step - accuracy: 0.7812 - loss: 0.9453
Epoch 29 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1097
1/1  0s 255ms/step - accuracy: 0.7812 - loss: 0.9453 - val_accuracy: 0.6250 - val_loss: 1.0550 - learning_rate: 5.0000e-04
Epoch 30/100
1/1  0s 41ms/step - accuracy: 0.6875 - loss: 1.0243
Epoch 30 Overfitting Status: Slight Overfitting
Train-Val Accuracy Gap: 0.0625, Loss Gap: -0.0281
1/1  0s 253ms/step - accuracy: 0.6875 - loss: 1.0243 - val_accuracy: 0.6250 - val_loss: 1.0524 - learning_rate: 5.0000e-04
Epoch 31/100
1/1  0s 38ms/step - accuracy: 0.7812 - loss: 0.9662
Epoch 31 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.0839
1/1  0s 245ms/step - accuracy: 0.7812 - loss: 0.9662 - val_accuracy: 0.6250 - val_loss: 1.0501 - learning_rate: 5.0000e-04
Epoch 32/100
1/1  0s 40ms/step - accuracy: 0.8750 - loss: 0.7705
Epoch 32 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2773
1/1  0s 255ms/step - accuracy: 0.8750 - loss: 0.7705 - val_accuracy: 0.6250 - val_loss: 1.0477 - learning_rate: 5.0000e-04
Epoch 33/100
1/1  0s 40ms/step - accuracy: 0.7812 - loss: 0.8972
Epoch 33 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1482
1/1  0s 253ms/step - accuracy: 0.7812 - loss: 0.8972 - val_accuracy: 0.6250 - val_loss: 1.0454 - learning_rate: 5.0000e-04
Epoch 34/100
1/1  0s 40ms/step - accuracy: 0.8750 - loss: 0.7443
Epoch 34 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2984
1/1  0s 258ms/step - accuracy: 0.8750 - loss: 0.7443 - val_accuracy: 0.6250 - val_loss: 1.0427 - learning_rate: 5.0000e-04
Epoch 35/100
1/1  0s 45ms/step - accuracy: 0.8750 - loss: 0.7363
Epoch 35 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.3035
1/1  0s 320ms/step - accuracy: 0.8750 - loss: 0.7363 - val_accuracy: 0.6250 - val_loss: 1.0398 - learning_rate: 5.0000e-04
Epoch 36/100
1/1  0s 49ms/step - accuracy: 0.8438 - loss: 0.8262
Epoch 36 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2107
1/1  0s 305ms/step - accuracy: 0.8438 - loss: 0.8262 - val_accuracy: 0.6250 - val_loss: 1.0369 - learning_rate: 5.0000e-04
Epoch 37/100
1/1  0s 39ms/step - accuracy: 0.9062 - loss: 0.7903
Epoch 37 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2812, Loss Gap: -0.2439
1/1  0s 247ms/step - accuracy: 0.9062 - loss: 0.7903 - val_accuracy: 0.6250 - val_loss: 1.0341 - learning_rate: 5.0000e-04
Epoch 38/100
1/1  0s 47ms/step - accuracy: 0.7812 - loss: 0.9203
Epoch 38 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1113
1/1  0s 303ms/step - accuracy: 0.7812 - loss: 0.9203 - val_accuracy: 0.6250 - val_loss: 1.0316 - learning_rate: 5.0000e-04
Epoch 39/100
1/1  0s 41ms/step - accuracy: 0.8438 - loss: 0.7524
Epoch 39 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2770
1/1  0s 263ms/step - accuracy: 0.8438 - loss: 0.7524 - val_accuracy: 0.6250 - val_loss: 1.0294 - learning_rate: 5.0000e-04
Epoch 40/100
1/1  0s 41ms/step - accuracy: 0.7188 - loss: 0.9347
Epoch 40 Overfitting Status: Moderate Overfitting
Train-Val Accuracy Gap: 0.0938, Loss Gap: -0.0922
1/1  0s 275ms/step - accuracy: 0.7188 - loss: 0.9347 - val_accuracy: 0.6250 - val_loss: 1.0269 - learning_rate: 5.0000e-04
Epoch 41/100
1/1  0s 52ms/step - accuracy: 0.8438 - loss: 0.7439
Epoch 41 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2808
1/1  0s 377ms/step - accuracy: 0.8438 - loss: 0.7439 - val_accuracy: 0.6250 - val_loss: 1.0247 - learning_rate: 5.0000e-04
Epoch 42/100
1/1  0s 47ms/step - accuracy: 0.7812 - loss: 0.8901
Epoch 42 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1327
1/1  0s 275ms/step - accuracy: 0.7812 - loss: 0.8901 - val_accuracy: 0.6250 - val_loss: 1.0228 - learning_rate: 5.0000e-04
Epoch 43/100
1/1  0s 40ms/step - accuracy: 0.8438 - loss: 0.7803
Epoch 43 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2404
1/1  0s 291ms/step - accuracy: 0.8438 - loss: 0.7803 - val_accuracy: 0.6250 - val_loss: 1.0207 - learning_rate: 5.0000e-04
Epoch 44/100
1/1  0s 49ms/step - accuracy: 0.8125 - loss: 0.9203
Epoch 44 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.0989
1/1  0s 357ms/step - accuracy: 0.8125 - loss: 0.9203 - val_accuracy: 0.6250 - val_loss: 1.0192 - learning_rate: 5.0000e-04
Epoch 45/100
1/1  0s 50ms/step - accuracy: 0.8438 - loss: 0.8052
Epoch 45 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2125
1/1  0s 369ms/step - accuracy: 0.8438 - loss: 0.8052 - val_accuracy: 0.6250 - val_loss: 1.0177 - learning_rate: 5.0000e-04
Epoch 46/100
1/1  0s 55ms/step - accuracy: 0.9062 - loss: 0.6801
Epoch 46 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2812, Loss Gap: -0.3361
1/1  0s 333ms/step - accuracy: 0.9062 - loss: 0.6801 - val_accuracy: 0.6250 - val_loss: 1.0162 - learning_rate: 5.0000e-04
Epoch 47/100
1/1  0s 52ms/step - accuracy: 0.8125 - loss: 1.0161
Epoch 47 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: 0.0009
1/1  0s 377ms/step - accuracy: 0.8125 - loss: 1.0161 - val_accuracy: 0.6250 - val_loss: 1.0152 - learning_rate: 5.0000e-04
Epoch 48/100
1/1  0s 44ms/step - accuracy: 0.9062 - loss: 0.7229
Epoch 48 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2812, Loss Gap: -0.2912
1/1  0s 306ms/step - accuracy: 0.9062 - loss: 0.7229 - val_accuracy: 0.6250 - val_loss: 1.0141 - learning_rate: 5.0000e-04
Epoch 49/100
1/1  0s 53ms/step - accuracy: 0.8750 - loss: 0.7442
Epoch 49 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2686
1/1  0s 391ms/step - accuracy: 0.8750 - loss: 0.7442 - val_accuracy: 0.6250 - val_loss: 1.0129 - learning_rate: 5.0000e-04
Epoch 50/100
1/1  0s 49ms/step - accuracy: 0.8125 - loss: 0.7415
Epoch 50 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.2704
1/1  0s 337ms/step - accuracy: 0.8125 - loss: 0.7415 - val_accuracy: 0.6250 - val_loss: 1.0119 - learning_rate: 5.0000e-04
Epoch 51/100
1/1  0s 51ms/step - accuracy: 0.9062 - loss: 0.6894
Epoch 51 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2812, Loss Gap: -0.3215
1/1  0s 373ms/step - accuracy: 0.9062 - loss: 0.6894 - val_accuracy: 0.6250 - val_loss: 1.0109 - learning_rate: 5.0000e-04
Epoch 52/100
1/1  0s 49ms/step - accuracy: 0.9062 - loss: 0.6651
Epoch 52 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2812, Loss Gap: -0.3445
1/1  0s 318ms/step - accuracy: 0.9062 - loss: 0.6651 - val_accuracy: 0.6250 - val_loss: 1.0096 - learning_rate: 5.0000e-04
Epoch 53/100
1/1  0s 54ms/step - accuracy: 0.7812 - loss: 0.7710
Epoch 53 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.2374
1/1  0s 397ms/step - accuracy: 0.7812 - loss: 0.7710 - val_accuracy: 0.6250 - val_loss: 1.0084 - learning_rate: 5.0000e-04
Epoch 54/100
1/1  0s 44ms/step - accuracy: 0.8750 - loss: 0.7300
Epoch 54 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2776
1/1  0s 265ms/step - accuracy: 0.8750 - loss: 0.7300 - val_accuracy: 0.6250 - val_loss: 1.0077 - learning_rate: 5.0000e-04
Epoch 55/100
1/1  0s 46ms/step - accuracy: 0.8125 - loss: 0.8208
Epoch 55 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.1860
1/1  0s 295ms/step - accuracy: 0.8125 - loss: 0.8208 - val_accuracy: 0.6250 - val_loss: 1.0068 - learning_rate: 5.0000e-04
Epoch 56/100
1/1  0s 41ms/step - accuracy: 0.7812 - loss: 0.8204
Epoch 56 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1562, Loss Gap: -0.1856
1/1  0s 318ms/step - accuracy: 0.7812 - loss: 0.8204 - val_accuracy: 0.6250 - val_loss: 1.0060 - learning_rate: 5.0000e-04
Epoch 57/100
1/1  0s 54ms/step - accuracy: 0.8438 - loss: 0.7143
Epoch 57 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2910
1/1  0s 360ms/step - accuracy: 0.8438 - loss: 0.7143 - val_accuracy: 0.6250 - val_loss: 1.0053 - learning_rate: 5.0000e-04
Epoch 58/100
1/1  0s 39ms/step - accuracy: 0.8750 - loss: 0.7329
Epoch 58 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2719
1/1  0s 250ms/step - accuracy: 0.8750 - loss: 0.7329 - val_accuracy: 0.6250 - val_loss: 1.0048 - learning_rate: 5.0000e-04
Epoch 59/100
1/1  0s 42ms/step - accuracy: 0.9062 - loss: 0.6761
Epoch 59 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2812, Loss Gap: -0.3281
1/1  0s 255ms/step - accuracy: 0.9062 - loss: 0.6761 - val_accuracy: 0.6250 - val_loss: 1.0041 - learning_rate: 5.0000e-04
Epoch 60/100
1/1  0s 40ms/step - accuracy: 0.8750 - loss: 0.6431
Epoch 60 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.3605
1/1  0s 266ms/step - accuracy: 0.8750 - loss: 0.6431 - val_accuracy: 0.6250 - val_loss: 1.0036 - learning_rate: 5.0000e-04
Epoch 61/100
1/1  0s 50ms/step - accuracy: 0.8438 - loss: 0.6892
Epoch 61 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.3139
1/1  0s 399ms/step - accuracy: 0.8438 - loss: 0.6892 - val_accuracy: 0.6250 - val_loss: 1.0031 - learning_rate: 5.0000e-04
Epoch 62/100
1/1  0s 52ms/step - accuracy: 0.8750 - loss: 0.7260
Epoch 62 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2766
1/1  0s 370ms/step - accuracy: 0.8750 - loss: 0.7260 - val_accuracy: 0.6250 - val_loss: 1.0026 - learning_rate: 5.0000e-04
Epoch 63/100
1/1  0s 55ms/step - accuracy: 0.8438 - loss: 0.7495
Epoch 63 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2526
1/1  0s 382ms/step - accuracy: 0.8438 - loss: 0.7495 - val_accuracy: 0.6250 - val_loss: 1.0021 - learning_rate: 5.0000e-04
Epoch 64/100
1/1  0s 60ms/step - accuracy: 0.8125 - loss: 0.7971
Epoch 64 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.1875, Loss Gap: -0.2046
1/1  0s 383ms/step - accuracy: 0.8125 - loss: 0.7971 - val_accuracy: 0.6250 - val_loss: 1.0017 - learning_rate: 5.0000e-04
Epoch 65/100
1/1  0s 61ms/step - accuracy: 0.8750 - loss: 0.7393
Epoch 65 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2615
1/1  0s 398ms/step - accuracy: 0.8750 - loss: 0.7393 - val_accuracy: 0.6250 - val_loss: 1.0008 - learning_rate: 5.0000e-04
Epoch 66/100
1/1  0s 63ms/step - accuracy: 0.8438 - loss: 0.7597
Epoch 66 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2404
1/1  0s 378ms/step - accuracy: 0.8438 - loss: 0.7597 - val_accuracy: 0.6250 - val_loss: 1.0000 - learning_rate: 5.0000e-04
Epoch 67/100
1/1  0s 58ms/step - accuracy: 0.8750 - loss: 0.7520
Epoch 67 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.2473
1/1  0s 401ms/step - accuracy: 0.8750 - loss: 0.7520 - val_accuracy: 0.6250 - val_loss: 0.9993 - learning_rate: 5.0000e-04
Epoch 68/100
1/1  0s 205ms/step - accuracy: 0.8750 - loss: 0.8101
Epoch 68 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2500, Loss Gap: -0.1886
1/1  1s 553ms/step - accuracy: 0.8750 - loss: 0.8101 - val_accuracy: 0.6250 - val_loss: 0.9988 - learning_rate: 5.0000e-04
Epoch 69/100
1/1  0s 42ms/step - accuracy: 0.8438 - loss: 0.7390
Epoch 69 Overfitting Status: SEVERE OVERFITTING
Train-Val Accuracy Gap: 0.2188, Loss Gap: -0.2594
1/1  0s 339ms/step - accuracy: 0.8438 - loss: 0.7390 - val_accuracy: 0.6250 - val_loss: 0.9984 - learning_rate: 5.0000e-04
Epoch 70/100
1/1  0s 39ms/step - accuracy: 0.7812 - loss: 0.8582